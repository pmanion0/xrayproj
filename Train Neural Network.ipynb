{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ddb34fde-926f-42f6-8bfc-b1b19cb4881d"
    }
   },
   "source": [
    "# Import and High-Level Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "80e3fe37-bc30-43b7-91c3-474b94a16db6"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# General Python Packages\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import lr_scheduler, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Module\n",
    "\n",
    "# General Analytics Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization / Image Packages\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Randomization Functions\n",
    "from random import random as randuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "9b582614-ccd7-4f48-8b66-a49ebe66807f"
    }
   },
   "outputs": [],
   "source": [
    "# Put MatPlotLib in interactive mode\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "97faafc3-219d-4a56-b587-32a9c2550eac"
    }
   },
   "source": [
    "# Define Data Manipulation Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6fed3f3e-b14a-457d-95d2-c3726ce0fb3e"
    }
   },
   "source": [
    "### Helper Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "f962c744-4459-4ca1-851c-a19fe8457118"
    }
   },
   "outputs": [],
   "source": [
    "def is_image_file(fname):\n",
    "    \"\"\"Checks if a file is an image.\n",
    "    Args:\n",
    "        fname (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    return fname.lower().endswith('.png')\n",
    "\n",
    "def create_label_maps(details_df):\n",
    "    \"\"\" Take a descriptive dataframe and extract the unique labels and map to index values\n",
    "    Args:\n",
    "        details_df: Dataframe with the image details\n",
    "    Returns:\n",
    "        label_list: list of unique labels in the dataframe\n",
    "        label_to_index: map from labels to indices\n",
    "    \"\"\"\n",
    "    \"\"\" TODO: Research paper also excludes these labels but need to figure out how to handle\n",
    "              cases that have these as positive findings (completely exclude?)\n",
    "    excluded_labels = ['Edema','Hernia','Emphysema','Fibrosis','No Finding'\n",
    "                      'Pleural_Thickening','Consolidation']\n",
    "    \"\"\"\n",
    "    excluded_labels = ['No Finding']\n",
    "    \n",
    "    label_groups = details_df['Finding Labels'].unique()\n",
    "    unique_labels = set([label for sublist in label_groups.tolist() for label in sublist.split('|')])\n",
    "    \n",
    "    # Drop some label that we do not want to include\n",
    "    unique_labels = [l for l in unique_labels if l not in excluded_labels]\n",
    "\n",
    "    index_to_label = {idx: val for idx, val in enumerate(unique_labels)}\n",
    "    label_to_index = {val: idx for idx, val in index_to_label.items()}\n",
    "\n",
    "    label_list = list(label_to_index.keys())\n",
    "\n",
    "    return label_list, label_to_index\n",
    "\n",
    "def create_image_list(dir):\n",
    "    \"\"\" Create a full list of images available \n",
    "    Args:\n",
    "        dir (string): root directory of images with subdirectories underneath\n",
    "                      that have the .png images within them\n",
    "    Returns:\n",
    "        image_list: list of tuples with (image_name, full_image_path)\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for subfolder in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, subfolder)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for subfolder_path, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(subfolder_path, fname)\n",
    "                    image_list.append((fname, path))\n",
    "    return image_list\n",
    "\n",
    "def pil_loader(path):\n",
    "    \"\"\" Opens path as file with Pillow (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    Args:\n",
    "        path (string): File path to the image\n",
    "    Returns:\n",
    "        img: Image in RGB format\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "        \n",
    "def imshow(inp, title=None):\n",
    "    \"\"\" Convert tensor array to an image (only use post-dataset transform) \"\"\"\n",
    "    inp = inp[0]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5d91972-9d4b-4022-842b-22c823f98fff"
    }
   },
   "source": [
    "### Implementation of Torch's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "5bf4e82b-13ca-4ac2-bbb6-3081e820ab4e"
    }
   },
   "outputs": [],
   "source": [
    "class XrayImageSet(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_root (string): root directory of the images in form image/subfolder/*.png\n",
    "        csv_file (string): path to the CSV data file\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "     Attributes:\n",
    "        labels (list): list of the possible label names.\n",
    "        label_to_index (dict): look from label name to a label index\n",
    "        imgs (list): List of (filename, image path) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_root, csv_file, transform=None, target_transform=None, loader = pil_loader):\n",
    "        \"\"\" Create an instance of the Xray Dataset \"\"\"\n",
    "        img_details = pd.read_csv(csv_file)\n",
    "        \n",
    "        labels, label_to_index = create_label_maps(img_details)\n",
    "        imgs = create_image_list(image_root)\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.image_details = img_details\n",
    "        self.image_root = image_root\n",
    "        self.labels = labels\n",
    "        self.label_to_index = label_to_index\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.max_label_index = max(label_to_index.values())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get image,labels pair by index\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        fname, path = self.imgs[index]\n",
    "        target = self.get_one_hot_labels(fname)\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Calculate length of the dataset (number of images) \"\"\"\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def get_labels(self, fname):\n",
    "        \"\"\" Return the label string for the file \"\"\"\n",
    "        return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
    "    \n",
    "    def one_hot_labels(self, labels):\n",
    "        \"\"\" Convert the labels string (with each label separated by |) into 1-hot encoding \"\"\"\n",
    "        if labels == None:\n",
    "            return None\n",
    "        \n",
    "        split_label_indices = [self.label_to_index.get(label)\n",
    "                               for label in labels.split('|')\n",
    "                               if label != 'No Finding']\n",
    "        \n",
    "        out = [1 if idx in split_label_indices else 0 for idx in range(self.max_label_index+1)]\n",
    "        # This code UNHOTs the labels:\n",
    "        # out = '|'.join([index_to_label.get(idx) for idx, val in enumerate(one_hot_tuple) if val == 1])\n",
    "        return out\n",
    "\n",
    "    def get_one_hot_labels(self, fname):\n",
    "        \"\"\" Get the 1-hot encoded label array for the provided file \"\"\"\n",
    "        labels = self.get_labels(fname)\n",
    "        one_hot_labels = self.one_hot_labels(labels)\n",
    "        return torch.FloatTensor(one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b6f705b5-c4e4-4fbd-a517-f0c3b58c4305"
    }
   },
   "source": [
    "### Create the dataset with necessary transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "6716d746-b7b1-4aff-aec0-2bd91632bf28"
    }
   },
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbpresent": {
     "id": "62d30308-7ecb-40f4-a831-dd0a9274618a"
    }
   },
   "outputs": [],
   "source": [
    "img_data_train = XrayImageSet(image_root = '/user/images/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_train.imgs = [img for i, img in enumerate(img_data_train.imgs) if i % 10 > 0]# and randuni() < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "nbpresent": {
     "id": "13f86c44-50f2-4809-8ee1-afb0d8ec0b7a"
    }
   },
   "outputs": [],
   "source": [
    "img_data_val   = XrayImageSet(image_root = '/user/images/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_val.imgs = [img for i, img in enumerate(img_data_val.imgs) if i % 10 == 0]# and randuni() < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "nbpresent": {
     "id": "09ab4157-ad5b-4602-8b93-85c86bd5a620"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 100908\n",
      "Validation Set Size: 11212\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size: {}\".format(len(img_data_train)))\n",
    "print(\"Validation Set Size: {}\".format(len(img_data_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1b2e1feb-e832-41c5-8b1d-70384f1fe915"
    }
   },
   "source": [
    "### Put the dataset into a Dataloader to handle batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "nbpresent": {
     "id": "e0484420-b2a8-429b-8da4-368a592db7b8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU: 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "num_gpus = torch.cuda.device_count()\n",
    "pin_mem_setting = True\n",
    "\n",
    "print(\"Number of GPU: {}\".format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "nbpresent": {
     "id": "1001fa5d-b820-4da6-9d18-0ee7f4462b90"
    }
   },
   "outputs": [],
   "source": [
    "img_loader_train = DataLoader(img_data_train,\n",
    "                              batch_size = batch_size * num_gpus,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 10,\n",
    "                              pin_memory = pin_mem_setting)\n",
    "\n",
    "img_loader_val   = DataLoader(img_data_val,\n",
    "                              batch_size = batch_size * num_gpus,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 10,\n",
    "                              pin_memory = pin_mem_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "nbpresent": {
     "id": "0e1d2cb0-4d0f-4ab1-a1cf-cd66a3e298a2"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': img_loader_train,\n",
    "    'val': img_loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "350daac2-9c37-42d0-a1ef-de7a8110b38f"
    }
   },
   "source": [
    "# Define model training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "nbpresent": {
     "id": "166ce9d1-ff17-4047-b9ae-b4903393ad15"
    }
   },
   "outputs": [],
   "source": [
    "class printer_writer:\n",
    "    def __init__(self, output_folder_path):\n",
    "        self.start_time = time.strftime('%Y%m%d-%Hh%Mm%Ss')\n",
    "        \n",
    "        self.outprefix = output_folder_path + '/' + self.start_time\n",
    "        \n",
    "        # Print Output File\n",
    "        self.print_out_path = self.outprefix + '_print.txt'\n",
    "        self.print_out_file = open(self.print_out_path, 'w', 1)\n",
    "        \n",
    "    def printw(self, string):\n",
    "        print(string)\n",
    "        try:\n",
    "            self.print_out_file.write(string + \"\\n\")\n",
    "        except: # Ignore errors\n",
    "            pass\n",
    "        \n",
    "    def save_checkpoint(self, epoch, model, optimizer, scheduler, val_error):\n",
    "        model_out_path = self.outprefix + '_model_' + str(epoch+1) + '.tar'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'state': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'val_error': val_error\n",
    "        }, model_out_path)\n",
    "        \n",
    "    def close(self):\n",
    "        self.print_out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "nbpresent": {
     "id": "eb99e6e4-00db-494a-ad27-70005761f49e"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, outfolder = '/user/xrayproj/output/'):\n",
    "    since = time.time()\n",
    "    scribe = printer_writer(outfolder)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scribe.printw('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        scribe.printw('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            obs_counter = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Store statistics (convert from autograd.Variable to float/int)\n",
    "                loss_val = loss.data[0]\n",
    "                correct_val = torch.sum( ((outputs.sigmoid()>0.5) == (labels>0.5)).long() ).data[0]\n",
    "                \n",
    "                running_loss += loss_val\n",
    "                running_corrects += correct_val\n",
    "                \n",
    "                obs_counter += len(inputs)\n",
    "                \n",
    "                batch_loss = 1.0 * loss_val / len(inputs)\n",
    "                batch_acc = 1.0 * correct_val / len(inputs)\n",
    "                status = ' |~~ {}@{}  Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    phase, obs_counter, batch_loss, batch_acc)\n",
    "                scribe.printw(status)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            scribe.printw('{}  Loss: {:.6f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                scribe.save_checkpoint(epoch, model, optimizer, scheduler, epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    scribe.printw('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    scribe.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e6eebab-809d-4550-b771-135dbf2b893d"
    }
   },
   "source": [
    "# Define Weighted Cost Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "nbpresent": {
     "id": "0c520d22-a18b-4e5f-b849-1960820f4d04"
    }
   },
   "outputs": [],
   "source": [
    "def imbalance_weighted_bce_with_logit(input, target, size_average=True):\n",
    "    if not (target.size() == input.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "\n",
    "    max_val = (-input).clamp(min=0)\n",
    "    loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "    # Determine |P| and |N|\n",
    "    positive_labels = target.sum()\n",
    "    negative_labels = (1-target).sum()\n",
    "\n",
    "    # Upweight the less common class (very often the 1s)\n",
    "    beta_p = (positive_labels + negative_labels) / positive_labels\n",
    "    beta_n = (positive_labels + negative_labels) / negative_labels\n",
    "\n",
    "    # Adjust the losses accordingly\n",
    "    loss_weight = target * beta_p + (1-target) * beta_n\n",
    "    \n",
    "    loss = loss * loss_weight\n",
    "\n",
    "    if size_average:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "nbpresent": {
     "id": "f66b439c-1f87-4d08-939d-f64d085b846b"
    }
   },
   "outputs": [],
   "source": [
    "class BCEWithLogitsImbalanceWeightedLoss(Module):\n",
    "    def __init__(self, class_weight=None, size_average=True):\n",
    "        super(BCEWithLogitsImbalanceWeightedLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return imbalance_weighted_bce_with_logit(input, target, size_average=self.size_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8ca6e253-06b7-4a15-ac6c-370629667ad6"
    }
   },
   "source": [
    "# Setup Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18PlusFlexibleFC():\n",
    "    # Create a base ResNet18 model\n",
    "    m = models.resnet18(pretrained=True)\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final FC layer\n",
    "    m.fc = nn.Linear(m.fc.in_features, len(img_data_train.labels))\n",
    "    \n",
    "    return m\n",
    "\n",
    "    def model(self, ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5580486c-3807-459c-a02d-68be7ffc7e08"
    }
   },
   "source": [
    "### Pull the ResNet-18 pre-trained model and replace the fully connected layer at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "nbpresent": {
     "id": "91946119-bf67-4871-92af-649559fa9bfd"
    }
   },
   "outputs": [],
   "source": [
    "model_base = ResNet18PlusFlexibleFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b85990f9-2c42-48b1-9e5b-857ebd8c2f30"
    }
   },
   "source": [
    "### Push model to CUDA/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "nbpresent": {
     "id": "0a904f1f-73ac-418b-86cf-cdafdf0f67b1"
    }
   },
   "outputs": [],
   "source": [
    "model_ft = DataParallel(model_base).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e6851de2-8645-4547-9f00-414ad8a3811a"
    }
   },
   "source": [
    "### Define loss measure and learning rates/procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "nbpresent": {
     "id": "1d61f077-84ec-4d2c-bdb0-82b28f8c4ed9"
    }
   },
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsImbalanceWeightedLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = SGD(model_ft.module.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future code for allowing optimization of the base layer with a lower learning rate\n",
    "\n",
    "```\n",
    "ignored_params = list(map(id, model.fc.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in ignored_params,\n",
    "                     model.parameters())\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "            {'params': base_params},\n",
    "            {'params': model.fc.parameters(), 'lr': opt.lr}\n",
    "        ], lr=opt.lr*0.1, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f9596b5-a2fc-4e0c-984b-4e13b68dcd6d"
    }
   },
   "source": [
    "# Begin Training Network (Normal Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "nbpresent": {
     "id": "f1672c30-c299-4265-934b-6af391d9de8c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001541 Acc: 6.1330\n",
      " |~~ train@2000  Loss: 0.001542 Acc: 6.0640\n",
      " |~~ train@3000  Loss: 0.001528 Acc: 6.0640\n",
      " |~~ train@4000  Loss: 0.001492 Acc: 6.0320\n",
      " |~~ train@5000  Loss: 0.001473 Acc: 6.0100\n",
      " |~~ train@6000  Loss: 0.001430 Acc: 6.0380\n",
      " |~~ train@7000  Loss: 0.001393 Acc: 6.1250\n",
      " |~~ train@8000  Loss: 0.001379 Acc: 6.2670\n",
      " |~~ train@9000  Loss: 0.001336 Acc: 6.4420\n",
      " |~~ train@10000  Loss: 0.001304 Acc: 6.6770\n",
      " |~~ train@11000  Loss: 0.001281 Acc: 7.0010\n",
      " |~~ train@12000  Loss: 0.001281 Acc: 7.3510\n",
      " |~~ train@13000  Loss: 0.001254 Acc: 7.7840\n",
      " |~~ train@14000  Loss: 0.001272 Acc: 8.1270\n",
      " |~~ train@15000  Loss: 0.001246 Acc: 8.5020\n",
      " |~~ train@16000  Loss: 0.001251 Acc: 8.8060\n",
      " |~~ train@17000  Loss: 0.001250 Acc: 8.9860\n",
      " |~~ train@18000  Loss: 0.001251 Acc: 9.1680\n",
      " |~~ train@19000  Loss: 0.001269 Acc: 9.3130\n",
      " |~~ train@20000  Loss: 0.001231 Acc: 9.4640\n",
      " |~~ train@21000  Loss: 0.001246 Acc: 9.4760\n",
      " |~~ train@22000  Loss: 0.001221 Acc: 9.5910\n",
      " |~~ train@23000  Loss: 0.001203 Acc: 9.6240\n",
      " |~~ train@24000  Loss: 0.001244 Acc: 9.6040\n",
      " |~~ train@41000  Loss: 0.001179 Acc: 9.6300\n",
      " |~~ train@42000  Loss: 0.001223 Acc: 9.5200\n",
      " |~~ train@43000  Loss: 0.001204 Acc: 9.5160\n",
      " |~~ train@44000  Loss: 0.001193 Acc: 9.5180\n",
      " |~~ train@45000  Loss: 0.001205 Acc: 9.5300\n",
      " |~~ train@46000  Loss: 0.001175 Acc: 9.5200\n",
      " |~~ train@47000  Loss: 0.001210 Acc: 9.4670\n",
      " |~~ train@48000  Loss: 0.001211 Acc: 9.4830\n",
      " |~~ train@49000  Loss: 0.001218 Acc: 9.4350\n",
      " |~~ train@50000  Loss: 0.001217 Acc: 9.4140\n",
      " |~~ train@51000  Loss: 0.001179 Acc: 9.5150\n",
      " |~~ train@52000  Loss: 0.001203 Acc: 9.4510\n",
      " |~~ train@53000  Loss: 0.001199 Acc: 9.4150\n",
      " |~~ train@54000  Loss: 0.001204 Acc: 9.4320\n",
      " |~~ train@55000  Loss: 0.001199 Acc: 9.4520\n",
      " |~~ train@56000  Loss: 0.001187 Acc: 9.3980\n",
      " |~~ train@57000  Loss: 0.001221 Acc: 9.3380\n",
      " |~~ train@58000  Loss: 0.001191 Acc: 9.4270\n",
      " |~~ train@59000  Loss: 0.001183 Acc: 9.3860\n",
      " |~~ train@60000  Loss: 0.001203 Acc: 9.3870\n",
      " |~~ train@61000  Loss: 0.001210 Acc: 9.3720\n",
      " |~~ train@62000  Loss: 0.001191 Acc: 9.3710\n",
      " |~~ train@63000  Loss: 0.001228 Acc: 9.3430\n",
      " |~~ train@64000  Loss: 0.001203 Acc: 9.3700\n",
      " |~~ train@65000  Loss: 0.001200 Acc: 9.3440\n",
      " |~~ train@66000  Loss: 0.001186 Acc: 9.3840\n",
      " |~~ train@67000  Loss: 0.001176 Acc: 9.3850\n",
      " |~~ train@68000  Loss: 0.001229 Acc: 9.2760\n",
      " |~~ train@69000  Loss: 0.001195 Acc: 9.3810\n",
      " |~~ train@70000  Loss: 0.001192 Acc: 9.3480\n",
      " |~~ train@71000  Loss: 0.001183 Acc: 9.3790\n",
      " |~~ train@72000  Loss: 0.001208 Acc: 9.3430\n",
      " |~~ train@73000  Loss: 0.001168 Acc: 9.3810\n",
      " |~~ train@74000  Loss: 0.001196 Acc: 9.3260\n",
      " |~~ train@75000  Loss: 0.001204 Acc: 9.3740\n",
      " |~~ train@76000  Loss: 0.001171 Acc: 9.4250\n",
      " |~~ train@77000  Loss: 0.001177 Acc: 9.4280\n",
      " |~~ train@78000  Loss: 0.001182 Acc: 9.4390\n",
      " |~~ train@79000  Loss: 0.001186 Acc: 9.4680\n",
      " |~~ train@80000  Loss: 0.001179 Acc: 9.4250\n",
      " |~~ train@81000  Loss: 0.001158 Acc: 9.4770\n",
      " |~~ train@82000  Loss: 0.001161 Acc: 9.4900\n",
      " |~~ train@83000  Loss: 0.001199 Acc: 9.4350\n",
      " |~~ train@84000  Loss: 0.001217 Acc: 9.4390\n",
      " |~~ train@85000  Loss: 0.001184 Acc: 9.4780\n",
      " |~~ train@86000  Loss: 0.001172 Acc: 9.5030\n",
      " |~~ train@87000  Loss: 0.001170 Acc: 9.5430\n",
      " |~~ train@88000  Loss: 0.001207 Acc: 9.4310\n",
      " |~~ train@89000  Loss: 0.001186 Acc: 9.5180\n",
      " |~~ train@90000  Loss: 0.001188 Acc: 9.5230\n",
      " |~~ train@91000  Loss: 0.001198 Acc: 9.4950\n",
      " |~~ train@92000  Loss: 0.001158 Acc: 9.5760\n",
      " |~~ train@93000  Loss: 0.001188 Acc: 9.5410\n",
      " |~~ train@94000  Loss: 0.001171 Acc: 9.5390\n",
      " |~~ train@95000  Loss: 0.001183 Acc: 9.5590\n",
      " |~~ train@96000  Loss: 0.001195 Acc: 9.5150\n",
      " |~~ train@97000  Loss: 0.001178 Acc: 9.5520\n",
      " |~~ train@98000  Loss: 0.001173 Acc: 9.5720\n",
      " |~~ train@99000  Loss: 0.001173 Acc: 9.5820\n",
      " |~~ train@100000  Loss: 0.001159 Acc: 9.5560\n",
      " |~~ train@100908  Loss: 0.001335 Acc: 9.4736\n",
      "train  Loss: 0.001229 Acc: 9.0639\n",
      " |~~ val@1000  Loss: 0.001147 Acc: 9.6440\n",
      " |~~ val@2000  Loss: 0.001149 Acc: 9.5670\n",
      " |~~ val@3000  Loss: 0.001179 Acc: 9.5920\n",
      " |~~ val@4000  Loss: 0.001174 Acc: 9.5760\n",
      " |~~ val@5000  Loss: 0.001161 Acc: 9.5700\n",
      " |~~ val@6000  Loss: 0.001174 Acc: 9.5630\n",
      " |~~ val@7000  Loss: 0.001176 Acc: 9.5740\n",
      " |~~ val@8000  Loss: 0.001197 Acc: 9.5330\n",
      " |~~ val@9000  Loss: 0.001173 Acc: 9.5580\n",
      " |~~ val@10000  Loss: 0.001191 Acc: 9.4510\n",
      " |~~ val@11000  Loss: 0.001169 Acc: 9.5710\n",
      " |~~ val@11212  Loss: 0.005488 Acc: 9.6887\n",
      "val  Loss: 0.001253 Acc: 9.5659\n",
      "Epoch 1/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001185 Acc: 9.5340\n",
      " |~~ train@2000  Loss: 0.001172 Acc: 9.6090\n",
      " |~~ train@3000  Loss: 0.001163 Acc: 9.5830\n",
      " |~~ train@4000  Loss: 0.001159 Acc: 9.5840\n",
      " |~~ train@5000  Loss: 0.001161 Acc: 9.5600\n",
      " |~~ train@6000  Loss: 0.001189 Acc: 9.5800\n",
      " |~~ train@7000  Loss: 0.001182 Acc: 9.5950\n",
      " |~~ train@8000  Loss: 0.001172 Acc: 9.5310\n",
      " |~~ train@9000  Loss: 0.001204 Acc: 9.5430\n",
      " |~~ train@10000  Loss: 0.001203 Acc: 9.4830\n",
      " |~~ train@11000  Loss: 0.001173 Acc: 9.5320\n",
      " |~~ train@12000  Loss: 0.001171 Acc: 9.5060\n",
      " |~~ train@13000  Loss: 0.001191 Acc: 9.5280\n",
      " |~~ train@14000  Loss: 0.001177 Acc: 9.5050\n",
      " |~~ train@15000  Loss: 0.001180 Acc: 9.5120\n",
      " |~~ train@16000  Loss: 0.001182 Acc: 9.4670\n",
      " |~~ train@17000  Loss: 0.001195 Acc: 9.4510\n",
      " |~~ train@18000  Loss: 0.001156 Acc: 9.5180\n",
      " |~~ train@19000  Loss: 0.001193 Acc: 9.4950\n",
      " |~~ train@20000  Loss: 0.001138 Acc: 9.5990\n",
      " |~~ train@21000  Loss: 0.001165 Acc: 9.5760\n",
      " |~~ train@22000  Loss: 0.001154 Acc: 9.5650\n",
      " |~~ train@23000  Loss: 0.001190 Acc: 9.5250\n",
      " |~~ train@24000  Loss: 0.001177 Acc: 9.5680\n",
      " |~~ train@25000  Loss: 0.001186 Acc: 9.5750\n",
      " |~~ train@26000  Loss: 0.001212 Acc: 9.5410\n",
      " |~~ train@27000  Loss: 0.001184 Acc: 9.5670\n",
      " |~~ train@28000  Loss: 0.001150 Acc: 9.6130\n",
      " |~~ train@29000  Loss: 0.001167 Acc: 9.6210\n",
      " |~~ train@30000  Loss: 0.001168 Acc: 9.6400\n",
      " |~~ train@31000  Loss: 0.001163 Acc: 9.6470\n",
      " |~~ train@32000  Loss: 0.001180 Acc: 9.6040\n",
      " |~~ train@33000  Loss: 0.001227 Acc: 9.6190\n",
      " |~~ train@34000  Loss: 0.001160 Acc: 9.6420\n",
      " |~~ train@35000  Loss: 0.001159 Acc: 9.6560\n",
      " |~~ train@36000  Loss: 0.001158 Acc: 9.6520\n",
      " |~~ train@37000  Loss: 0.001165 Acc: 9.6270\n",
      " |~~ train@38000  Loss: 0.001142 Acc: 9.6320\n",
      " |~~ train@39000  Loss: 0.001152 Acc: 9.6130\n",
      " |~~ train@40000  Loss: 0.001160 Acc: 9.6110\n",
      " |~~ train@41000  Loss: 0.001169 Acc: 9.6600\n",
      " |~~ train@42000  Loss: 0.001176 Acc: 9.6190\n",
      " |~~ train@43000  Loss: 0.001166 Acc: 9.6690\n",
      " |~~ train@44000  Loss: 0.001200 Acc: 9.5760\n",
      " |~~ train@45000  Loss: 0.001155 Acc: 9.6920\n",
      " |~~ train@46000  Loss: 0.001171 Acc: 9.5890\n",
      " |~~ train@47000  Loss: 0.001189 Acc: 9.5650\n",
      " |~~ train@48000  Loss: 0.001170 Acc: 9.5010\n",
      " |~~ train@49000  Loss: 0.001169 Acc: 9.6030\n",
      " |~~ train@50000  Loss: 0.001128 Acc: 9.5790\n",
      " |~~ train@51000  Loss: 0.001170 Acc: 9.5430\n",
      " |~~ train@52000  Loss: 0.001155 Acc: 9.5850\n",
      " |~~ train@53000  Loss: 0.001166 Acc: 9.5600\n",
      " |~~ train@54000  Loss: 0.001163 Acc: 9.5280\n",
      " |~~ train@55000  Loss: 0.001150 Acc: 9.5360\n",
      " |~~ train@56000  Loss: 0.001162 Acc: 9.5960\n",
      " |~~ train@57000  Loss: 0.001197 Acc: 9.5410\n",
      " |~~ train@58000  Loss: 0.001178 Acc: 9.5300\n",
      " |~~ train@59000  Loss: 0.001166 Acc: 9.5730\n",
      " |~~ train@60000  Loss: 0.001157 Acc: 9.5980\n",
      " |~~ train@61000  Loss: 0.001121 Acc: 9.6350\n",
      " |~~ train@62000  Loss: 0.001173 Acc: 9.5470\n",
      " |~~ train@63000  Loss: 0.001165 Acc: 9.5960\n",
      " |~~ train@64000  Loss: 0.001193 Acc: 9.5780\n",
      " |~~ train@65000  Loss: 0.001160 Acc: 9.6490\n",
      " |~~ train@66000  Loss: 0.001172 Acc: 9.5850\n",
      " |~~ train@67000  Loss: 0.001157 Acc: 9.6410\n",
      " |~~ train@68000  Loss: 0.001150 Acc: 9.6080\n",
      " |~~ train@69000  Loss: 0.001169 Acc: 9.5970\n",
      " |~~ train@70000  Loss: 0.001170 Acc: 9.6000\n",
      " |~~ train@71000  Loss: 0.001187 Acc: 9.6250\n",
      " |~~ train@72000  Loss: 0.001145 Acc: 9.6180\n",
      " |~~ train@73000  Loss: 0.001158 Acc: 9.6510\n",
      " |~~ train@74000  Loss: 0.001194 Acc: 9.5960\n",
      " |~~ train@75000  Loss: 0.001159 Acc: 9.6280\n",
      " |~~ train@76000  Loss: 0.001171 Acc: 9.5690\n",
      " |~~ train@77000  Loss: 0.001175 Acc: 9.6080\n",
      " |~~ train@78000  Loss: 0.001210 Acc: 9.5370\n",
      " |~~ train@79000  Loss: 0.001097 Acc: 9.6470\n",
      " |~~ train@80000  Loss: 0.001154 Acc: 9.6060\n",
      " |~~ train@81000  Loss: 0.001174 Acc: 9.6080\n",
      " |~~ train@82000  Loss: 0.001148 Acc: 9.5970\n",
      " |~~ train@83000  Loss: 0.001180 Acc: 9.6120\n",
      " |~~ train@84000  Loss: 0.001169 Acc: 9.5210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@85000  Loss: 0.001163 Acc: 9.6080\n",
      " |~~ train@86000  Loss: 0.001156 Acc: 9.6520\n",
      " |~~ train@87000  Loss: 0.001162 Acc: 9.6020\n",
      " |~~ train@88000  Loss: 0.001144 Acc: 9.6370\n",
      " |~~ train@89000  Loss: 0.001132 Acc: 9.6460\n",
      " |~~ train@90000  Loss: 0.001168 Acc: 9.6340\n",
      " |~~ train@91000  Loss: 0.001178 Acc: 9.6290\n",
      " |~~ train@92000  Loss: 0.001166 Acc: 9.6010\n",
      " |~~ train@93000  Loss: 0.001127 Acc: 9.6260\n",
      " |~~ train@94000  Loss: 0.001132 Acc: 9.6780\n",
      " |~~ train@95000  Loss: 0.001168 Acc: 9.6510\n",
      " |~~ train@96000  Loss: 0.001154 Acc: 9.6720\n",
      " |~~ train@97000  Loss: 0.001150 Acc: 9.6600\n",
      " |~~ train@98000  Loss: 0.001167 Acc: 9.6690\n",
      " |~~ train@99000  Loss: 0.001149 Acc: 9.7180\n",
      " |~~ train@100000  Loss: 0.001171 Acc: 9.6860\n",
      " |~~ train@100908  Loss: 0.001281 Acc: 9.7401\n",
      "train  Loss: 0.001169 Acc: 9.5941\n",
      " |~~ val@1000  Loss: 0.001154 Acc: 9.6690\n",
      " |~~ val@2000  Loss: 0.001136 Acc: 9.7370\n",
      " |~~ val@3000  Loss: 0.001153 Acc: 9.6350\n",
      " |~~ val@4000  Loss: 0.001159 Acc: 9.6650\n",
      " |~~ val@5000  Loss: 0.001129 Acc: 9.6740\n",
      " |~~ val@6000  Loss: 0.001118 Acc: 9.7680\n",
      " |~~ val@7000  Loss: 0.001171 Acc: 9.7380\n",
      " |~~ val@8000  Loss: 0.001161 Acc: 9.6720\n",
      " |~~ val@9000  Loss: 0.001135 Acc: 9.6410\n",
      " |~~ val@10000  Loss: 0.001159 Acc: 9.6500\n",
      " |~~ val@11000  Loss: 0.001147 Acc: 9.6520\n",
      " |~~ val@11212  Loss: 0.005291 Acc: 9.7170\n",
      "val  Loss: 0.001226 Acc: 9.6826\n",
      "Epoch 2/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001165 Acc: 9.6180\n",
      " |~~ train@2000  Loss: 0.001146 Acc: 9.6670\n",
      " |~~ train@3000  Loss: 0.001124 Acc: 9.7360\n",
      " |~~ train@4000  Loss: 0.001141 Acc: 9.6660\n",
      " |~~ train@5000  Loss: 0.001141 Acc: 9.7530\n",
      " |~~ train@6000  Loss: 0.001130 Acc: 9.7720\n",
      " |~~ train@7000  Loss: 0.001163 Acc: 9.6360\n",
      " |~~ train@8000  Loss: 0.001143 Acc: 9.7250\n",
      " |~~ train@9000  Loss: 0.001163 Acc: 9.6730\n",
      " |~~ train@10000  Loss: 0.001180 Acc: 9.6210\n",
      " |~~ train@11000  Loss: 0.001134 Acc: 9.7190\n",
      " |~~ train@12000  Loss: 0.001133 Acc: 9.7170\n",
      " |~~ train@13000  Loss: 0.001144 Acc: 9.7180\n",
      " |~~ train@14000  Loss: 0.001166 Acc: 9.6730\n",
      " |~~ train@15000  Loss: 0.001167 Acc: 9.6900\n",
      " |~~ train@16000  Loss: 0.001174 Acc: 9.6680\n",
      " |~~ train@17000  Loss: 0.001155 Acc: 9.6690\n",
      " |~~ train@18000  Loss: 0.001163 Acc: 9.6600\n",
      " |~~ train@19000  Loss: 0.001145 Acc: 9.6340\n",
      " |~~ train@20000  Loss: 0.001178 Acc: 9.6210\n",
      " |~~ train@21000  Loss: 0.001150 Acc: 9.7000\n",
      " |~~ train@22000  Loss: 0.001151 Acc: 9.7300\n",
      " |~~ train@23000  Loss: 0.001162 Acc: 9.6540\n",
      " |~~ train@24000  Loss: 0.001155 Acc: 9.6880\n",
      " |~~ train@25000  Loss: 0.001132 Acc: 9.6540\n",
      " |~~ train@26000  Loss: 0.001153 Acc: 9.6460\n",
      " |~~ train@27000  Loss: 0.001153 Acc: 9.5930\n",
      " |~~ train@28000  Loss: 0.001150 Acc: 9.6470\n",
      " |~~ train@29000  Loss: 0.001169 Acc: 9.6400\n",
      " |~~ train@30000  Loss: 0.001168 Acc: 9.6260\n",
      " |~~ train@31000  Loss: 0.001148 Acc: 9.6830\n",
      " |~~ train@32000  Loss: 0.001160 Acc: 9.6470\n",
      " |~~ train@33000  Loss: 0.001164 Acc: 9.6320\n",
      " |~~ train@34000  Loss: 0.001160 Acc: 9.6370\n",
      " |~~ train@35000  Loss: 0.001144 Acc: 9.6320\n",
      " |~~ train@36000  Loss: 0.001144 Acc: 9.6480\n",
      " |~~ train@37000  Loss: 0.001138 Acc: 9.6610\n",
      " |~~ train@38000  Loss: 0.001152 Acc: 9.6920\n",
      " |~~ train@39000  Loss: 0.001148 Acc: 9.6970\n",
      " |~~ train@40000  Loss: 0.001116 Acc: 9.7000\n",
      " |~~ train@41000  Loss: 0.001118 Acc: 9.7360\n",
      " |~~ train@42000  Loss: 0.001144 Acc: 9.6690\n",
      " |~~ train@43000  Loss: 0.001171 Acc: 9.6700\n",
      " |~~ train@44000  Loss: 0.001150 Acc: 9.6430\n",
      " |~~ train@45000  Loss: 0.001141 Acc: 9.7440\n",
      " |~~ train@46000  Loss: 0.001164 Acc: 9.7340\n",
      " |~~ train@47000  Loss: 0.001156 Acc: 9.6770\n",
      " |~~ train@48000  Loss: 0.001145 Acc: 9.7030\n",
      " |~~ train@49000  Loss: 0.001162 Acc: 9.6870\n",
      " |~~ train@50000  Loss: 0.001177 Acc: 9.6280\n",
      " |~~ train@51000  Loss: 0.001124 Acc: 9.7440\n",
      " |~~ train@52000  Loss: 0.001168 Acc: 9.6740\n",
      " |~~ train@53000  Loss: 0.001156 Acc: 9.7250\n",
      " |~~ train@54000  Loss: 0.001154 Acc: 9.6510\n",
      " |~~ train@55000  Loss: 0.001134 Acc: 9.7050\n",
      " |~~ train@56000  Loss: 0.001133 Acc: 9.6870\n",
      " |~~ train@57000  Loss: 0.001141 Acc: 9.6340\n",
      " |~~ train@58000  Loss: 0.001149 Acc: 9.6560\n",
      " |~~ train@59000  Loss: 0.001147 Acc: 9.6810\n",
      " |~~ train@60000  Loss: 0.001140 Acc: 9.6310\n",
      " |~~ train@61000  Loss: 0.001144 Acc: 9.6530\n",
      " |~~ train@62000  Loss: 0.001182 Acc: 9.6310\n",
      " |~~ train@63000  Loss: 0.001177 Acc: 9.6320\n",
      " |~~ train@64000  Loss: 0.001169 Acc: 9.6870\n",
      " |~~ train@65000  Loss: 0.001123 Acc: 9.7420\n",
      " |~~ train@66000  Loss: 0.001143 Acc: 9.6780\n",
      " |~~ train@67000  Loss: 0.001148 Acc: 9.7000\n",
      " |~~ train@68000  Loss: 0.001151 Acc: 9.6630\n",
      " |~~ train@69000  Loss: 0.001129 Acc: 9.7400\n",
      " |~~ train@70000  Loss: 0.001165 Acc: 9.6680\n",
      " |~~ train@71000  Loss: 0.001148 Acc: 9.7130\n",
      " |~~ train@72000  Loss: 0.001162 Acc: 9.6790\n",
      " |~~ train@73000  Loss: 0.001172 Acc: 9.6910\n",
      " |~~ train@74000  Loss: 0.001142 Acc: 9.7270\n",
      " |~~ train@75000  Loss: 0.001166 Acc: 9.7470\n",
      " |~~ train@76000  Loss: 0.001139 Acc: 9.7810\n",
      " |~~ train@77000  Loss: 0.001133 Acc: 9.8020\n",
      " |~~ train@78000  Loss: 0.001113 Acc: 9.7950\n",
      " |~~ train@79000  Loss: 0.001155 Acc: 9.7710\n",
      " |~~ train@80000  Loss: 0.001139 Acc: 9.8230\n",
      " |~~ train@81000  Loss: 0.001122 Acc: 9.7930\n",
      " |~~ train@82000  Loss: 0.001152 Acc: 9.7850\n",
      " |~~ train@83000  Loss: 0.001122 Acc: 9.8290\n",
      " |~~ train@84000  Loss: 0.001145 Acc: 9.7540\n",
      " |~~ train@85000  Loss: 0.001121 Acc: 9.7650\n",
      " |~~ train@86000  Loss: 0.001129 Acc: 9.7930\n",
      " |~~ train@87000  Loss: 0.001146 Acc: 9.7960\n",
      " |~~ train@88000  Loss: 0.001148 Acc: 9.7610\n",
      " |~~ train@89000  Loss: 0.001127 Acc: 9.8080\n",
      " |~~ train@90000  Loss: 0.001157 Acc: 9.7480\n",
      " |~~ train@91000  Loss: 0.001130 Acc: 9.8150\n",
      " |~~ train@92000  Loss: 0.001147 Acc: 9.8020\n",
      " |~~ train@93000  Loss: 0.001164 Acc: 9.7810\n",
      " |~~ train@94000  Loss: 0.001158 Acc: 9.7450\n",
      " |~~ train@95000  Loss: 0.001131 Acc: 9.8560\n",
      " |~~ train@96000  Loss: 0.001139 Acc: 9.8060\n",
      " |~~ train@97000  Loss: 0.001121 Acc: 9.7830\n",
      " |~~ train@98000  Loss: 0.001142 Acc: 9.8120\n",
      " |~~ train@99000  Loss: 0.001109 Acc: 9.8170\n",
      " |~~ train@100000  Loss: 0.001128 Acc: 9.7520\n",
      " |~~ train@100908  Loss: 0.001237 Acc: 9.8106\n",
      "train  Loss: 0.001149 Acc: 9.7074\n",
      " |~~ val@1000  Loss: 0.001127 Acc: 9.7740\n",
      " |~~ val@2000  Loss: 0.001116 Acc: 9.7530\n",
      " |~~ val@3000  Loss: 0.001122 Acc: 9.7700\n",
      " |~~ val@4000  Loss: 0.001136 Acc: 9.7120\n",
      " |~~ val@5000  Loss: 0.001118 Acc: 9.7740\n",
      " |~~ val@6000  Loss: 0.001118 Acc: 9.8220\n",
      " |~~ val@7000  Loss: 0.001143 Acc: 9.7110\n",
      " |~~ val@8000  Loss: 0.001138 Acc: 9.7590\n",
      " |~~ val@9000  Loss: 0.001139 Acc: 9.7840\n",
      " |~~ val@10000  Loss: 0.001137 Acc: 9.7450\n",
      " |~~ val@11000  Loss: 0.001137 Acc: 9.7590\n",
      " |~~ val@11212  Loss: 0.005669 Acc: 9.8113\n",
      "val  Loss: 0.001216 Acc: 9.7612\n",
      "Epoch 3/9\n",
      "----------\n",
      " |~~ train@21000  Loss: 0.001145 Acc: 9.8070\n",
      " |~~ train@22000  Loss: 0.001132 Acc: 9.8460\n",
      " |~~ train@23000  Loss: 0.001128 Acc: 9.8090\n",
      " |~~ train@24000  Loss: 0.001140 Acc: 9.7770\n",
      " |~~ train@25000  Loss: 0.001146 Acc: 9.7750\n",
      " |~~ train@26000  Loss: 0.001139 Acc: 9.7980\n",
      " |~~ train@27000  Loss: 0.001153 Acc: 9.7410\n",
      " |~~ train@28000  Loss: 0.001151 Acc: 9.7240\n",
      " |~~ train@29000  Loss: 0.001147 Acc: 9.7250\n",
      " |~~ train@30000  Loss: 0.001103 Acc: 9.7700\n",
      " |~~ train@31000  Loss: 0.001134 Acc: 9.7110\n",
      " |~~ train@32000  Loss: 0.001111 Acc: 9.7840\n",
      " |~~ train@33000  Loss: 0.001133 Acc: 9.7660\n",
      " |~~ train@34000  Loss: 0.001146 Acc: 9.8000\n",
      " |~~ train@35000  Loss: 0.001114 Acc: 9.7810\n",
      " |~~ train@36000  Loss: 0.001131 Acc: 9.8380\n",
      " |~~ train@37000  Loss: 0.001162 Acc: 9.7980\n",
      " |~~ train@38000  Loss: 0.001109 Acc: 9.8930\n",
      " |~~ train@39000  Loss: 0.001095 Acc: 9.9160\n",
      " |~~ train@40000  Loss: 0.001132 Acc: 9.8390\n",
      " |~~ train@41000  Loss: 0.001145 Acc: 9.8380\n",
      " |~~ train@42000  Loss: 0.001137 Acc: 9.7890\n",
      " |~~ train@43000  Loss: 0.001140 Acc: 9.8520\n",
      " |~~ train@44000  Loss: 0.001136 Acc: 9.8420\n",
      " |~~ train@45000  Loss: 0.001137 Acc: 9.8860\n",
      " |~~ train@46000  Loss: 0.001122 Acc: 9.8210\n",
      " |~~ train@47000  Loss: 0.001165 Acc: 9.8450\n",
      " |~~ train@48000  Loss: 0.001125 Acc: 9.8570\n",
      " |~~ train@49000  Loss: 0.001154 Acc: 9.8060\n",
      " |~~ train@50000  Loss: 0.001129 Acc: 9.8520\n",
      " |~~ train@51000  Loss: 0.001123 Acc: 9.8560\n",
      " |~~ train@52000  Loss: 0.001130 Acc: 9.8180\n",
      " |~~ train@53000  Loss: 0.001113 Acc: 9.8610\n",
      " |~~ train@54000  Loss: 0.001121 Acc: 9.8760\n",
      " |~~ train@55000  Loss: 0.001124 Acc: 9.8570\n",
      " |~~ train@56000  Loss: 0.001118 Acc: 9.8610\n",
      " |~~ train@57000  Loss: 0.001147 Acc: 9.8640\n",
      " |~~ train@58000  Loss: 0.001110 Acc: 9.8630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@59000  Loss: 0.001150 Acc: 9.8230\n",
      " |~~ train@60000  Loss: 0.001116 Acc: 9.8630\n",
      " |~~ train@61000  Loss: 0.001144 Acc: 9.8410\n",
      " |~~ train@62000  Loss: 0.001119 Acc: 9.8380\n",
      " |~~ train@63000  Loss: 0.001137 Acc: 9.7970\n",
      " |~~ train@64000  Loss: 0.001145 Acc: 9.8180\n",
      " |~~ train@65000  Loss: 0.001126 Acc: 9.8800\n",
      " |~~ train@66000  Loss: 0.001106 Acc: 9.8390\n",
      " |~~ train@67000  Loss: 0.001155 Acc: 9.7970\n",
      " |~~ train@68000  Loss: 0.001141 Acc: 9.7830\n",
      " |~~ train@69000  Loss: 0.001132 Acc: 9.7380\n",
      " |~~ train@70000  Loss: 0.001135 Acc: 9.8420\n",
      " |~~ train@71000  Loss: 0.001144 Acc: 9.8110\n",
      " |~~ train@72000  Loss: 0.001112 Acc: 9.7980\n",
      " |~~ train@73000  Loss: 0.001112 Acc: 9.8570\n",
      " |~~ train@74000  Loss: 0.001136 Acc: 9.8140\n",
      " |~~ train@75000  Loss: 0.001121 Acc: 9.8580\n",
      " |~~ train@76000  Loss: 0.001115 Acc: 9.8590\n",
      " |~~ train@77000  Loss: 0.001144 Acc: 9.8020\n",
      " |~~ train@78000  Loss: 0.001139 Acc: 9.8240\n",
      " |~~ train@79000  Loss: 0.001135 Acc: 9.8300\n",
      " |~~ train@80000  Loss: 0.001138 Acc: 9.8180\n",
      " |~~ train@81000  Loss: 0.001155 Acc: 9.7790\n",
      " |~~ train@82000  Loss: 0.001111 Acc: 9.8170\n",
      " |~~ train@83000  Loss: 0.001112 Acc: 9.7960\n",
      " |~~ train@84000  Loss: 0.001112 Acc: 9.7740\n",
      " |~~ train@85000  Loss: 0.001121 Acc: 9.7150\n",
      " |~~ train@86000  Loss: 0.001170 Acc: 9.7510\n",
      " |~~ train@87000  Loss: 0.001144 Acc: 9.7510\n",
      " |~~ train@88000  Loss: 0.001139 Acc: 9.7410\n",
      " |~~ train@89000  Loss: 0.001116 Acc: 9.7790\n",
      " |~~ train@90000  Loss: 0.001138 Acc: 9.7770\n",
      " |~~ train@91000  Loss: 0.001145 Acc: 9.7140\n",
      " |~~ train@92000  Loss: 0.001131 Acc: 9.7410\n",
      " |~~ train@93000  Loss: 0.001132 Acc: 9.7460\n",
      " |~~ train@94000  Loss: 0.001125 Acc: 9.7350\n",
      " |~~ train@95000  Loss: 0.001106 Acc: 9.7270\n",
      " |~~ train@96000  Loss: 0.001103 Acc: 9.7230\n",
      " |~~ train@97000  Loss: 0.001128 Acc: 9.7010\n",
      " |~~ train@98000  Loss: 0.001161 Acc: 9.7130\n",
      " |~~ train@99000  Loss: 0.001131 Acc: 9.7130\n",
      " |~~ train@100000  Loss: 0.001155 Acc: 9.7250\n",
      " |~~ train@100908  Loss: 0.001229 Acc: 9.7775\n",
      "train  Loss: 0.001134 Acc: 9.7972\n",
      " |~~ val@1000  Loss: 0.001131 Acc: 9.7940\n",
      " |~~ val@2000  Loss: 0.001091 Acc: 9.7580\n",
      " |~~ val@3000  Loss: 0.001133 Acc: 9.7950\n",
      " |~~ val@4000  Loss: 0.001122 Acc: 9.8440\n",
      " |~~ val@5000  Loss: 0.001115 Acc: 9.7640\n",
      " |~~ val@6000  Loss: 0.001132 Acc: 9.7100\n",
      " |~~ val@7000  Loss: 0.001118 Acc: 9.7690\n",
      " |~~ val@8000  Loss: 0.001095 Acc: 9.8590\n",
      " |~~ val@9000  Loss: 0.001123 Acc: 9.6790\n",
      " |~~ val@10000  Loss: 0.001132 Acc: 9.7460\n",
      " |~~ val@11000  Loss: 0.001118 Acc: 9.8040\n",
      " |~~ val@11212  Loss: 0.005292 Acc: 9.6651\n",
      "val  Loss: 0.001198 Acc: 9.7727\n",
      "Epoch 4/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001101 Acc: 9.8080\n",
      " |~~ train@2000  Loss: 0.001107 Acc: 9.7880\n",
      " |~~ train@3000  Loss: 0.001151 Acc: 9.7780\n",
      " |~~ train@4000  Loss: 0.001139 Acc: 9.7730\n",
      " |~~ train@5000  Loss: 0.001109 Acc: 9.8430\n",
      " |~~ train@6000  Loss: 0.001159 Acc: 9.8190\n",
      " |~~ train@7000  Loss: 0.001090 Acc: 9.9110\n",
      " |~~ train@8000  Loss: 0.001136 Acc: 9.8510\n",
      " |~~ train@9000  Loss: 0.001121 Acc: 9.8580\n",
      " |~~ train@10000  Loss: 0.001138 Acc: 9.8420\n",
      " |~~ train@11000  Loss: 0.001133 Acc: 9.8490\n",
      " |~~ train@12000  Loss: 0.001116 Acc: 9.8590\n",
      " |~~ train@13000  Loss: 0.001102 Acc: 9.8630\n",
      " |~~ train@14000  Loss: 0.001147 Acc: 9.8220\n",
      " |~~ train@15000  Loss: 0.001120 Acc: 9.8870\n",
      " |~~ train@16000  Loss: 0.001113 Acc: 9.8650\n",
      " |~~ train@17000  Loss: 0.001160 Acc: 9.7710\n",
      " |~~ train@18000  Loss: 0.001126 Acc: 9.8440\n",
      " |~~ train@19000  Loss: 0.001108 Acc: 9.8920\n",
      " |~~ train@20000  Loss: 0.001134 Acc: 9.8310\n",
      " |~~ train@21000  Loss: 0.001130 Acc: 9.8480\n",
      " |~~ train@22000  Loss: 0.001137 Acc: 9.8880\n",
      " |~~ train@23000  Loss: 0.001132 Acc: 9.8690\n",
      " |~~ train@24000  Loss: 0.001110 Acc: 9.9000\n",
      " |~~ train@25000  Loss: 0.001135 Acc: 9.8220\n",
      " |~~ train@26000  Loss: 0.001113 Acc: 9.8910\n",
      " |~~ train@27000  Loss: 0.001149 Acc: 9.8640\n",
      " |~~ train@28000  Loss: 0.001137 Acc: 9.9020\n",
      " |~~ train@29000  Loss: 0.001124 Acc: 9.8470\n",
      " |~~ train@30000  Loss: 0.001118 Acc: 9.8540\n",
      " |~~ train@31000  Loss: 0.001124 Acc: 9.8610\n",
      " |~~ train@32000  Loss: 0.001103 Acc: 9.8920\n",
      " |~~ train@33000  Loss: 0.001120 Acc: 9.8700\n",
      " |~~ train@34000  Loss: 0.001129 Acc: 9.9070\n",
      " |~~ train@35000  Loss: 0.001106 Acc: 9.8760\n",
      " |~~ train@36000  Loss: 0.001100 Acc: 9.9450\n",
      " |~~ train@37000  Loss: 0.001121 Acc: 9.8920\n",
      " |~~ train@38000  Loss: 0.001097 Acc: 9.9120\n",
      " |~~ train@39000  Loss: 0.001140 Acc: 9.9300\n",
      " |~~ train@40000  Loss: 0.001122 Acc: 9.9070\n",
      " |~~ train@41000  Loss: 0.001113 Acc: 9.9190\n",
      " |~~ train@42000  Loss: 0.001137 Acc: 9.8800\n",
      " |~~ train@43000  Loss: 0.001093 Acc: 9.9310\n",
      " |~~ train@44000  Loss: 0.001117 Acc: 9.9470\n",
      " |~~ train@45000  Loss: 0.001127 Acc: 9.8700\n",
      " |~~ train@46000  Loss: 0.001108 Acc: 9.9310\n",
      " |~~ train@47000  Loss: 0.001136 Acc: 9.8860\n",
      " |~~ train@48000  Loss: 0.001112 Acc: 9.8740\n",
      " |~~ train@49000  Loss: 0.001127 Acc: 9.9270\n",
      " |~~ train@50000  Loss: 0.001099 Acc: 9.9240\n",
      " |~~ train@51000  Loss: 0.001129 Acc: 9.8380\n",
      " |~~ train@52000  Loss: 0.001142 Acc: 9.8200\n",
      " |~~ train@53000  Loss: 0.001119 Acc: 9.8840\n",
      " |~~ train@54000  Loss: 0.001112 Acc: 9.8530\n",
      " |~~ train@55000  Loss: 0.001126 Acc: 9.8540\n",
      " |~~ train@56000  Loss: 0.001133 Acc: 9.7780\n",
      " |~~ train@57000  Loss: 0.001094 Acc: 9.9300\n",
      " |~~ train@58000  Loss: 0.001156 Acc: 9.8920\n",
      " |~~ train@59000  Loss: 0.001129 Acc: 9.8620\n",
      " |~~ train@60000  Loss: 0.001130 Acc: 9.8430\n",
      " |~~ train@61000  Loss: 0.001109 Acc: 9.8950\n",
      " |~~ train@62000  Loss: 0.001122 Acc: 9.8720\n",
      " |~~ train@63000  Loss: 0.001145 Acc: 9.8770\n",
      " |~~ train@64000  Loss: 0.001128 Acc: 9.8920\n",
      " |~~ train@65000  Loss: 0.001118 Acc: 9.8870\n",
      " |~~ train@66000  Loss: 0.001113 Acc: 9.8980\n",
      " |~~ train@67000  Loss: 0.001109 Acc: 9.9070\n",
      " |~~ train@68000  Loss: 0.001122 Acc: 9.9320\n",
      " |~~ train@69000  Loss: 0.001117 Acc: 9.9400\n",
      " |~~ train@70000  Loss: 0.001117 Acc: 9.9450\n",
      " |~~ train@71000  Loss: 0.001132 Acc: 9.9160\n",
      " |~~ train@72000  Loss: 0.001137 Acc: 9.9760\n",
      " |~~ train@73000  Loss: 0.001147 Acc: 9.9110\n",
      " |~~ train@74000  Loss: 0.001164 Acc: 9.9190\n",
      " |~~ train@75000  Loss: 0.001119 Acc: 9.9100\n",
      " |~~ train@76000  Loss: 0.001122 Acc: 9.9060\n",
      " |~~ train@77000  Loss: 0.001149 Acc: 9.9550\n",
      " |~~ train@78000  Loss: 0.001148 Acc: 9.9150\n",
      " |~~ train@79000  Loss: 0.001138 Acc: 9.8850\n",
      " |~~ train@80000  Loss: 0.001126 Acc: 9.8680\n",
      " |~~ train@81000  Loss: 0.001131 Acc: 9.8950\n",
      " |~~ train@82000  Loss: 0.001107 Acc: 9.8920\n",
      " |~~ train@83000  Loss: 0.001116 Acc: 9.8630\n",
      " |~~ train@84000  Loss: 0.001099 Acc: 9.8490\n",
      " |~~ train@85000  Loss: 0.001138 Acc: 9.8180\n",
      " |~~ train@86000  Loss: 0.001106 Acc: 9.8600\n",
      " |~~ train@87000  Loss: 0.001118 Acc: 9.7900\n",
      " |~~ train@88000  Loss: 0.001085 Acc: 9.8860\n",
      " |~~ train@89000  Loss: 0.001098 Acc: 9.8420\n",
      " |~~ train@90000  Loss: 0.001110 Acc: 9.8000\n",
      " |~~ train@91000  Loss: 0.001108 Acc: 9.8200\n",
      " |~~ train@92000  Loss: 0.001145 Acc: 9.7990\n",
      " |~~ train@93000  Loss: 0.001108 Acc: 9.8450\n",
      " |~~ train@94000  Loss: 0.001131 Acc: 9.8330\n",
      " |~~ train@95000  Loss: 0.001111 Acc: 9.8990\n",
      " |~~ train@96000  Loss: 0.001109 Acc: 9.8550\n",
      " |~~ train@97000  Loss: 0.001089 Acc: 9.8870\n",
      " |~~ train@98000  Loss: 0.001121 Acc: 9.8510\n",
      " |~~ train@99000  Loss: 0.001156 Acc: 9.8170\n",
      " |~~ train@100000  Loss: 0.001104 Acc: 9.8480\n",
      " |~~ train@100908  Loss: 0.001212 Acc: 9.8700\n",
      "train  Loss: 0.001124 Acc: 9.8723\n",
      " |~~ val@1000  Loss: 0.001092 Acc: 9.9710\n",
      " |~~ val@2000  Loss: 0.001105 Acc: 9.8990\n",
      " |~~ val@3000  Loss: 0.001110 Acc: 9.8240\n",
      " |~~ val@4000  Loss: 0.001095 Acc: 9.8830\n",
      " |~~ val@5000  Loss: 0.001125 Acc: 9.8450\n",
      " |~~ val@6000  Loss: 0.001104 Acc: 9.8760\n",
      " |~~ val@7000  Loss: 0.001110 Acc: 9.8330\n",
      " |~~ val@8000  Loss: 0.001100 Acc: 9.8560\n",
      " |~~ val@9000  Loss: 0.001111 Acc: 9.8630\n",
      " |~~ val@10000  Loss: 0.001129 Acc: 9.8190\n",
      " |~~ val@11000  Loss: 0.001129 Acc: 9.7960\n",
      " |~~ val@11212  Loss: 0.005216 Acc: 9.8302\n",
      "val  Loss: 0.001188 Acc: 9.8599\n",
      "Epoch 5/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001128 Acc: 9.8430\n",
      " |~~ train@2000  Loss: 0.001095 Acc: 9.8660\n",
      " |~~ train@3000  Loss: 0.001117 Acc: 9.8890\n",
      " |~~ train@4000  Loss: 0.001117 Acc: 9.8620\n",
      " |~~ train@5000  Loss: 0.001118 Acc: 9.9410\n",
      " |~~ train@6000  Loss: 0.001147 Acc: 9.8770\n",
      " |~~ train@7000  Loss: 0.001086 Acc: 9.9630\n",
      " |~~ train@8000  Loss: 0.001103 Acc: 9.8790\n",
      " |~~ train@9000  Loss: 0.001121 Acc: 9.9110\n",
      " |~~ train@10000  Loss: 0.001158 Acc: 9.8700\n",
      " |~~ train@11000  Loss: 0.001114 Acc: 9.9460\n",
      " |~~ train@12000  Loss: 0.001145 Acc: 9.8830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@13000  Loss: 0.001117 Acc: 9.9260\n",
      " |~~ train@14000  Loss: 0.001135 Acc: 9.8910\n",
      " |~~ train@15000  Loss: 0.001110 Acc: 9.9390\n",
      " |~~ train@16000  Loss: 0.001091 Acc: 9.9970\n",
      " |~~ train@17000  Loss: 0.001116 Acc: 9.9360\n",
      " |~~ train@18000  Loss: 0.001117 Acc: 9.9040\n",
      " |~~ train@19000  Loss: 0.001129 Acc: 9.8990\n",
      " |~~ train@20000  Loss: 0.001131 Acc: 9.9490\n",
      " |~~ train@21000  Loss: 0.001098 Acc: 9.9950\n",
      " |~~ train@22000  Loss: 0.001082 Acc: 9.9940\n",
      " |~~ train@23000  Loss: 0.001109 Acc: 9.9100\n",
      " |~~ train@24000  Loss: 0.001115 Acc: 9.9310\n",
      " |~~ train@25000  Loss: 0.001101 Acc: 9.9000\n",
      " |~~ train@26000  Loss: 0.001084 Acc: 9.9200\n",
      " |~~ train@27000  Loss: 0.001124 Acc: 9.8770\n",
      " |~~ train@28000  Loss: 0.001121 Acc: 9.9050\n",
      " |~~ train@29000  Loss: 0.001122 Acc: 9.8670\n",
      " |~~ train@30000  Loss: 0.001103 Acc: 9.9100\n",
      " |~~ train@31000  Loss: 0.001110 Acc: 9.8750\n",
      " |~~ train@32000  Loss: 0.001129 Acc: 9.8870\n",
      " |~~ train@33000  Loss: 0.001113 Acc: 9.8980\n",
      " |~~ train@34000  Loss: 0.001128 Acc: 9.8560\n",
      " |~~ train@35000  Loss: 0.001162 Acc: 9.8710\n",
      " |~~ train@36000  Loss: 0.001118 Acc: 9.9090\n",
      " |~~ train@37000  Loss: 0.001113 Acc: 9.9320\n",
      " |~~ train@38000  Loss: 0.001126 Acc: 9.9060\n",
      " |~~ train@39000  Loss: 0.001124 Acc: 9.9160\n",
      " |~~ train@40000  Loss: 0.001101 Acc: 9.9390\n",
      " |~~ train@41000  Loss: 0.001130 Acc: 9.9450\n",
      " |~~ train@42000  Loss: 0.001082 Acc: 9.9780\n",
      " |~~ train@43000  Loss: 0.001090 Acc: 10.0400\n",
      " |~~ train@44000  Loss: 0.001110 Acc: 9.9150\n",
      " |~~ train@45000  Loss: 0.001098 Acc: 10.0160\n",
      " |~~ train@46000  Loss: 0.001152 Acc: 9.9870\n",
      " |~~ train@47000  Loss: 0.001105 Acc: 9.9850\n",
      " |~~ train@48000  Loss: 0.001106 Acc: 9.9970\n",
      " |~~ train@49000  Loss: 0.001117 Acc: 9.9730\n",
      " |~~ train@50000  Loss: 0.001134 Acc: 9.9260\n",
      " |~~ train@51000  Loss: 0.001134 Acc: 9.9560\n",
      " |~~ train@52000  Loss: 0.001101 Acc: 9.9960\n",
      " |~~ train@53000  Loss: 0.001119 Acc: 9.9890\n",
      " |~~ train@54000  Loss: 0.001097 Acc: 9.9990\n",
      " |~~ train@55000  Loss: 0.001102 Acc: 9.9540\n",
      " |~~ train@56000  Loss: 0.001095 Acc: 9.9950\n",
      " |~~ train@57000  Loss: 0.001116 Acc: 9.9720\n",
      " |~~ train@58000  Loss: 0.001114 Acc: 9.9510\n",
      " |~~ train@59000  Loss: 0.001127 Acc: 9.9550\n",
      " |~~ train@60000  Loss: 0.001103 Acc: 9.9250\n",
      " |~~ train@61000  Loss: 0.001121 Acc: 9.9240\n",
      " |~~ train@62000  Loss: 0.001129 Acc: 9.8770\n",
      " |~~ train@63000  Loss: 0.001112 Acc: 9.9680\n",
      " |~~ train@64000  Loss: 0.001093 Acc: 10.0070\n",
      " |~~ train@65000  Loss: 0.001085 Acc: 9.9710\n",
      " |~~ train@66000  Loss: 0.001110 Acc: 9.9130\n",
      " |~~ train@67000  Loss: 0.001098 Acc: 9.9580\n",
      " |~~ train@68000  Loss: 0.001086 Acc: 9.9700\n",
      " |~~ train@69000  Loss: 0.001118 Acc: 10.0150\n",
      " |~~ train@70000  Loss: 0.001161 Acc: 9.9250\n",
      " |~~ train@71000  Loss: 0.001098 Acc: 9.9700\n",
      " |~~ train@72000  Loss: 0.001111 Acc: 9.9460\n",
      " |~~ train@73000  Loss: 0.001114 Acc: 9.8880\n",
      " |~~ train@74000  Loss: 0.001112 Acc: 9.9400\n",
      " |~~ train@75000  Loss: 0.001130 Acc: 9.9330\n",
      " |~~ train@76000  Loss: 0.001086 Acc: 9.9510\n",
      " |~~ train@77000  Loss: 0.001087 Acc: 9.9820\n",
      " |~~ train@78000  Loss: 0.001110 Acc: 9.8700\n",
      " |~~ train@79000  Loss: 0.001115 Acc: 9.8640\n",
      " |~~ train@80000  Loss: 0.001116 Acc: 9.8710\n",
      " |~~ train@81000  Loss: 0.001119 Acc: 9.8850\n",
      " |~~ train@82000  Loss: 0.001099 Acc: 9.9050\n",
      " |~~ train@83000  Loss: 0.001121 Acc: 9.9410\n",
      " |~~ train@84000  Loss: 0.001114 Acc: 9.8650\n",
      " |~~ train@85000  Loss: 0.001142 Acc: 9.9090\n",
      " |~~ train@86000  Loss: 0.001131 Acc: 9.8680\n",
      " |~~ train@87000  Loss: 0.001115 Acc: 9.9380\n",
      " |~~ train@88000  Loss: 0.001122 Acc: 9.8810\n",
      " |~~ train@89000  Loss: 0.001113 Acc: 9.8910\n",
      " |~~ train@90000  Loss: 0.001093 Acc: 9.9160\n",
      " |~~ train@91000  Loss: 0.001080 Acc: 9.9540\n",
      " |~~ train@92000  Loss: 0.001135 Acc: 9.9290\n",
      " |~~ train@93000  Loss: 0.001115 Acc: 9.9170\n",
      " |~~ train@94000  Loss: 0.001120 Acc: 9.8740\n",
      " |~~ train@95000  Loss: 0.001124 Acc: 9.8700\n",
      " |~~ train@96000  Loss: 0.001097 Acc: 9.8910\n",
      " |~~ train@97000  Loss: 0.001116 Acc: 9.8520\n",
      " |~~ train@98000  Loss: 0.001106 Acc: 9.8680\n",
      " |~~ train@99000  Loss: 0.001142 Acc: 9.8350\n",
      " |~~ train@100000  Loss: 0.001097 Acc: 9.9110\n",
      " |~~ train@100908  Loss: 0.001221 Acc: 9.8205\n",
      "train  Loss: 0.001115 Acc: 9.9237\n",
      " |~~ val@1000  Loss: 0.001117 Acc: 9.8200\n",
      " |~~ val@2000  Loss: 0.001121 Acc: 9.8510\n",
      " |~~ val@3000  Loss: 0.001106 Acc: 9.8160\n",
      " |~~ val@4000  Loss: 0.001096 Acc: 9.8700\n",
      " |~~ val@5000  Loss: 0.001122 Acc: 9.8620\n",
      " |~~ val@6000  Loss: 0.001083 Acc: 9.9000\n",
      " |~~ val@7000  Loss: 0.001100 Acc: 9.9060\n",
      " |~~ val@8000  Loss: 0.001098 Acc: 9.9280\n",
      " |~~ val@9000  Loss: 0.001082 Acc: 9.9200\n",
      " |~~ val@10000  Loss: 0.001093 Acc: 9.7970\n",
      " |~~ val@11000  Loss: 0.001095 Acc: 9.9000\n",
      " |~~ val@11212  Loss: 0.005398 Acc: 9.8160\n",
      "val  Loss: 0.001182 Acc: 9.8690\n",
      "Epoch 6/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001099 Acc: 9.8840\n",
      " |~~ train@2000  Loss: 0.001114 Acc: 9.8970\n",
      " |~~ train@3000  Loss: 0.001107 Acc: 9.8840\n",
      " |~~ train@4000  Loss: 0.001079 Acc: 9.8870\n",
      " |~~ train@5000  Loss: 0.001104 Acc: 9.8890\n",
      " |~~ train@6000  Loss: 0.001112 Acc: 9.8720\n",
      " |~~ train@7000  Loss: 0.001104 Acc: 9.9020\n",
      " |~~ train@8000  Loss: 0.001119 Acc: 9.9350\n",
      " |~~ train@9000  Loss: 0.001121 Acc: 9.8920\n",
      " |~~ train@10000  Loss: 0.001105 Acc: 9.9660\n",
      " |~~ train@11000  Loss: 0.001104 Acc: 10.0070\n",
      " |~~ train@12000  Loss: 0.001094 Acc: 9.9450\n",
      " |~~ train@13000  Loss: 0.001120 Acc: 9.9600\n",
      " |~~ train@14000  Loss: 0.001127 Acc: 10.0050\n",
      " |~~ train@15000  Loss: 0.001113 Acc: 9.9630\n",
      " |~~ train@16000  Loss: 0.001094 Acc: 9.9780\n",
      " |~~ train@17000  Loss: 0.001094 Acc: 10.0700\n",
      " |~~ train@18000  Loss: 0.001098 Acc: 10.0470\n",
      " |~~ train@19000  Loss: 0.001123 Acc: 10.0040\n",
      " |~~ train@20000  Loss: 0.001107 Acc: 10.0890\n",
      " |~~ train@21000  Loss: 0.001106 Acc: 10.0250\n",
      " |~~ train@22000  Loss: 0.001112 Acc: 10.0600\n",
      " |~~ train@23000  Loss: 0.001113 Acc: 10.0580\n",
      " |~~ train@24000  Loss: 0.001146 Acc: 10.0360\n",
      " |~~ train@25000  Loss: 0.001110 Acc: 10.0540\n",
      " |~~ train@26000  Loss: 0.001107 Acc: 10.0030\n",
      " |~~ train@27000  Loss: 0.001099 Acc: 10.0370\n",
      " |~~ train@28000  Loss: 0.001112 Acc: 10.0030\n",
      " |~~ train@29000  Loss: 0.001076 Acc: 9.9560\n",
      " |~~ train@30000  Loss: 0.001126 Acc: 9.9850\n",
      " |~~ train@31000  Loss: 0.001102 Acc: 9.9990\n",
      " |~~ train@32000  Loss: 0.001137 Acc: 9.9220\n",
      " |~~ train@33000  Loss: 0.001101 Acc: 9.9610\n",
      " |~~ train@34000  Loss: 0.001118 Acc: 9.9250\n",
      " |~~ train@35000  Loss: 0.001101 Acc: 9.9230\n",
      " |~~ train@36000  Loss: 0.001102 Acc: 9.9440\n",
      " |~~ train@37000  Loss: 0.001117 Acc: 9.8940\n",
      " |~~ train@38000  Loss: 0.001119 Acc: 9.9090\n",
      " |~~ train@39000  Loss: 0.001110 Acc: 9.9390\n",
      " |~~ train@40000  Loss: 0.001090 Acc: 9.9260\n",
      " |~~ train@41000  Loss: 0.001108 Acc: 9.8960\n",
      " |~~ train@42000  Loss: 0.001103 Acc: 9.9290\n",
      " |~~ train@43000  Loss: 0.001114 Acc: 9.9290\n",
      " |~~ train@44000  Loss: 0.001110 Acc: 9.9530\n",
      " |~~ train@45000  Loss: 0.001134 Acc: 9.9340\n",
      " |~~ train@46000  Loss: 0.001110 Acc: 9.8760\n",
      " |~~ train@47000  Loss: 0.001097 Acc: 9.9060\n",
      " |~~ train@48000  Loss: 0.001138 Acc: 9.8840\n",
      " |~~ train@49000  Loss: 0.001095 Acc: 9.9360\n",
      " |~~ train@50000  Loss: 0.001079 Acc: 9.9620\n",
      " |~~ train@51000  Loss: 0.001078 Acc: 10.0010\n",
      " |~~ train@52000  Loss: 0.001139 Acc: 9.8800\n",
      " |~~ train@53000  Loss: 0.001107 Acc: 9.9390\n",
      " |~~ train@54000  Loss: 0.001112 Acc: 9.9530\n",
      " |~~ train@55000  Loss: 0.001119 Acc: 9.9210\n",
      " |~~ train@56000  Loss: 0.001106 Acc: 9.9520\n",
      " |~~ train@57000  Loss: 0.001100 Acc: 9.9620\n",
      " |~~ train@58000  Loss: 0.001129 Acc: 9.9510\n",
      " |~~ train@59000  Loss: 0.001119 Acc: 9.9360\n",
      " |~~ train@60000  Loss: 0.001078 Acc: 10.0040\n",
      " |~~ train@61000  Loss: 0.001087 Acc: 10.0180\n",
      " |~~ train@62000  Loss: 0.001097 Acc: 9.9710\n",
      " |~~ train@63000  Loss: 0.001097 Acc: 9.9920\n",
      " |~~ train@64000  Loss: 0.001086 Acc: 9.9810\n",
      " |~~ train@65000  Loss: 0.001085 Acc: 10.0570\n",
      " |~~ train@66000  Loss: 0.001125 Acc: 9.9890\n",
      " |~~ train@67000  Loss: 0.001120 Acc: 9.9950\n",
      " |~~ train@68000  Loss: 0.001125 Acc: 10.0060\n",
      " |~~ train@69000  Loss: 0.001108 Acc: 10.0160\n",
      " |~~ train@70000  Loss: 0.001122 Acc: 9.9640\n",
      " |~~ train@71000  Loss: 0.001099 Acc: 9.9570\n",
      " |~~ train@72000  Loss: 0.001095 Acc: 10.0050\n",
      " |~~ train@73000  Loss: 0.001085 Acc: 10.0200\n",
      " |~~ train@74000  Loss: 0.001110 Acc: 10.0830\n",
      " |~~ train@75000  Loss: 0.001120 Acc: 9.9570\n",
      " |~~ train@76000  Loss: 0.001106 Acc: 10.0450\n",
      " |~~ train@77000  Loss: 0.001081 Acc: 10.0150\n",
      " |~~ train@78000  Loss: 0.001103 Acc: 9.9630\n",
      " |~~ train@79000  Loss: 0.001096 Acc: 9.9560\n",
      " |~~ train@80000  Loss: 0.001093 Acc: 9.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@81000  Loss: 0.001106 Acc: 9.9930\n",
      " |~~ train@82000  Loss: 0.001097 Acc: 10.0320\n",
      " |~~ train@83000  Loss: 0.001115 Acc: 9.9370\n",
      " |~~ train@84000  Loss: 0.001094 Acc: 10.0190\n",
      " |~~ train@85000  Loss: 0.001090 Acc: 9.9300\n",
      " |~~ train@86000  Loss: 0.001092 Acc: 9.9530\n",
      " |~~ train@87000  Loss: 0.001136 Acc: 9.9410\n",
      " |~~ train@88000  Loss: 0.001089 Acc: 9.8630\n",
      " |~~ train@89000  Loss: 0.001097 Acc: 10.0090\n",
      " |~~ train@90000  Loss: 0.001117 Acc: 9.9230\n",
      " |~~ train@91000  Loss: 0.001126 Acc: 9.8660\n",
      " |~~ train@92000  Loss: 0.001116 Acc: 9.8850\n",
      " |~~ train@93000  Loss: 0.001110 Acc: 9.8760\n",
      " |~~ train@94000  Loss: 0.001079 Acc: 9.9510\n",
      " |~~ train@95000  Loss: 0.001106 Acc: 9.9070\n",
      " |~~ train@96000  Loss: 0.001105 Acc: 9.8920\n",
      " |~~ train@97000  Loss: 0.001102 Acc: 9.9430\n",
      " |~~ train@98000  Loss: 0.001111 Acc: 9.9360\n",
      " |~~ train@99000  Loss: 0.001131 Acc: 9.9210\n",
      " |~~ train@100000  Loss: 0.001107 Acc: 9.9430\n",
      " |~~ train@100908  Loss: 0.001222 Acc: 9.9791\n",
      "train  Loss: 0.001108 Acc: 9.9603\n",
      " |~~ val@1000  Loss: 0.001100 Acc: 10.0080\n",
      " |~~ val@2000  Loss: 0.001107 Acc: 9.9690\n",
      " |~~ val@3000  Loss: 0.001076 Acc: 10.0290\n",
      " |~~ val@4000  Loss: 0.001115 Acc: 9.8240\n",
      " |~~ val@5000  Loss: 0.001092 Acc: 10.0570\n",
      " |~~ val@6000  Loss: 0.001076 Acc: 9.9930\n",
      " |~~ val@7000  Loss: 0.001088 Acc: 9.9680\n",
      " |~~ val@8000  Loss: 0.001089 Acc: 9.9840\n",
      " |~~ val@9000  Loss: 0.001098 Acc: 9.8800\n",
      " |~~ val@10000  Loss: 0.001104 Acc: 9.9600\n",
      " |~~ val@11000  Loss: 0.001109 Acc: 9.9000\n",
      " |~~ val@11212  Loss: 0.005140 Acc: 10.0283\n",
      "val  Loss: 0.001172 Acc: 9.9624\n",
      "Epoch 7/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001095 Acc: 10.0080\n",
      " |~~ train@2000  Loss: 0.001104 Acc: 9.9760\n",
      " |~~ train@3000  Loss: 0.001115 Acc: 9.9820\n",
      " |~~ train@4000  Loss: 0.001088 Acc: 9.9670\n",
      " |~~ train@5000  Loss: 0.001112 Acc: 9.9240\n",
      " |~~ train@6000  Loss: 0.001103 Acc: 9.9540\n",
      " |~~ train@7000  Loss: 0.001105 Acc: 10.0010\n",
      " |~~ train@8000  Loss: 0.001105 Acc: 10.0090\n",
      " |~~ train@9000  Loss: 0.001091 Acc: 10.0230\n",
      " |~~ train@10000  Loss: 0.001082 Acc: 9.9970\n",
      " |~~ train@11000  Loss: 0.001123 Acc: 9.9620\n",
      " |~~ train@12000  Loss: 0.001105 Acc: 10.0180\n",
      " |~~ train@13000  Loss: 0.001115 Acc: 9.9240\n",
      " |~~ train@14000  Loss: 0.001141 Acc: 9.8380\n",
      " |~~ train@15000  Loss: 0.001142 Acc: 9.9280\n",
      " |~~ train@16000  Loss: 0.001099 Acc: 10.0070\n",
      " |~~ train@17000  Loss: 0.001100 Acc: 9.9570\n",
      " |~~ train@18000  Loss: 0.001101 Acc: 9.9730\n",
      " |~~ train@19000  Loss: 0.001091 Acc: 9.9800\n",
      " |~~ train@20000  Loss: 0.001086 Acc: 9.9840\n",
      " |~~ train@21000  Loss: 0.001094 Acc: 10.0010\n",
      " |~~ train@22000  Loss: 0.001114 Acc: 9.9670\n",
      " |~~ train@23000  Loss: 0.001070 Acc: 9.9940\n",
      " |~~ train@24000  Loss: 0.001091 Acc: 9.9750\n",
      " |~~ train@25000  Loss: 0.001098 Acc: 9.9680\n",
      " |~~ train@26000  Loss: 0.001107 Acc: 9.9370\n",
      " |~~ train@27000  Loss: 0.001090 Acc: 9.9640\n",
      " |~~ train@28000  Loss: 0.001110 Acc: 9.9310\n",
      " |~~ train@29000  Loss: 0.001103 Acc: 9.9830\n",
      " |~~ train@30000  Loss: 0.001102 Acc: 10.0170\n",
      " |~~ train@31000  Loss: 0.001106 Acc: 9.9940\n",
      " |~~ train@32000  Loss: 0.001078 Acc: 10.0260\n",
      " |~~ train@33000  Loss: 0.001143 Acc: 9.8900\n",
      " |~~ train@34000  Loss: 0.001081 Acc: 10.0270\n",
      " |~~ train@35000  Loss: 0.001078 Acc: 9.9890\n",
      " |~~ train@36000  Loss: 0.001095 Acc: 9.9540\n",
      " |~~ train@37000  Loss: 0.001082 Acc: 10.0350\n",
      " |~~ train@38000  Loss: 0.001078 Acc: 9.9910\n",
      " |~~ train@39000  Loss: 0.001113 Acc: 9.9900\n",
      " |~~ train@40000  Loss: 0.001109 Acc: 9.9470\n",
      " |~~ train@41000  Loss: 0.001107 Acc: 9.9630\n",
      " |~~ train@42000  Loss: 0.001080 Acc: 9.9820\n",
      " |~~ train@43000  Loss: 0.001097 Acc: 9.9680\n",
      " |~~ train@44000  Loss: 0.001112 Acc: 9.9430\n",
      " |~~ train@45000  Loss: 0.001098 Acc: 9.9530\n",
      " |~~ train@46000  Loss: 0.001112 Acc: 9.9930\n",
      " |~~ train@47000  Loss: 0.001114 Acc: 9.9160\n",
      " |~~ train@48000  Loss: 0.001123 Acc: 9.9860\n",
      " |~~ train@49000  Loss: 0.001131 Acc: 9.9850\n",
      " |~~ train@50000  Loss: 0.001123 Acc: 9.8850\n",
      " |~~ train@51000  Loss: 0.001106 Acc: 9.9510\n",
      " |~~ train@52000  Loss: 0.001113 Acc: 9.9270\n",
      " |~~ train@53000  Loss: 0.001085 Acc: 10.0160\n",
      " |~~ train@54000  Loss: 0.001111 Acc: 9.9400\n",
      " |~~ train@55000  Loss: 0.001102 Acc: 9.9310\n",
      " |~~ train@56000  Loss: 0.001097 Acc: 9.9830\n",
      " |~~ train@57000  Loss: 0.001123 Acc: 9.9760\n",
      " |~~ train@58000  Loss: 0.001119 Acc: 9.9360\n",
      " |~~ train@59000  Loss: 0.001091 Acc: 9.9570\n",
      " |~~ train@60000  Loss: 0.001122 Acc: 9.9380\n",
      " |~~ train@61000  Loss: 0.001088 Acc: 9.9320\n",
      " |~~ train@62000  Loss: 0.001118 Acc: 9.9190\n",
      " |~~ train@63000  Loss: 0.001123 Acc: 9.8900\n",
      " |~~ train@64000  Loss: 0.001103 Acc: 9.9560\n",
      " |~~ train@65000  Loss: 0.001118 Acc: 9.9190\n",
      " |~~ train@66000  Loss: 0.001118 Acc: 9.9400\n",
      " |~~ train@67000  Loss: 0.001113 Acc: 9.9570\n",
      " |~~ train@68000  Loss: 0.001080 Acc: 9.9920\n",
      " |~~ train@69000  Loss: 0.001110 Acc: 9.9660\n",
      " |~~ train@70000  Loss: 0.001090 Acc: 10.0150\n",
      " |~~ train@71000  Loss: 0.001104 Acc: 9.9110\n",
      " |~~ train@72000  Loss: 0.001107 Acc: 9.9550\n",
      " |~~ train@73000  Loss: 0.001074 Acc: 9.9260\n",
      " |~~ train@74000  Loss: 0.001103 Acc: 9.9450\n",
      " |~~ train@75000  Loss: 0.001078 Acc: 9.9830\n",
      " |~~ train@76000  Loss: 0.001094 Acc: 9.9290\n",
      " |~~ train@77000  Loss: 0.001111 Acc: 9.9210\n",
      " |~~ train@78000  Loss: 0.001112 Acc: 9.8620\n",
      " |~~ train@79000  Loss: 0.001083 Acc: 9.9880\n",
      " |~~ train@80000  Loss: 0.001104 Acc: 9.9560\n",
      " |~~ train@81000  Loss: 0.001105 Acc: 9.9560\n",
      " |~~ train@82000  Loss: 0.001086 Acc: 9.9860\n",
      " |~~ train@83000  Loss: 0.001095 Acc: 9.9720\n",
      " |~~ train@84000  Loss: 0.001095 Acc: 10.0010\n",
      " |~~ train@85000  Loss: 0.001099 Acc: 9.9930\n",
      " |~~ train@86000  Loss: 0.001096 Acc: 10.0250\n",
      " |~~ train@87000  Loss: 0.001085 Acc: 10.0200\n",
      " |~~ train@88000  Loss: 0.001089 Acc: 9.9910\n",
      " |~~ train@89000  Loss: 0.001115 Acc: 9.9500\n",
      " |~~ train@90000  Loss: 0.001110 Acc: 9.9800\n",
      " |~~ train@91000  Loss: 0.001082 Acc: 9.9220\n",
      " |~~ train@92000  Loss: 0.001103 Acc: 9.9530\n",
      " |~~ train@93000  Loss: 0.001107 Acc: 10.0320\n",
      " |~~ train@94000  Loss: 0.001089 Acc: 9.9910\n",
      " |~~ train@95000  Loss: 0.001103 Acc: 9.9250\n",
      " |~~ train@96000  Loss: 0.001125 Acc: 9.9620\n",
      " |~~ train@97000  Loss: 0.001130 Acc: 9.9210\n",
      " |~~ train@98000  Loss: 0.001095 Acc: 9.9630\n",
      " |~~ train@99000  Loss: 0.001108 Acc: 9.9610\n",
      " |~~ train@100000  Loss: 0.001106 Acc: 9.9750\n",
      " |~~ train@100908  Loss: 0.001242 Acc: 9.9670\n",
      "train  Loss: 0.001104 Acc: 9.9646\n",
      " |~~ train@1000  Loss: 0.001083 Acc: 10.0450\n",
      " |~~ train@2000  Loss: 0.001085 Acc: 10.0460\n",
      " |~~ train@3000  Loss: 0.001106 Acc: 9.9820\n",
      " |~~ train@4000  Loss: 0.001121 Acc: 10.0230\n",
      " |~~ train@5000  Loss: 0.001106 Acc: 9.9870\n",
      " |~~ train@6000  Loss: 0.001090 Acc: 9.9920\n",
      " |~~ train@7000  Loss: 0.001081 Acc: 10.0300\n",
      " |~~ train@8000  Loss: 0.001083 Acc: 10.0130\n",
      " |~~ train@9000  Loss: 0.001106 Acc: 9.9380\n",
      " |~~ train@10000  Loss: 0.001086 Acc: 10.0120\n",
      " |~~ train@11000  Loss: 0.001135 Acc: 9.9690\n",
      " |~~ train@12000  Loss: 0.001086 Acc: 10.0270\n",
      " |~~ train@13000  Loss: 0.001090 Acc: 10.0620\n",
      " |~~ train@14000  Loss: 0.001120 Acc: 9.9780\n",
      " |~~ train@15000  Loss: 0.001127 Acc: 9.9950\n",
      " |~~ train@16000  Loss: 0.001061 Acc: 10.0810\n",
      " |~~ train@17000  Loss: 0.001107 Acc: 9.9620\n",
      " |~~ train@18000  Loss: 0.001094 Acc: 10.0330\n",
      " |~~ train@19000  Loss: 0.001088 Acc: 10.0630\n",
      " |~~ train@20000  Loss: 0.001126 Acc: 9.9640\n",
      " |~~ train@21000  Loss: 0.001111 Acc: 9.9770\n",
      " |~~ train@22000  Loss: 0.001129 Acc: 9.9560\n",
      " |~~ train@23000  Loss: 0.001101 Acc: 9.9850\n",
      " |~~ train@24000  Loss: 0.001098 Acc: 9.9870\n",
      " |~~ train@25000  Loss: 0.001076 Acc: 10.0620\n",
      " |~~ train@26000  Loss: 0.001113 Acc: 9.9950\n",
      " |~~ train@27000  Loss: 0.001109 Acc: 9.9940\n",
      " |~~ train@28000  Loss: 0.001104 Acc: 9.9940\n",
      " |~~ train@29000  Loss: 0.001137 Acc: 9.9450\n",
      " |~~ train@30000  Loss: 0.001109 Acc: 10.0130\n",
      " |~~ train@31000  Loss: 0.001097 Acc: 9.9800\n",
      " |~~ train@32000  Loss: 0.001110 Acc: 9.9360\n",
      " |~~ train@33000  Loss: 0.001104 Acc: 10.0200\n",
      " |~~ train@34000  Loss: 0.001101 Acc: 10.0140\n",
      " |~~ train@35000  Loss: 0.001080 Acc: 10.0200\n",
      " |~~ train@36000  Loss: 0.001091 Acc: 10.0020\n",
      " |~~ train@37000  Loss: 0.001092 Acc: 10.0020\n",
      " |~~ train@38000  Loss: 0.001136 Acc: 9.9260\n",
      " |~~ train@39000  Loss: 0.001103 Acc: 10.0120\n",
      " |~~ train@40000  Loss: 0.001118 Acc: 9.9670\n",
      " |~~ train@41000  Loss: 0.001090 Acc: 10.0050\n",
      " |~~ train@42000  Loss: 0.001106 Acc: 9.9920\n",
      " |~~ train@43000  Loss: 0.001101 Acc: 9.9810\n",
      " |~~ train@44000  Loss: 0.001074 Acc: 10.0270\n",
      " |~~ train@45000  Loss: 0.001106 Acc: 9.9880\n",
      " |~~ train@46000  Loss: 0.001096 Acc: 9.9970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@47000  Loss: 0.001115 Acc: 10.0240\n",
      " |~~ train@48000  Loss: 0.001107 Acc: 9.9230\n",
      " |~~ train@49000  Loss: 0.001095 Acc: 9.9940\n",
      " |~~ train@50000  Loss: 0.001104 Acc: 9.9890\n",
      " |~~ train@51000  Loss: 0.001096 Acc: 10.0180\n",
      " |~~ train@52000  Loss: 0.001115 Acc: 10.0340\n",
      " |~~ train@53000  Loss: 0.001104 Acc: 10.0180\n",
      " |~~ train@54000  Loss: 0.001112 Acc: 9.9610\n",
      " |~~ train@55000  Loss: 0.001110 Acc: 9.9640\n",
      " |~~ train@56000  Loss: 0.001123 Acc: 9.9520\n",
      " |~~ train@57000  Loss: 0.001110 Acc: 9.9390\n",
      " |~~ train@58000  Loss: 0.001089 Acc: 9.9920\n",
      " |~~ train@59000  Loss: 0.001100 Acc: 9.9780\n",
      " |~~ train@60000  Loss: 0.001104 Acc: 10.0200\n",
      " |~~ train@61000  Loss: 0.001116 Acc: 9.9620\n",
      " |~~ train@62000  Loss: 0.001092 Acc: 9.9250\n",
      " |~~ train@63000  Loss: 0.001076 Acc: 10.0050\n",
      " |~~ train@64000  Loss: 0.001104 Acc: 9.9910\n",
      " |~~ train@65000  Loss: 0.001119 Acc: 9.9650\n",
      " |~~ train@66000  Loss: 0.001084 Acc: 10.0550\n",
      " |~~ train@67000  Loss: 0.001096 Acc: 9.9610\n",
      " |~~ train@68000  Loss: 0.001065 Acc: 10.0300\n",
      " |~~ train@69000  Loss: 0.001113 Acc: 9.9830\n",
      " |~~ train@70000  Loss: 0.001105 Acc: 9.9920\n",
      " |~~ train@71000  Loss: 0.001098 Acc: 9.9800\n",
      " |~~ train@72000  Loss: 0.001085 Acc: 10.0010\n",
      " |~~ train@73000  Loss: 0.001102 Acc: 10.0170\n",
      " |~~ train@74000  Loss: 0.001098 Acc: 9.9790\n",
      " |~~ train@75000  Loss: 0.001094 Acc: 9.9970\n",
      " |~~ train@76000  Loss: 0.001093 Acc: 10.0220\n",
      " |~~ train@77000  Loss: 0.001114 Acc: 10.0750\n",
      " |~~ train@78000  Loss: 0.001091 Acc: 9.9760\n",
      " |~~ train@79000  Loss: 0.001121 Acc: 9.9760\n",
      " |~~ train@80000  Loss: 0.001097 Acc: 10.0470\n",
      " |~~ train@81000  Loss: 0.001113 Acc: 10.0530\n",
      " |~~ train@82000  Loss: 0.001114 Acc: 9.9720\n",
      " |~~ train@83000  Loss: 0.001094 Acc: 9.9600\n",
      " |~~ train@84000  Loss: 0.001110 Acc: 10.0000\n",
      " |~~ train@85000  Loss: 0.001089 Acc: 10.0630\n",
      " |~~ train@86000  Loss: 0.001085 Acc: 10.0080\n",
      " |~~ train@87000  Loss: 0.001132 Acc: 9.9510\n",
      " |~~ train@88000  Loss: 0.001071 Acc: 10.0510\n",
      " |~~ train@89000  Loss: 0.001086 Acc: 9.9590\n",
      " |~~ train@90000  Loss: 0.001132 Acc: 9.9910\n",
      " |~~ train@91000  Loss: 0.001105 Acc: 9.9830\n",
      " |~~ train@92000  Loss: 0.001088 Acc: 10.0190\n",
      " |~~ train@93000  Loss: 0.001098 Acc: 9.9560\n",
      " |~~ train@94000  Loss: 0.001120 Acc: 9.9640\n",
      " |~~ train@95000  Loss: 0.001111 Acc: 10.0210\n",
      " |~~ train@96000  Loss: 0.001113 Acc: 10.0160\n",
      " |~~ train@97000  Loss: 0.001136 Acc: 9.9530\n",
      " |~~ train@98000  Loss: 0.001091 Acc: 10.0250\n",
      " |~~ train@99000  Loss: 0.001106 Acc: 10.0820\n",
      " |~~ train@100000  Loss: 0.001115 Acc: 9.9480\n",
      " |~~ train@100908  Loss: 0.001224 Acc: 9.9548\n",
      "train  Loss: 0.001103 Acc: 9.9966\n",
      " |~~ val@1000  Loss: 0.001079 Acc: 10.0100\n",
      " |~~ val@2000  Loss: 0.001093 Acc: 9.9600\n",
      " |~~ val@3000  Loss: 0.001087 Acc: 9.9740\n",
      " |~~ val@4000  Loss: 0.001101 Acc: 9.9820\n",
      " |~~ val@5000  Loss: 0.001083 Acc: 10.0740\n",
      " |~~ val@6000  Loss: 0.001085 Acc: 10.0360\n",
      " |~~ val@7000  Loss: 0.001112 Acc: 10.0260\n",
      " |~~ val@8000  Loss: 0.001110 Acc: 10.0170\n",
      " |~~ val@9000  Loss: 0.001069 Acc: 10.0330\n",
      " |~~ val@10000  Loss: 0.001103 Acc: 10.0620\n",
      " |~~ val@11000  Loss: 0.001118 Acc: 10.0010\n",
      " |~~ val@11212  Loss: 0.005165 Acc: 9.9717\n",
      "val  Loss: 0.001171 Acc: 10.0151\n",
      "Epoch 9/9\n",
      "----------\n",
      " |~~ train@1000  Loss: 0.001087 Acc: 10.0070\n",
      " |~~ train@2000  Loss: 0.001104 Acc: 9.9540\n",
      " |~~ train@3000  Loss: 0.001114 Acc: 9.9840\n",
      " |~~ train@4000  Loss: 0.001113 Acc: 10.0200\n",
      " |~~ train@5000  Loss: 0.001108 Acc: 9.9800\n",
      " |~~ train@6000  Loss: 0.001107 Acc: 10.0040\n",
      " |~~ train@7000  Loss: 0.001138 Acc: 9.9540\n",
      " |~~ train@8000  Loss: 0.001099 Acc: 10.0140\n",
      " |~~ train@9000  Loss: 0.001085 Acc: 9.9600\n",
      " |~~ train@10000  Loss: 0.001131 Acc: 9.9640\n",
      " |~~ train@11000  Loss: 0.001113 Acc: 9.9520\n",
      " |~~ train@12000  Loss: 0.001102 Acc: 9.9800\n",
      " |~~ train@13000  Loss: 0.001088 Acc: 10.0010\n",
      " |~~ train@14000  Loss: 0.001132 Acc: 9.9760\n",
      " |~~ train@15000  Loss: 0.001129 Acc: 9.9460\n",
      " |~~ train@16000  Loss: 0.001111 Acc: 9.9970\n",
      " |~~ train@17000  Loss: 0.001102 Acc: 9.9690\n",
      " |~~ train@18000  Loss: 0.001048 Acc: 10.0030\n",
      " |~~ train@19000  Loss: 0.001101 Acc: 9.9840\n",
      " |~~ train@20000  Loss: 0.001133 Acc: 9.9490\n",
      " |~~ train@21000  Loss: 0.001101 Acc: 9.9760\n",
      " |~~ train@22000  Loss: 0.001111 Acc: 9.9580\n",
      " |~~ train@23000  Loss: 0.001105 Acc: 9.9870\n",
      " |~~ train@24000  Loss: 0.001090 Acc: 9.9120\n",
      " |~~ train@25000  Loss: 0.001079 Acc: 9.9780\n",
      " |~~ train@26000  Loss: 0.001089 Acc: 9.9650\n",
      " |~~ train@27000  Loss: 0.001110 Acc: 9.9770\n",
      " |~~ train@28000  Loss: 0.001083 Acc: 10.0420\n",
      " |~~ train@29000  Loss: 0.001078 Acc: 9.9920\n",
      " |~~ train@30000  Loss: 0.001108 Acc: 9.9500\n",
      " |~~ train@31000  Loss: 0.001122 Acc: 10.0080\n",
      " |~~ train@32000  Loss: 0.001092 Acc: 9.9750\n",
      " |~~ train@33000  Loss: 0.001092 Acc: 9.9220\n",
      " |~~ train@34000  Loss: 0.001098 Acc: 9.9910\n",
      " |~~ train@35000  Loss: 0.001126 Acc: 9.9420\n",
      " |~~ train@36000  Loss: 0.001107 Acc: 10.0510\n",
      " |~~ train@37000  Loss: 0.001083 Acc: 9.9800\n",
      " |~~ train@38000  Loss: 0.001110 Acc: 9.9860\n",
      " |~~ train@39000  Loss: 0.001087 Acc: 9.9610\n",
      " |~~ train@40000  Loss: 0.001088 Acc: 9.9980\n",
      " |~~ train@41000  Loss: 0.001086 Acc: 9.9790\n",
      " |~~ train@42000  Loss: 0.001125 Acc: 9.9470\n",
      " |~~ train@43000  Loss: 0.001113 Acc: 10.0020\n",
      " |~~ train@44000  Loss: 0.001107 Acc: 9.9480\n",
      " |~~ train@45000  Loss: 0.001115 Acc: 9.9730\n",
      " |~~ train@46000  Loss: 0.001106 Acc: 9.9650\n",
      " |~~ train@47000  Loss: 0.001115 Acc: 10.0110\n",
      " |~~ train@48000  Loss: 0.001092 Acc: 10.0070\n",
      " |~~ train@49000  Loss: 0.001107 Acc: 9.9830\n",
      " |~~ train@50000  Loss: 0.001086 Acc: 9.9510\n",
      " |~~ train@51000  Loss: 0.001092 Acc: 9.9720\n",
      " |~~ train@52000  Loss: 0.001097 Acc: 9.9390\n",
      " |~~ train@53000  Loss: 0.001115 Acc: 9.9790\n",
      " |~~ train@54000  Loss: 0.001091 Acc: 10.0240\n",
      " |~~ train@55000  Loss: 0.001101 Acc: 10.0110\n",
      " |~~ train@56000  Loss: 0.001114 Acc: 10.0280\n",
      " |~~ train@57000  Loss: 0.001117 Acc: 9.9570\n",
      " |~~ train@58000  Loss: 0.001087 Acc: 9.9630\n",
      " |~~ train@59000  Loss: 0.001078 Acc: 10.0350\n",
      " |~~ train@60000  Loss: 0.001102 Acc: 9.9980\n",
      " |~~ train@61000  Loss: 0.001077 Acc: 10.0000\n",
      " |~~ train@62000  Loss: 0.001118 Acc: 9.9720\n",
      " |~~ train@63000  Loss: 0.001096 Acc: 10.0060\n",
      " |~~ train@64000  Loss: 0.001097 Acc: 9.9750\n",
      " |~~ train@65000  Loss: 0.001103 Acc: 10.0110\n",
      " |~~ train@66000  Loss: 0.001118 Acc: 9.9290\n",
      " |~~ train@67000  Loss: 0.001122 Acc: 9.9860\n",
      " |~~ train@68000  Loss: 0.001107 Acc: 9.9720\n",
      " |~~ train@69000  Loss: 0.001126 Acc: 9.9670\n",
      " |~~ train@70000  Loss: 0.001095 Acc: 9.9950\n",
      " |~~ train@71000  Loss: 0.001081 Acc: 10.0440\n",
      " |~~ train@72000  Loss: 0.001081 Acc: 9.9810\n",
      " |~~ train@73000  Loss: 0.001096 Acc: 10.0210\n",
      " |~~ train@74000  Loss: 0.001092 Acc: 10.0260\n",
      " |~~ train@75000  Loss: 0.001135 Acc: 9.9470\n",
      " |~~ train@76000  Loss: 0.001107 Acc: 10.0340\n",
      " |~~ train@77000  Loss: 0.001122 Acc: 9.9440\n",
      " |~~ train@78000  Loss: 0.001073 Acc: 10.0280\n",
      " |~~ train@79000  Loss: 0.001086 Acc: 9.9830\n",
      " |~~ train@80000  Loss: 0.001076 Acc: 10.0100\n",
      " |~~ train@81000  Loss: 0.001103 Acc: 9.9700\n",
      " |~~ train@82000  Loss: 0.001119 Acc: 9.9640\n",
      " |~~ train@83000  Loss: 0.001074 Acc: 10.0250\n",
      " |~~ train@84000  Loss: 0.001101 Acc: 9.9940\n",
      " |~~ train@85000  Loss: 0.001079 Acc: 9.9780\n",
      " |~~ train@86000  Loss: 0.001111 Acc: 10.0200\n",
      " |~~ train@87000  Loss: 0.001110 Acc: 9.9920\n",
      " |~~ train@88000  Loss: 0.001115 Acc: 10.0220\n",
      " |~~ train@89000  Loss: 0.001066 Acc: 10.0380\n",
      " |~~ train@90000  Loss: 0.001084 Acc: 10.0530\n",
      " |~~ train@91000  Loss: 0.001111 Acc: 9.9750\n",
      " |~~ train@92000  Loss: 0.001060 Acc: 10.1140\n",
      " |~~ train@93000  Loss: 0.001080 Acc: 9.9920\n",
      " |~~ train@94000  Loss: 0.001124 Acc: 10.0140\n",
      " |~~ train@95000  Loss: 0.001143 Acc: 9.9300\n",
      " |~~ train@96000  Loss: 0.001109 Acc: 9.9940\n",
      " |~~ train@97000  Loss: 0.001084 Acc: 10.0490\n",
      " |~~ train@98000  Loss: 0.001122 Acc: 9.9600\n",
      " |~~ train@99000  Loss: 0.001095 Acc: 9.9660\n",
      " |~~ train@100000  Loss: 0.001137 Acc: 9.9960\n",
      " |~~ train@100908  Loss: 0.001202 Acc: 10.0308\n",
      "train  Loss: 0.001103 Acc: 9.9874\n",
      " |~~ val@1000  Loss: 0.001083 Acc: 10.0260\n",
      " |~~ val@2000  Loss: 0.001083 Acc: 9.9200\n",
      " |~~ val@3000  Loss: 0.001088 Acc: 10.0080\n",
      " |~~ val@4000  Loss: 0.001108 Acc: 9.9850\n",
      " |~~ val@5000  Loss: 0.001097 Acc: 10.0010\n",
      " |~~ val@6000  Loss: 0.001111 Acc: 10.0280\n",
      " |~~ val@7000  Loss: 0.001091 Acc: 9.9910\n",
      " |~~ val@8000  Loss: 0.001095 Acc: 9.9570\n",
      " |~~ val@9000  Loss: 0.001085 Acc: 10.0280\n",
      " |~~ val@10000  Loss: 0.001084 Acc: 10.0070\n",
      " |~~ val@11000  Loss: 0.001107 Acc: 9.9460\n",
      " |~~ val@11212  Loss: 0.005197 Acc: 9.9858\n",
      "val  Loss: 0.001172 Acc: 9.9905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 141m 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d (3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d (128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d (256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "    (fc): Linear(in_features=512, out_features=14)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_ft,\n",
    "            criterion,\n",
    "            optimizer_ft,\n",
    "            exp_lr_scheduler,\n",
    "            num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ed2ea89b-99da-4fde-a3ac-91d4d4e3865a"
    }
   },
   "source": [
    "# Analysis of Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbpresent": {
     "id": "9235665b-a758-4c35-a922-55466a19bd44"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING ITERATION...\n",
      "PROCESSING FIRST 750 OBSERVATIONS\n",
      "STARTING ITERATION...\n",
      "PROCESSING FIRST 406 OBSERVATIONS\n"
     ]
    }
   ],
   "source": [
    "out_model_30.train(mode=False)\n",
    "\n",
    "obs_counter = 0\n",
    "total_pred = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "total_act = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "conf_a = {}\n",
    "conf_b = {}\n",
    "conf_c = {}\n",
    "conf_d = {}\n",
    "for i in range(1,10):\n",
    "    conf_a[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_b[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_c[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_d[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "for data in dataloaders['val']:\n",
    "    print(\"STARTING ITERATION...\")\n",
    "    inputs, labels = data\n",
    "    print(\"PROCESSING FIRST {} OBSERVATIONS\".format(len(inputs)))\n",
    "\n",
    "    inputs = Variable(inputs.cuda())\n",
    "    labels = Variable(labels.cuda())\n",
    "\n",
    "    outputs = out_model_30(inputs).sigmoid()\n",
    "    \n",
    "    total_act += labels.sum(0).cpu()\n",
    "    total_pred += outputs.sum(0).cpu()\n",
    "\n",
    "    # Store statistics (convert from autograd.Variable to float/int)\n",
    "    for i in range(1,10):\n",
    "        t = i/10\n",
    "        conf_a[i] += ((outputs.sigmoid()>t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_b[i] += ((outputs.sigmoid()<t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_c[i] += ((outputs.sigmoid()>t) == (labels<0.5)).sum(0).cpu().float()\n",
    "        conf_d[i] += ((outputs.sigmoid()<t) == (labels<0.5)).sum(0).cpu().float()\n",
    "\n",
    "    obs_counter += len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "nbpresent": {
     "id": "82758b84-e7ce-48d3-a3e1-f012ab124eea"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 -2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00 -2.1475e+09  0.0000e+00 -2.1475e+09  0.0000e+00\n",
      " 6.2634e+06 -1.0000e+00 -2.1475e+09  0.0000e+00 -2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "\n",
      "Columns 6 to 11 \n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 -6.5460e+04  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "-2.1475e+09  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "-2.1475e+09  0.0000e+00 -2.1475e+09 -2.1475e+09 -2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "\n",
      "Columns 12 to 13 \n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      "-2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      "[torch.IntTensor of size 9x14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comparison = Variable(torch.FloatTensor(9, 14))\n",
    "for i in range(9):\n",
    "    comparison[0] = conf_a[1] / obs_counter\n",
    "print(comparison.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "nbpresent": {
     "id": "b5bbfb54-d465-4c52-90ea-fff045789b9b"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3183\n",
       " 0.1462\n",
       " 0.3356\n",
       " 0.2171\n",
       " 0.3166\n",
       " 0.3183\n",
       " 0.2803\n",
       " 0.2898\n",
       " 0.2232\n",
       " 0.3279\n",
       " 0.2846\n",
       " 0.3045\n",
       " 0.3002\n",
       " 0.3192\n",
       "[torch.FloatTensor of size 14]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_d[9] / obs_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbpresent": {
     "id": "2b02d75e-a8b2-4d29-a9c3-27903e05d160"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fibrosis',\n",
       " 'Infiltration',\n",
       " 'Hernia',\n",
       " 'Effusion',\n",
       " 'Emphysema',\n",
       " 'Edema',\n",
       " 'Cardiomegaly',\n",
       " 'Mass',\n",
       " 'Nodule',\n",
       " 'Atelectasis',\n",
       " 'Pneumothorax',\n",
       " 'Pleural_Thickening',\n",
       " 'Consolidation',\n",
       " 'Pneumonia']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data_train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "nbpresent": {
     "id": "4feafc69-38b0-41e8-b670-92c3aa26bfff"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'state': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'val_error': val_error\n",
    "        }, model_out_path)\n",
    "'''\n",
    "test_load = torch.load('/user/xrayproj/output/20171120-01h41m56s_model_9.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'state', 'optimizer', 'scheduler', 'val_error'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_load.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_opt = test_load['optimizer']\n",
    "load_sched = test_load['scheduler']\n",
    "load_state = test_load['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.resnet18(pretrained=True)\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace FC layer\n",
    "model2.fc = nn.Linear(model2.fc.in_features, len(img_data_train.labels))\n",
    "\n",
    "model2_c = DataParallel(model2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_c.load_state_dict(load_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.7873  0.5093  0.2980  0.6386  0.4665  0.3371  0.1614  0.1846  0.1925  0.6630\n",
       "\n",
       "Columns 10 to 13 \n",
       " 0.0288  0.4468  0.3684  0.2549\n",
       "[torch.cuda.FloatTensor of size 1x14 (GPU 0)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_c.forward(Variable(img_data_train[0][0].unsqueeze(0).cuda())).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

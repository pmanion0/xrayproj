{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ddb34fde-926f-42f6-8bfc-b1b19cb4881d"
    }
   },
   "source": [
    "# Import and High-Level Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "80e3fe37-bc30-43b7-91c3-474b94a16db6"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# General Python Packages\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import lr_scheduler, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Module\n",
    "\n",
    "# General Analytics Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization / Image Packages\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Randomization Functions\n",
    "from random import random as randuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "9b582614-ccd7-4f48-8b66-a49ebe66807f"
    }
   },
   "outputs": [],
   "source": [
    "# Put MatPlotLib in interactive mode\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "97faafc3-219d-4a56-b587-32a9c2550eac"
    }
   },
   "source": [
    "# Define Data Manipulation Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6fed3f3e-b14a-457d-95d2-c3726ce0fb3e"
    }
   },
   "source": [
    "### Helper Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "f962c744-4459-4ca1-851c-a19fe8457118"
    }
   },
   "outputs": [],
   "source": [
    "def is_image_file(fname):\n",
    "    \"\"\"Checks if a file is an image.\n",
    "    Args:\n",
    "        fname (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    return fname.lower().endswith('.png')\n",
    "\n",
    "def create_label_maps(details_df):\n",
    "    \"\"\" Take a descriptive dataframe and extract the unique labels and map to index values\n",
    "    Args:\n",
    "        details_df: Dataframe with the image details\n",
    "    Returns:\n",
    "        label_list: list of unique labels in the dataframe\n",
    "        label_to_index: map from labels to indices\n",
    "    \"\"\"\n",
    "    \"\"\" TODO: Research paper also excludes these labels but need to figure out how to handle\n",
    "              cases that have these as positive findings (completely exclude?)\n",
    "    excluded_labels = ['Edema','Hernia','Emphysema','Fibrosis','No Finding'\n",
    "                      'Pleural_Thickening','Consolidation']\n",
    "    \"\"\"\n",
    "    excluded_labels = ['No Finding']\n",
    "    \n",
    "    label_groups = details_df['Finding Labels'].unique()\n",
    "    unique_labels = set([label for sublist in label_groups.tolist() for label in sublist.split('|')])\n",
    "    \n",
    "    # Drop some label that we do not want to include\n",
    "    unique_labels = [l for l in unique_labels if l not in excluded_labels]\n",
    "\n",
    "    index_to_label = {idx: val for idx, val in enumerate(unique_labels)}\n",
    "    label_to_index = {val: idx for idx, val in index_to_label.items()}\n",
    "\n",
    "    label_list = list(label_to_index.keys())\n",
    "\n",
    "    return label_list, label_to_index\n",
    "\n",
    "def create_image_list(dir):\n",
    "    \"\"\" Create a full list of images available \n",
    "    Args:\n",
    "        dir (string): root directory of images with subdirectories underneath\n",
    "                      that have the .png images within them\n",
    "    Returns:\n",
    "        image_list: list of tuples with (image_name, full_image_path)\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for subfolder in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, subfolder)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for subfolder_path, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(subfolder_path, fname)\n",
    "                    image_list.append((fname, path))\n",
    "    return image_list\n",
    "\n",
    "def pil_loader(path):\n",
    "    \"\"\" Opens path as file with Pillow (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    Args:\n",
    "        path (string): File path to the image\n",
    "    Returns:\n",
    "        img: Image in RGB format\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "        \n",
    "def imshow(inp, title=None):\n",
    "    \"\"\" Convert tensor array to an image (only use post-dataset transform) \"\"\"\n",
    "    inp = inp[0]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5d91972-9d4b-4022-842b-22c823f98fff"
    }
   },
   "source": [
    "### Implementation of Torch's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "5bf4e82b-13ca-4ac2-bbb6-3081e820ab4e"
    }
   },
   "outputs": [],
   "source": [
    "class XrayImageSet(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_root (string): root directory of the images in form image/subfolder/*.png\n",
    "        csv_file (string): path to the CSV data file\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "     Attributes:\n",
    "        labels (list): list of the possible label names.\n",
    "        label_to_index (dict): look from label name to a label index\n",
    "        imgs (list): List of (filename, image path) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_root, csv_file, transform=None, target_transform=None, loader = pil_loader):\n",
    "        \"\"\" Create an instance of the Xray Dataset \"\"\"\n",
    "        img_details = pd.read_csv(csv_file)\n",
    "        \n",
    "        labels, label_to_index = create_label_maps(img_details)\n",
    "        imgs = create_image_list(image_root)\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.image_details = img_details\n",
    "        self.image_root = image_root\n",
    "        self.labels = labels\n",
    "        self.label_to_index = label_to_index\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.max_label_index = max(label_to_index.values())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get image,labels pair by index\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        fname, path = self.imgs[index]\n",
    "        target = self.get_one_hot_labels(fname)\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Calculate length of the dataset (number of images) \"\"\"\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def get_labels(self, fname):\n",
    "        \"\"\" Return the label string for the file \"\"\"\n",
    "        return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
    "    \n",
    "    def one_hot_labels(self, labels):\n",
    "        \"\"\" Convert the labels string (with each label separated by |) into 1-hot encoding \"\"\"\n",
    "        if labels == None:\n",
    "            return None\n",
    "        \n",
    "        split_label_indices = [self.label_to_index.get(label)\n",
    "                               for label in labels.split('|')\n",
    "                               if label != 'No Finding']\n",
    "        \n",
    "        out = [1 if idx in split_label_indices else 0 for idx in range(self.max_label_index+1)]\n",
    "        # This code UNHOTs the labels:\n",
    "        # out = '|'.join([index_to_label.get(idx) for idx, val in enumerate(one_hot_tuple) if val == 1])\n",
    "        return out\n",
    "\n",
    "    def get_one_hot_labels(self, fname):\n",
    "        \"\"\" Get the 1-hot encoded label array for the provided file \"\"\"\n",
    "        labels = self.get_labels(fname)\n",
    "        one_hot_labels = self.one_hot_labels(labels)\n",
    "        return torch.FloatTensor(one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b6f705b5-c4e4-4fbd-a517-f0c3b58c4305"
    }
   },
   "source": [
    "### Create the dataset with necessary transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "6716d746-b7b1-4aff-aec0-2bd91632bf28"
    }
   },
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "62d30308-7ecb-40f4-a831-dd0a9274618a"
    }
   },
   "outputs": [],
   "source": [
    "img_data_train = XrayImageSet(image_root = '/user/images/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_train.imgs = [img for i, img in enumerate(img_data_train.imgs) if i % 10 > 0]# and randuni() < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "13f86c44-50f2-4809-8ee1-afb0d8ec0b7a"
    }
   },
   "outputs": [],
   "source": [
    "img_data_val   = XrayImageSet(image_root = '/user/images/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_val.imgs = [img for i, img in enumerate(img_data_val.imgs) if i % 10 == 0]# and randuni() < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "09ab4157-ad5b-4602-8b93-85c86bd5a620"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 100908\n",
      "Validation Set Size: 11212\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size: {}\".format(len(img_data_train)))\n",
    "print(\"Validation Set Size: {}\".format(len(img_data_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1b2e1feb-e832-41c5-8b1d-70384f1fe915"
    }
   },
   "source": [
    "### Put the dataset into a Dataloader to handle batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "e0484420-b2a8-429b-8da4-368a592db7b8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU: 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_gpus = torch.cuda.device_count()\n",
    "pin_mem_setting = True\n",
    "\n",
    "print(\"Number of GPU: {}\".format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "1001fa5d-b820-4da6-9d18-0ee7f4462b90"
    }
   },
   "outputs": [],
   "source": [
    "img_loader_train = DataLoader(img_data_train,\n",
    "                              batch_size = batch_size * num_gpus,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 10,\n",
    "                              pin_memory = pin_mem_setting)\n",
    "\n",
    "img_loader_val   = DataLoader(img_data_val,\n",
    "                              batch_size = batch_size * num_gpus,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 10,\n",
    "                              pin_memory = pin_mem_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "0e1d2cb0-4d0f-4ab1-a1cf-cd66a3e298a2"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': img_loader_train,\n",
    "    'val': img_loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "350daac2-9c37-42d0-a1ef-de7a8110b38f"
    }
   },
   "source": [
    "# Define model training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "166ce9d1-ff17-4047-b9ae-b4903393ad15"
    }
   },
   "outputs": [],
   "source": [
    "class printer_writer:\n",
    "    def __init__(self, output_folder_path):\n",
    "        self.start_time = time.strftime('%Y%m%d-%Hh%Mm%Ss')\n",
    "        \n",
    "        self.outprefix = output_folder_path + '/' + self.start_time\n",
    "        \n",
    "        # Print Output File\n",
    "        self.print_out_path = self.outprefix + '_print.txt'\n",
    "        self.print_out_file = open(self.print_out_path, 'w', 1)\n",
    "        \n",
    "    def printw(self, string):\n",
    "        print(string)\n",
    "        try:\n",
    "            self.print_out_file.write(string + \"\\n\")\n",
    "        except: # Ignore errors\n",
    "            pass\n",
    "        \n",
    "    def save_checkpoint(self, epoch, model, optimizer, scheduler, val_error):\n",
    "        model_out_path = self.outprefix + '_model_' + str(epoch+1) + '.tar'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'state': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'val_error': val_error\n",
    "        }, model_out_path)\n",
    "        \n",
    "    def close(self):\n",
    "        self.print_out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "eb99e6e4-00db-494a-ad27-70005761f49e"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, outfolder = '/user/xrayproj/output/'):\n",
    "    since = time.time()\n",
    "    scribe = printer_writer(outfolder)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scribe.printw('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        scribe.printw('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            obs_counter = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Store statistics (convert from autograd.Variable to float/int)\n",
    "                loss_val = loss.data[0]\n",
    "                correct_val = torch.sum( ((outputs.sigmoid()>0.5) == (labels>0.5)).long() ).data[0]\n",
    "                \n",
    "                running_loss += loss_val\n",
    "                running_corrects += correct_val\n",
    "                \n",
    "                obs_counter += len(inputs)\n",
    "                \n",
    "                batch_loss = 1.0 * loss_val / len(inputs)\n",
    "                batch_acc = 1.0 * correct_val / len(inputs)\n",
    "                status = ' |~~ {}@{}  Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    phase, obs_counter, batch_loss, batch_acc)\n",
    "                scribe.printw(status)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            scribe.printw('{}  Loss: {:.6f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                scribe.save_checkpoint(epoch, model, optimizer, scheduler, epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    scribe.printw('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    scribe.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e6eebab-809d-4550-b771-135dbf2b893d"
    }
   },
   "source": [
    "# Define Weighted Cost Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbpresent": {
     "id": "0c520d22-a18b-4e5f-b849-1960820f4d04"
    }
   },
   "outputs": [],
   "source": [
    "def imbalance_weighted_bce_with_logit(input, target, size_average=True):\n",
    "    if not (target.size() == input.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "\n",
    "    max_val = (-input).clamp(min=0)\n",
    "    loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "    # Determine |P| and |N|\n",
    "    positive_labels = target.sum()\n",
    "    negative_labels = (1-target).sum()\n",
    "\n",
    "    # Upweight the less common class (very often the 1s)\n",
    "    beta_p = (positive_labels + negative_labels) / positive_labels\n",
    "    beta_n = (positive_labels + negative_labels) / negative_labels\n",
    "\n",
    "    # Adjust the losses accordingly\n",
    "    loss_weight = target * beta_p + (1-target) * beta_n\n",
    "    \n",
    "    loss = loss * loss_weight\n",
    "\n",
    "    if size_average:\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbpresent": {
     "id": "f66b439c-1f87-4d08-939d-f64d085b846b"
    }
   },
   "outputs": [],
   "source": [
    "class BCEWithLogitsImbalanceWeightedLoss(Module):\n",
    "    def __init__(self, class_weight=None, size_average=True):\n",
    "        super(BCEWithLogitsImbalanceWeightedLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return imbalance_weighted_bce_with_logit(input, target, size_average=self.size_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8ca6e253-06b7-4a15-ac6c-370629667ad6"
    }
   },
   "source": [
    "# Setup Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18PlusFlexibleFC():\n",
    "    # Create a base ResNet18 model\n",
    "    m = models.resnet18(pretrained=True)\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final FC layer\n",
    "    m.fc = nn.Linear(m.fc.in_features, len(img_data_train.labels))\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50PlusFlexibleFC():\n",
    "    # Create a base ResNet18 model\n",
    "    m = models.resnet50(pretrained=True)\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final FC layer\n",
    "    m.fc = nn.Linear(m.fc.in_features, len(img_data_train.labels))\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18PlusFCFullyFlexible():\n",
    "    # Create a base ResNet18 model\n",
    "    m = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Replace the final FC layer\n",
    "    m.fc = nn.Linear(m.fc.in_features, len(img_data_train.labels))\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5580486c-3807-459c-a02d-68be7ffc7e08"
    }
   },
   "source": [
    "### Pull the ResNet-18 pre-trained model and replace the fully connected layer at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbpresent": {
     "id": "91946119-bf67-4871-92af-649559fa9bfd"
    }
   },
   "outputs": [],
   "source": [
    "#model_base = ResNet18PlusFlexibleFC()\n",
    "#model_base = ResNet18PlusFCFullyFlexible()\n",
    "model_base = ResNet50PlusFlexibleFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b85990f9-2c42-48b1-9e5b-857ebd8c2f30"
    }
   },
   "source": [
    "### Push model to CUDA/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbpresent": {
     "id": "0a904f1f-73ac-418b-86cf-cdafdf0f67b1"
    }
   },
   "outputs": [],
   "source": [
    "model_ft = DataParallel(model_base).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e6851de2-8645-4547-9f00-414ad8a3811a"
    }
   },
   "source": [
    "### Define loss measure and learning rates/procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbpresent": {
     "id": "1d61f077-84ec-4d2c-bdb0-82b28f8c4ed9"
    }
   },
   "outputs": [],
   "source": [
    "#criterion = BCEWithLogitsImbalanceWeightedLoss()\n",
    "criterion_base = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_ft = SGD(model_ft.module.fc.parameters(), lr=0.0001, momentum=0.9)\n",
    "#optimizer_ft = SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = criterion_base.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future code for allowing optimization of the base layer with a lower learning rate\n",
    "\n",
    "```\n",
    "ignored_params = list(map(id, model.fc.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in ignored_params,\n",
    "                     model.parameters())\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "            {'params': base_params},\n",
    "            {'params': model.fc.parameters(), 'lr': opt.lr}\n",
    "        ], lr=opt.lr*0.1, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f9596b5-a2fc-4e0c-984b-4e13b68dcd6d"
    }
   },
   "source": [
    "# Begin Training Network (Normal Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "nbpresent": {
     "id": "f1672c30-c299-4265-934b-6af391d9de8c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/7\n",
      "----------\n",
      " |~~ train@64  Loss: 0.010631 Acc: 7.9688\n",
      " |~~ train@128  Loss: 0.010612 Acc: 7.9219\n",
      " |~~ train@192  Loss: 0.010615 Acc: 8.0938\n",
      " |~~ train@256  Loss: 0.010579 Acc: 8.0938\n",
      " |~~ train@320  Loss: 0.010523 Acc: 8.2656\n",
      " |~~ train@384  Loss: 0.010541 Acc: 8.1250\n",
      " |~~ train@448  Loss: 0.010451 Acc: 8.4531\n",
      " |~~ train@512  Loss: 0.010437 Acc: 8.4375\n",
      " |~~ train@576  Loss: 0.010367 Acc: 8.7969\n",
      " |~~ train@640  Loss: 0.010313 Acc: 9.0156\n",
      " |~~ train@704  Loss: 0.010324 Acc: 8.7656\n",
      " |~~ train@768  Loss: 0.010167 Acc: 9.2344\n",
      " |~~ train@832  Loss: 0.010076 Acc: 9.4688\n",
      " |~~ train@896  Loss: 0.010046 Acc: 9.3906\n",
      " |~~ train@960  Loss: 0.009978 Acc: 9.7969\n",
      " |~~ train@1024  Loss: 0.009907 Acc: 9.8281\n",
      " |~~ train@1088  Loss: 0.009845 Acc: 10.0938\n",
      " |~~ train@1152  Loss: 0.009701 Acc: 10.3125\n",
      " |~~ train@1216  Loss: 0.009694 Acc: 10.5469\n",
      " |~~ train@1280  Loss: 0.009666 Acc: 10.4219\n",
      " |~~ train@1344  Loss: 0.009594 Acc: 10.5781\n",
      " |~~ train@1408  Loss: 0.009465 Acc: 10.9531\n",
      " |~~ train@1472  Loss: 0.009415 Acc: 10.9219\n",
      " |~~ train@1536  Loss: 0.009384 Acc: 11.1562\n",
      " |~~ train@1600  Loss: 0.009328 Acc: 11.3750\n",
      " |~~ train@1664  Loss: 0.009197 Acc: 11.6094\n",
      " |~~ train@1728  Loss: 0.009060 Acc: 11.6094\n",
      " |~~ train@1792  Loss: 0.009084 Acc: 11.7500\n",
      " |~~ train@1856  Loss: 0.008990 Acc: 11.8438\n",
      " |~~ train@1920  Loss: 0.008774 Acc: 12.2500\n",
      " |~~ train@1984  Loss: 0.008805 Acc: 12.1094\n",
      " |~~ train@2048  Loss: 0.008736 Acc: 12.0625\n",
      " |~~ train@2112  Loss: 0.008628 Acc: 12.2812\n",
      " |~~ train@2176  Loss: 0.008564 Acc: 12.5781\n",
      " |~~ train@2240  Loss: 0.008643 Acc: 12.2969\n",
      " |~~ train@2304  Loss: 0.008545 Acc: 12.4531\n",
      " |~~ train@2368  Loss: 0.008468 Acc: 12.6562\n",
      " |~~ train@2432  Loss: 0.008357 Acc: 12.6094\n",
      " |~~ train@2496  Loss: 0.008196 Acc: 12.7500\n",
      " |~~ train@2560  Loss: 0.008160 Acc: 12.9688\n",
      " |~~ train@2624  Loss: 0.008219 Acc: 12.7344\n",
      " |~~ train@2688  Loss: 0.008073 Acc: 13.0312\n",
      " |~~ train@2752  Loss: 0.008014 Acc: 13.0156\n",
      " |~~ train@2816  Loss: 0.007968 Acc: 12.9844\n",
      " |~~ train@2880  Loss: 0.007813 Acc: 13.2031\n",
      " |~~ train@2944  Loss: 0.007854 Acc: 12.9375\n",
      " |~~ train@3008  Loss: 0.007778 Acc: 13.1719\n",
      " |~~ train@3072  Loss: 0.007763 Acc: 12.9844\n",
      " |~~ train@3136  Loss: 0.007532 Acc: 13.2969\n",
      " |~~ train@3200  Loss: 0.007711 Acc: 13.0469\n",
      " |~~ train@3264  Loss: 0.007420 Acc: 13.3750\n",
      " |~~ train@3328  Loss: 0.007579 Acc: 13.0938\n",
      " |~~ train@3392  Loss: 0.007568 Acc: 13.1406\n",
      " |~~ train@3456  Loss: 0.007463 Acc: 13.2812\n",
      " |~~ train@3520  Loss: 0.007338 Acc: 13.2969\n",
      " |~~ train@3584  Loss: 0.007157 Acc: 13.4688\n",
      " |~~ train@3648  Loss: 0.007216 Acc: 13.3750\n",
      " |~~ train@3712  Loss: 0.007293 Acc: 13.1406\n",
      " |~~ train@3776  Loss: 0.007068 Acc: 13.3750\n",
      " |~~ train@3840  Loss: 0.007116 Acc: 13.2188\n",
      " |~~ train@3904  Loss: 0.007208 Acc: 13.0625\n",
      " |~~ train@3968  Loss: 0.007032 Acc: 13.2812\n",
      " |~~ train@4032  Loss: 0.006898 Acc: 13.3438\n",
      " |~~ train@4096  Loss: 0.006933 Acc: 13.3125\n",
      " |~~ train@4160  Loss: 0.006876 Acc: 13.2969\n",
      " |~~ train@4224  Loss: 0.006729 Acc: 13.3906\n",
      " |~~ train@4288  Loss: 0.006841 Acc: 13.2500\n",
      " |~~ train@4352  Loss: 0.006778 Acc: 13.2031\n",
      " |~~ train@4416  Loss: 0.006784 Acc: 13.1719\n",
      " |~~ train@4480  Loss: 0.006707 Acc: 13.2188\n",
      " |~~ train@4544  Loss: 0.006547 Acc: 13.3594\n",
      " |~~ train@4608  Loss: 0.006659 Acc: 13.1875\n",
      " |~~ train@4672  Loss: 0.006392 Acc: 13.4844\n",
      " |~~ train@4736  Loss: 0.006559 Acc: 13.2188\n",
      " |~~ train@4800  Loss: 0.006692 Acc: 13.0938\n",
      " |~~ train@4864  Loss: 0.006445 Acc: 13.2812\n",
      " |~~ train@4928  Loss: 0.006236 Acc: 13.4531\n",
      " |~~ train@4992  Loss: 0.006549 Acc: 13.0938\n",
      " |~~ train@5056  Loss: 0.006466 Acc: 13.1562\n",
      " |~~ train@5120  Loss: 0.006396 Acc: 13.2188\n",
      " |~~ train@5184  Loss: 0.006318 Acc: 13.2500\n",
      " |~~ train@5248  Loss: 0.006202 Acc: 13.2969\n",
      " |~~ train@5312  Loss: 0.006361 Acc: 13.1406\n",
      " |~~ train@5376  Loss: 0.006190 Acc: 13.2656\n",
      " |~~ train@5440  Loss: 0.006233 Acc: 13.1406\n",
      " |~~ train@5504  Loss: 0.006274 Acc: 13.1250\n",
      " |~~ train@5568  Loss: 0.006316 Acc: 13.0469\n",
      " |~~ train@5632  Loss: 0.006066 Acc: 13.2500\n",
      " |~~ train@5696  Loss: 0.005888 Acc: 13.4375\n",
      " |~~ train@5760  Loss: 0.005911 Acc: 13.3750\n",
      " |~~ train@5824  Loss: 0.006078 Acc: 13.1562\n",
      " |~~ train@5888  Loss: 0.005952 Acc: 13.2969\n",
      " |~~ train@5952  Loss: 0.005666 Acc: 13.5156\n",
      " |~~ train@6016  Loss: 0.005850 Acc: 13.2344\n",
      " |~~ train@6080  Loss: 0.005872 Acc: 13.2656\n",
      " |~~ train@6144  Loss: 0.005715 Acc: 13.3906\n",
      " |~~ train@6208  Loss: 0.005663 Acc: 13.4062\n",
      " |~~ train@6272  Loss: 0.005993 Acc: 13.0625\n",
      " |~~ train@6336  Loss: 0.005877 Acc: 13.1406\n",
      " |~~ train@6400  Loss: 0.005754 Acc: 13.2344\n",
      " |~~ train@6464  Loss: 0.005474 Acc: 13.5000\n",
      " |~~ train@6528  Loss: 0.005834 Acc: 13.1094\n",
      " |~~ train@6592  Loss: 0.005708 Acc: 13.2188\n",
      " |~~ train@6656  Loss: 0.005458 Acc: 13.4375\n",
      " |~~ train@6720  Loss: 0.005705 Acc: 13.1875\n",
      " |~~ train@6784  Loss: 0.005341 Acc: 13.4688\n",
      " |~~ train@6848  Loss: 0.005386 Acc: 13.4062\n",
      " |~~ train@6912  Loss: 0.005536 Acc: 13.2500\n",
      " |~~ train@6976  Loss: 0.005341 Acc: 13.4219\n",
      " |~~ train@7040  Loss: 0.005306 Acc: 13.4062\n",
      " |~~ train@7104  Loss: 0.005437 Acc: 13.2656\n",
      " |~~ train@7168  Loss: 0.005816 Acc: 12.9375\n",
      " |~~ train@7232  Loss: 0.005338 Acc: 13.3594\n",
      " |~~ train@7296  Loss: 0.005481 Acc: 13.1875\n",
      " |~~ train@7360  Loss: 0.005468 Acc: 13.2188\n",
      " |~~ train@7424  Loss: 0.005434 Acc: 13.1875\n",
      " |~~ train@7488  Loss: 0.005361 Acc: 13.2188\n",
      " |~~ train@7552  Loss: 0.005273 Acc: 13.3125\n",
      " |~~ train@7616  Loss: 0.005122 Acc: 13.4375\n",
      " |~~ train@7680  Loss: 0.004994 Acc: 13.5000\n",
      " |~~ train@7744  Loss: 0.005379 Acc: 13.1562\n",
      " |~~ train@7808  Loss: 0.004993 Acc: 13.4844\n",
      " |~~ train@7872  Loss: 0.005405 Acc: 13.1094\n",
      " |~~ train@7936  Loss: 0.005081 Acc: 13.3125\n",
      " |~~ train@8000  Loss: 0.004932 Acc: 13.4844\n",
      " |~~ train@8064  Loss: 0.005116 Acc: 13.2969\n",
      " |~~ train@8128  Loss: 0.004927 Acc: 13.4375\n",
      " |~~ train@8192  Loss: 0.005044 Acc: 13.3281\n",
      " |~~ train@8256  Loss: 0.004852 Acc: 13.4531\n",
      " |~~ train@8320  Loss: 0.004913 Acc: 13.4219\n",
      " |~~ train@8384  Loss: 0.004933 Acc: 13.3750\n",
      " |~~ train@8448  Loss: 0.005212 Acc: 13.1406\n",
      " |~~ train@8512  Loss: 0.004761 Acc: 13.5000\n",
      " |~~ train@8576  Loss: 0.005005 Acc: 13.2812\n",
      " |~~ train@8640  Loss: 0.004885 Acc: 13.3906\n",
      " |~~ train@8704  Loss: 0.004768 Acc: 13.4375\n",
      " |~~ train@8768  Loss: 0.005119 Acc: 13.1719\n",
      " |~~ train@8832  Loss: 0.005055 Acc: 13.1562\n",
      " |~~ train@8896  Loss: 0.004829 Acc: 13.3750\n",
      " |~~ train@8960  Loss: 0.004729 Acc: 13.3906\n",
      " |~~ train@9024  Loss: 0.005087 Acc: 13.1250\n",
      " |~~ train@9088  Loss: 0.004663 Acc: 13.4375\n",
      " |~~ train@9152  Loss: 0.005007 Acc: 13.1562\n",
      " |~~ train@9216  Loss: 0.004681 Acc: 13.3594\n",
      " |~~ train@9280  Loss: 0.004846 Acc: 13.2656\n",
      " |~~ train@9344  Loss: 0.004897 Acc: 13.1719\n",
      " |~~ train@9408  Loss: 0.004961 Acc: 13.1406\n",
      " |~~ train@9472  Loss: 0.004604 Acc: 13.4062\n",
      " |~~ train@9536  Loss: 0.004670 Acc: 13.3281\n",
      " |~~ train@9600  Loss: 0.004453 Acc: 13.4844\n",
      " |~~ train@9664  Loss: 0.004804 Acc: 13.2188\n",
      " |~~ train@9728  Loss: 0.004491 Acc: 13.4375\n",
      " |~~ train@9792  Loss: 0.004836 Acc: 13.1719\n",
      " |~~ train@9856  Loss: 0.004812 Acc: 13.1875\n",
      " |~~ train@9920  Loss: 0.004902 Acc: 13.0938\n",
      " |~~ train@9984  Loss: 0.004438 Acc: 13.4375\n",
      " |~~ train@10048  Loss: 0.005004 Acc: 13.0156\n",
      " |~~ train@10112  Loss: 0.004422 Acc: 13.4219\n",
      " |~~ train@10176  Loss: 0.004670 Acc: 13.2500\n",
      " |~~ train@10240  Loss: 0.004322 Acc: 13.5156\n",
      " |~~ train@10304  Loss: 0.004870 Acc: 13.0938\n",
      " |~~ train@10368  Loss: 0.004717 Acc: 13.2344\n",
      " |~~ train@10432  Loss: 0.004577 Acc: 13.3125\n",
      " |~~ train@10496  Loss: 0.004398 Acc: 13.3906\n",
      " |~~ train@10560  Loss: 0.004424 Acc: 13.3594\n",
      " |~~ train@10624  Loss: 0.004437 Acc: 13.3281\n",
      " |~~ train@10688  Loss: 0.004466 Acc: 13.2812\n",
      " |~~ train@10752  Loss: 0.004457 Acc: 13.3281\n",
      " |~~ train@10816  Loss: 0.004620 Acc: 13.2188\n",
      " |~~ train@10880  Loss: 0.004524 Acc: 13.2188\n",
      " |~~ train@10944  Loss: 0.004390 Acc: 13.3594\n",
      " |~~ train@11008  Loss: 0.004700 Acc: 13.1250\n",
      " |~~ train@11072  Loss: 0.004776 Acc: 13.0469\n",
      " |~~ train@11136  Loss: 0.004382 Acc: 13.3125\n",
      " |~~ train@11200  Loss: 0.004500 Acc: 13.2500\n",
      " |~~ train@11264  Loss: 0.004618 Acc: 13.1719\n",
      " |~~ train@11328  Loss: 0.004616 Acc: 13.1094\n",
      " |~~ train@11392  Loss: 0.004331 Acc: 13.3125\n",
      " |~~ train@11456  Loss: 0.004178 Acc: 13.4375\n",
      " |~~ train@11520  Loss: 0.004308 Acc: 13.3594\n",
      " |~~ train@11584  Loss: 0.004496 Acc: 13.1719\n",
      " |~~ train@11648  Loss: 0.004131 Acc: 13.4531\n",
      " |~~ train@11712  Loss: 0.004049 Acc: 13.4531\n",
      " |~~ train@11776  Loss: 0.004288 Acc: 13.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@11840  Loss: 0.004195 Acc: 13.3594\n",
      " |~~ train@11904  Loss: 0.004112 Acc: 13.3906\n",
      " |~~ train@11968  Loss: 0.004030 Acc: 13.4844\n",
      " |~~ train@12032  Loss: 0.004292 Acc: 13.2969\n",
      " |~~ train@12096  Loss: 0.004419 Acc: 13.1719\n",
      " |~~ train@12160  Loss: 0.004275 Acc: 13.2812\n",
      " |~~ train@12224  Loss: 0.004281 Acc: 13.2656\n",
      " |~~ train@12288  Loss: 0.003991 Acc: 13.4531\n",
      " |~~ train@12352  Loss: 0.004662 Acc: 13.0469\n",
      " |~~ train@12416  Loss: 0.004705 Acc: 13.0312\n",
      " |~~ train@12480  Loss: 0.004044 Acc: 13.3906\n",
      " |~~ train@12544  Loss: 0.004524 Acc: 13.1094\n",
      " |~~ train@12608  Loss: 0.004364 Acc: 13.1562\n",
      " |~~ train@12672  Loss: 0.004014 Acc: 13.4062\n",
      " |~~ train@12736  Loss: 0.004142 Acc: 13.3281\n",
      " |~~ train@12800  Loss: 0.003777 Acc: 13.5469\n",
      " |~~ train@12864  Loss: 0.004000 Acc: 13.4062\n",
      " |~~ train@12928  Loss: 0.003988 Acc: 13.4219\n",
      " |~~ train@12992  Loss: 0.004292 Acc: 13.2031\n",
      " |~~ train@13056  Loss: 0.004334 Acc: 13.1406\n",
      " |~~ train@13120  Loss: 0.004320 Acc: 13.2031\n",
      " |~~ train@13184  Loss: 0.004065 Acc: 13.3125\n",
      " |~~ train@13248  Loss: 0.004024 Acc: 13.3438\n",
      " |~~ train@13312  Loss: 0.003857 Acc: 13.4531\n",
      " |~~ train@13376  Loss: 0.003857 Acc: 13.4219\n",
      " |~~ train@13440  Loss: 0.004251 Acc: 13.1719\n",
      " |~~ train@13504  Loss: 0.003858 Acc: 13.4219\n",
      " |~~ train@13568  Loss: 0.003998 Acc: 13.3125\n",
      " |~~ train@13632  Loss: 0.004010 Acc: 13.3125\n",
      " |~~ train@13696  Loss: 0.004135 Acc: 13.2500\n",
      " |~~ train@13760  Loss: 0.004104 Acc: 13.3125\n",
      " |~~ train@13824  Loss: 0.003685 Acc: 13.5156\n",
      " |~~ train@13888  Loss: 0.004213 Acc: 13.1562\n",
      " |~~ train@13952  Loss: 0.004151 Acc: 13.2031\n",
      " |~~ train@14016  Loss: 0.004025 Acc: 13.2969\n",
      " |~~ train@14080  Loss: 0.004043 Acc: 13.2812\n",
      " |~~ train@14144  Loss: 0.004208 Acc: 13.1562\n",
      " |~~ train@14208  Loss: 0.003975 Acc: 13.3281\n",
      " |~~ train@14272  Loss: 0.004261 Acc: 13.1562\n",
      " |~~ train@14336  Loss: 0.003920 Acc: 13.3438\n",
      " |~~ train@14400  Loss: 0.004157 Acc: 13.1875\n",
      " |~~ train@14464  Loss: 0.004114 Acc: 13.2188\n",
      " |~~ train@14528  Loss: 0.003996 Acc: 13.2656\n",
      " |~~ train@14592  Loss: 0.004034 Acc: 13.2188\n",
      " |~~ train@14656  Loss: 0.004081 Acc: 13.2188\n",
      " |~~ train@14720  Loss: 0.003881 Acc: 13.3125\n",
      " |~~ train@14784  Loss: 0.003914 Acc: 13.3125\n",
      " |~~ train@14848  Loss: 0.004072 Acc: 13.2500\n",
      " |~~ train@14912  Loss: 0.003714 Acc: 13.3906\n",
      " |~~ train@14976  Loss: 0.003900 Acc: 13.2969\n",
      " |~~ train@15040  Loss: 0.003898 Acc: 13.2812\n",
      " |~~ train@15104  Loss: 0.003805 Acc: 13.3281\n",
      " |~~ train@15168  Loss: 0.003720 Acc: 13.3750\n",
      " |~~ train@15232  Loss: 0.004159 Acc: 13.1719\n",
      " |~~ train@15296  Loss: 0.003677 Acc: 13.3906\n",
      " |~~ train@15360  Loss: 0.003689 Acc: 13.3906\n",
      " |~~ train@15424  Loss: 0.004048 Acc: 13.1875\n",
      " |~~ train@15488  Loss: 0.003748 Acc: 13.3438\n",
      " |~~ train@15552  Loss: 0.003696 Acc: 13.3906\n",
      " |~~ train@15616  Loss: 0.003775 Acc: 13.3438\n",
      " |~~ train@15680  Loss: 0.003809 Acc: 13.3125\n",
      " |~~ train@15744  Loss: 0.003928 Acc: 13.2344\n",
      " |~~ train@15808  Loss: 0.004210 Acc: 13.1250\n",
      " |~~ train@15872  Loss: 0.003769 Acc: 13.3750\n",
      " |~~ train@15936  Loss: 0.003716 Acc: 13.3750\n",
      " |~~ train@16000  Loss: 0.003732 Acc: 13.3594\n",
      " |~~ train@16064  Loss: 0.003713 Acc: 13.3594\n",
      " |~~ train@16128  Loss: 0.003970 Acc: 13.2031\n",
      " |~~ train@16192  Loss: 0.003819 Acc: 13.2812\n",
      " |~~ train@16256  Loss: 0.003729 Acc: 13.3438\n",
      " |~~ train@16320  Loss: 0.003759 Acc: 13.2812\n",
      " |~~ train@16384  Loss: 0.003811 Acc: 13.2969\n",
      " |~~ train@16448  Loss: 0.003444 Acc: 13.4844\n",
      " |~~ train@16512  Loss: 0.003895 Acc: 13.2344\n",
      " |~~ train@16576  Loss: 0.003728 Acc: 13.3281\n",
      " |~~ train@16640  Loss: 0.003788 Acc: 13.2500\n",
      " |~~ train@16704  Loss: 0.003647 Acc: 13.3750\n",
      " |~~ train@16768  Loss: 0.003842 Acc: 13.2656\n",
      " |~~ train@16832  Loss: 0.004007 Acc: 13.1406\n",
      " |~~ train@16896  Loss: 0.003608 Acc: 13.3906\n",
      " |~~ train@16960  Loss: 0.003853 Acc: 13.2344\n",
      " |~~ train@17024  Loss: 0.003966 Acc: 13.2031\n",
      " |~~ train@17088  Loss: 0.004197 Acc: 13.0312\n",
      " |~~ train@17152  Loss: 0.003619 Acc: 13.3281\n",
      " |~~ train@17216  Loss: 0.003786 Acc: 13.2656\n",
      " |~~ train@17280  Loss: 0.004003 Acc: 13.1250\n",
      " |~~ train@17344  Loss: 0.003846 Acc: 13.2500\n",
      " |~~ train@17408  Loss: 0.003914 Acc: 13.1719\n",
      " |~~ train@17472  Loss: 0.003858 Acc: 13.2344\n",
      " |~~ train@17536  Loss: 0.003511 Acc: 13.4062\n",
      " |~~ train@17600  Loss: 0.003549 Acc: 13.3750\n",
      " |~~ train@17664  Loss: 0.003656 Acc: 13.3125\n",
      " |~~ train@17728  Loss: 0.003911 Acc: 13.1562\n",
      " |~~ train@17792  Loss: 0.003764 Acc: 13.2656\n",
      " |~~ train@17856  Loss: 0.003657 Acc: 13.2969\n",
      " |~~ train@17920  Loss: 0.004016 Acc: 13.1406\n",
      " |~~ train@17984  Loss: 0.003831 Acc: 13.1719\n",
      " |~~ train@18048  Loss: 0.003601 Acc: 13.3594\n",
      " |~~ train@18112  Loss: 0.003802 Acc: 13.2188\n",
      " |~~ train@18176  Loss: 0.003819 Acc: 13.2188\n",
      " |~~ train@18240  Loss: 0.003839 Acc: 13.1875\n",
      " |~~ train@18304  Loss: 0.003490 Acc: 13.3906\n",
      " |~~ train@18368  Loss: 0.004068 Acc: 13.0781\n",
      " |~~ train@18432  Loss: 0.003850 Acc: 13.2031\n",
      " |~~ train@18496  Loss: 0.003708 Acc: 13.2656\n",
      " |~~ train@18560  Loss: 0.003651 Acc: 13.2812\n",
      " |~~ train@18624  Loss: 0.003506 Acc: 13.3594\n",
      " |~~ train@18688  Loss: 0.003693 Acc: 13.2500\n",
      " |~~ train@18752  Loss: 0.003445 Acc: 13.4219\n",
      " |~~ train@18816  Loss: 0.003383 Acc: 13.4062\n",
      " |~~ train@18880  Loss: 0.003525 Acc: 13.3125\n",
      " |~~ train@18944  Loss: 0.003556 Acc: 13.3281\n",
      " |~~ train@19008  Loss: 0.003661 Acc: 13.2656\n",
      " |~~ train@19072  Loss: 0.003772 Acc: 13.1719\n",
      " |~~ train@19136  Loss: 0.003936 Acc: 13.1562\n",
      " |~~ train@19200  Loss: 0.003531 Acc: 13.3438\n",
      " |~~ train@19264  Loss: 0.003350 Acc: 13.4219\n",
      " |~~ train@19328  Loss: 0.003484 Acc: 13.3750\n",
      " |~~ train@19392  Loss: 0.003630 Acc: 13.2656\n",
      " |~~ train@19456  Loss: 0.003488 Acc: 13.3281\n",
      " |~~ train@19520  Loss: 0.003492 Acc: 13.3594\n",
      " |~~ train@19584  Loss: 0.003860 Acc: 13.1250\n",
      " |~~ train@19648  Loss: 0.003661 Acc: 13.2031\n",
      " |~~ train@19712  Loss: 0.003569 Acc: 13.3281\n",
      " |~~ train@19776  Loss: 0.003629 Acc: 13.2812\n",
      " |~~ train@19840  Loss: 0.003611 Acc: 13.2500\n",
      " |~~ train@19904  Loss: 0.003660 Acc: 13.2500\n",
      " |~~ train@19968  Loss: 0.003223 Acc: 13.4688\n",
      " |~~ train@20032  Loss: 0.003629 Acc: 13.2031\n",
      " |~~ train@20096  Loss: 0.003342 Acc: 13.3906\n",
      " |~~ train@20160  Loss: 0.003499 Acc: 13.3281\n",
      " |~~ train@20224  Loss: 0.003411 Acc: 13.3750\n",
      " |~~ train@20288  Loss: 0.003805 Acc: 13.1562\n",
      " |~~ train@20352  Loss: 0.003728 Acc: 13.2344\n",
      " |~~ train@20416  Loss: 0.003667 Acc: 13.2031\n",
      " |~~ train@20480  Loss: 0.003571 Acc: 13.2656\n",
      " |~~ train@20544  Loss: 0.003348 Acc: 13.4062\n",
      " |~~ train@20608  Loss: 0.003566 Acc: 13.2500\n",
      " |~~ train@20672  Loss: 0.003172 Acc: 13.4375\n",
      " |~~ train@20736  Loss: 0.003172 Acc: 13.4688\n",
      " |~~ train@20800  Loss: 0.003672 Acc: 13.1875\n",
      " |~~ train@20864  Loss: 0.003840 Acc: 13.1250\n",
      " |~~ train@20928  Loss: 0.003823 Acc: 13.1250\n",
      " |~~ train@20992  Loss: 0.003698 Acc: 13.2031\n",
      " |~~ train@21056  Loss: 0.003208 Acc: 13.4375\n",
      " |~~ train@21120  Loss: 0.003217 Acc: 13.4219\n",
      " |~~ train@21184  Loss: 0.003362 Acc: 13.3438\n",
      " |~~ train@21248  Loss: 0.003502 Acc: 13.2812\n",
      " |~~ train@21312  Loss: 0.003311 Acc: 13.3750\n",
      " |~~ train@21376  Loss: 0.003067 Acc: 13.5156\n",
      " |~~ train@21440  Loss: 0.003496 Acc: 13.2969\n",
      " |~~ train@21504  Loss: 0.003425 Acc: 13.3281\n",
      " |~~ train@21568  Loss: 0.003385 Acc: 13.3594\n",
      " |~~ train@21632  Loss: 0.003529 Acc: 13.2344\n",
      " |~~ train@21696  Loss: 0.003907 Acc: 13.1094\n",
      " |~~ train@21760  Loss: 0.003934 Acc: 13.0781\n",
      " |~~ train@21824  Loss: 0.003878 Acc: 13.0938\n",
      " |~~ train@21888  Loss: 0.003139 Acc: 13.4531\n",
      " |~~ train@21952  Loss: 0.003713 Acc: 13.1719\n",
      " |~~ train@22016  Loss: 0.003600 Acc: 13.2500\n",
      " |~~ train@22080  Loss: 0.003380 Acc: 13.3438\n",
      " |~~ train@22144  Loss: 0.003461 Acc: 13.2812\n",
      " |~~ train@22208  Loss: 0.003109 Acc: 13.4688\n",
      " |~~ train@22272  Loss: 0.003422 Acc: 13.2812\n",
      " |~~ train@22336  Loss: 0.003311 Acc: 13.3438\n",
      " |~~ train@22400  Loss: 0.003472 Acc: 13.2500\n",
      " |~~ train@22464  Loss: 0.003533 Acc: 13.2500\n",
      " |~~ train@22528  Loss: 0.003535 Acc: 13.2812\n",
      " |~~ train@22592  Loss: 0.003374 Acc: 13.3125\n",
      " |~~ train@22656  Loss: 0.003854 Acc: 13.1094\n",
      " |~~ train@22720  Loss: 0.003545 Acc: 13.2656\n",
      " |~~ train@22784  Loss: 0.003578 Acc: 13.2188\n",
      " |~~ train@22848  Loss: 0.003600 Acc: 13.1875\n",
      " |~~ train@22912  Loss: 0.003277 Acc: 13.3438\n",
      " |~~ train@22976  Loss: 0.003865 Acc: 13.0625\n",
      " |~~ train@23040  Loss: 0.003781 Acc: 13.0938\n",
      " |~~ train@23104  Loss: 0.003202 Acc: 13.3438\n",
      " |~~ train@23168  Loss: 0.003464 Acc: 13.2812\n",
      " |~~ train@23232  Loss: 0.003402 Acc: 13.3125\n",
      " |~~ train@23296  Loss: 0.003260 Acc: 13.3594\n",
      " |~~ train@23360  Loss: 0.003515 Acc: 13.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@23424  Loss: 0.003152 Acc: 13.4062\n",
      " |~~ train@23488  Loss: 0.003288 Acc: 13.3281\n",
      " |~~ train@23552  Loss: 0.003898 Acc: 13.0469\n",
      " |~~ train@23616  Loss: 0.003126 Acc: 13.3906\n",
      " |~~ train@23680  Loss: 0.003249 Acc: 13.3281\n",
      " |~~ train@23744  Loss: 0.003262 Acc: 13.3906\n",
      " |~~ train@23808  Loss: 0.003376 Acc: 13.2812\n",
      " |~~ train@23872  Loss: 0.003843 Acc: 13.0781\n",
      " |~~ train@23936  Loss: 0.003186 Acc: 13.3750\n",
      " |~~ train@24000  Loss: 0.003273 Acc: 13.3125\n",
      " |~~ train@24064  Loss: 0.003799 Acc: 13.1094\n",
      " |~~ train@24128  Loss: 0.003291 Acc: 13.3125\n",
      " |~~ train@24192  Loss: 0.003160 Acc: 13.4219\n",
      " |~~ train@24256  Loss: 0.003113 Acc: 13.4219\n",
      " |~~ train@24320  Loss: 0.003111 Acc: 13.4375\n",
      " |~~ train@24384  Loss: 0.003741 Acc: 13.1406\n",
      " |~~ train@24448  Loss: 0.004089 Acc: 12.9375\n",
      " |~~ train@24512  Loss: 0.003621 Acc: 13.1719\n",
      " |~~ train@24576  Loss: 0.003661 Acc: 13.1719\n",
      " |~~ train@24640  Loss: 0.003691 Acc: 13.1406\n",
      " |~~ train@24704  Loss: 0.003517 Acc: 13.1875\n",
      " |~~ train@24768  Loss: 0.003851 Acc: 13.0312\n",
      " |~~ train@24832  Loss: 0.003691 Acc: 13.0938\n",
      " |~~ train@24896  Loss: 0.003160 Acc: 13.4062\n",
      " |~~ train@24960  Loss: 0.003758 Acc: 13.0781\n",
      " |~~ train@25024  Loss: 0.003325 Acc: 13.2969\n",
      " |~~ train@25088  Loss: 0.003509 Acc: 13.2031\n",
      " |~~ train@25152  Loss: 0.003316 Acc: 13.3125\n",
      " |~~ train@25216  Loss: 0.003270 Acc: 13.3438\n",
      " |~~ train@25280  Loss: 0.002847 Acc: 13.5156\n",
      " |~~ train@25344  Loss: 0.003297 Acc: 13.2812\n",
      " |~~ train@25408  Loss: 0.003398 Acc: 13.2500\n",
      " |~~ train@25472  Loss: 0.003328 Acc: 13.3125\n",
      " |~~ train@25536  Loss: 0.003409 Acc: 13.2812\n",
      " |~~ train@25600  Loss: 0.003656 Acc: 13.1094\n",
      " |~~ train@25664  Loss: 0.003367 Acc: 13.2656\n",
      " |~~ train@25728  Loss: 0.003047 Acc: 13.4219\n",
      " |~~ train@25792  Loss: 0.003091 Acc: 13.4219\n",
      " |~~ train@25856  Loss: 0.002806 Acc: 13.5781\n",
      " |~~ train@25920  Loss: 0.003343 Acc: 13.2969\n",
      " |~~ train@25984  Loss: 0.003605 Acc: 13.1406\n",
      " |~~ train@26048  Loss: 0.003418 Acc: 13.2812\n",
      " |~~ train@26112  Loss: 0.003031 Acc: 13.4062\n",
      " |~~ train@26176  Loss: 0.003342 Acc: 13.2812\n",
      " |~~ train@26240  Loss: 0.003295 Acc: 13.2969\n",
      " |~~ train@26304  Loss: 0.003199 Acc: 13.3281\n",
      " |~~ train@26368  Loss: 0.003386 Acc: 13.2344\n",
      " |~~ train@26432  Loss: 0.003284 Acc: 13.2969\n",
      " |~~ train@26496  Loss: 0.003098 Acc: 13.3750\n",
      " |~~ train@26560  Loss: 0.003288 Acc: 13.2812\n",
      " |~~ train@26624  Loss: 0.003208 Acc: 13.3281\n",
      " |~~ train@26688  Loss: 0.003347 Acc: 13.2656\n",
      " |~~ train@26752  Loss: 0.003614 Acc: 13.1562\n",
      " |~~ train@26816  Loss: 0.003758 Acc: 13.1094\n",
      " |~~ train@26880  Loss: 0.002972 Acc: 13.4062\n",
      " |~~ train@26944  Loss: 0.003300 Acc: 13.3281\n",
      " |~~ train@27008  Loss: 0.003506 Acc: 13.2031\n",
      " |~~ train@27072  Loss: 0.003398 Acc: 13.2188\n",
      " |~~ train@27136  Loss: 0.003410 Acc: 13.2344\n",
      " |~~ train@27200  Loss: 0.003404 Acc: 13.2188\n",
      " |~~ train@27264  Loss: 0.003441 Acc: 13.1875\n",
      " |~~ train@27328  Loss: 0.003392 Acc: 13.2500\n",
      " |~~ train@27392  Loss: 0.003231 Acc: 13.3281\n",
      " |~~ train@27456  Loss: 0.003041 Acc: 13.3906\n",
      " |~~ train@27520  Loss: 0.003205 Acc: 13.3281\n",
      " |~~ train@27584  Loss: 0.003207 Acc: 13.2969\n",
      " |~~ train@27648  Loss: 0.003235 Acc: 13.2969\n",
      " |~~ train@27712  Loss: 0.003310 Acc: 13.2812\n",
      " |~~ train@27776  Loss: 0.003287 Acc: 13.2656\n",
      " |~~ train@27840  Loss: 0.003623 Acc: 13.1250\n",
      " |~~ train@27904  Loss: 0.003080 Acc: 13.3438\n",
      " |~~ train@27968  Loss: 0.003009 Acc: 13.4219\n",
      " |~~ train@28032  Loss: 0.002954 Acc: 13.4219\n",
      " |~~ train@28096  Loss: 0.003317 Acc: 13.2500\n",
      " |~~ train@28160  Loss: 0.003808 Acc: 13.0469\n",
      " |~~ train@28224  Loss: 0.002991 Acc: 13.4062\n",
      " |~~ train@28288  Loss: 0.003330 Acc: 13.2500\n",
      " |~~ train@28352  Loss: 0.003664 Acc: 13.0938\n",
      " |~~ train@28416  Loss: 0.003536 Acc: 13.1406\n",
      " |~~ train@28480  Loss: 0.003333 Acc: 13.2812\n",
      " |~~ train@28544  Loss: 0.003554 Acc: 13.1719\n",
      " |~~ train@28608  Loss: 0.003428 Acc: 13.1875\n",
      " |~~ train@28672  Loss: 0.003448 Acc: 13.1875\n",
      " |~~ train@28736  Loss: 0.003198 Acc: 13.2656\n",
      " |~~ train@28800  Loss: 0.003669 Acc: 13.0781\n",
      " |~~ train@28864  Loss: 0.002982 Acc: 13.4062\n",
      " |~~ train@28928  Loss: 0.003298 Acc: 13.2969\n",
      " |~~ train@28992  Loss: 0.003093 Acc: 13.3594\n",
      " |~~ train@29056  Loss: 0.003453 Acc: 13.1875\n",
      " |~~ train@29120  Loss: 0.003410 Acc: 13.2188\n",
      " |~~ train@29184  Loss: 0.003479 Acc: 13.2344\n",
      " |~~ train@29248  Loss: 0.003329 Acc: 13.1875\n",
      " |~~ train@29312  Loss: 0.003029 Acc: 13.3750\n",
      " |~~ train@29376  Loss: 0.003034 Acc: 13.3750\n",
      " |~~ train@29440  Loss: 0.002927 Acc: 13.4062\n",
      " |~~ train@29504  Loss: 0.003723 Acc: 13.0625\n",
      " |~~ train@29568  Loss: 0.003397 Acc: 13.2188\n",
      " |~~ train@29632  Loss: 0.003480 Acc: 13.1562\n",
      " |~~ train@29696  Loss: 0.002993 Acc: 13.4062\n",
      " |~~ train@29760  Loss: 0.003135 Acc: 13.3281\n",
      " |~~ train@29824  Loss: 0.002805 Acc: 13.4844\n",
      " |~~ train@29888  Loss: 0.003115 Acc: 13.3125\n",
      " |~~ train@29952  Loss: 0.002957 Acc: 13.3906\n",
      " |~~ train@30016  Loss: 0.003263 Acc: 13.2812\n",
      " |~~ train@30080  Loss: 0.003740 Acc: 13.0156\n",
      " |~~ train@30144  Loss: 0.002998 Acc: 13.3750\n",
      " |~~ train@30208  Loss: 0.003225 Acc: 13.3125\n",
      " |~~ train@30272  Loss: 0.003653 Acc: 13.1094\n",
      " |~~ train@30336  Loss: 0.003380 Acc: 13.2188\n",
      " |~~ train@30400  Loss: 0.003253 Acc: 13.2656\n",
      " |~~ train@30464  Loss: 0.003382 Acc: 13.2188\n",
      " |~~ train@30528  Loss: 0.002999 Acc: 13.3906\n",
      " |~~ train@30592  Loss: 0.003160 Acc: 13.2812\n",
      " |~~ train@30656  Loss: 0.003217 Acc: 13.2812\n",
      " |~~ train@30720  Loss: 0.003447 Acc: 13.1875\n",
      " |~~ train@30784  Loss: 0.003315 Acc: 13.2656\n",
      " |~~ train@30848  Loss: 0.003055 Acc: 13.3750\n",
      " |~~ train@30912  Loss: 0.003114 Acc: 13.3281\n",
      " |~~ train@30976  Loss: 0.003174 Acc: 13.2812\n",
      " |~~ train@31040  Loss: 0.003374 Acc: 13.1875\n",
      " |~~ train@31104  Loss: 0.003019 Acc: 13.3906\n",
      " |~~ train@31168  Loss: 0.003377 Acc: 13.1875\n",
      " |~~ train@31232  Loss: 0.003178 Acc: 13.3125\n",
      " |~~ train@31296  Loss: 0.002994 Acc: 13.3906\n",
      " |~~ train@31360  Loss: 0.003587 Acc: 13.1094\n",
      " |~~ train@31424  Loss: 0.002937 Acc: 13.3906\n",
      " |~~ train@31488  Loss: 0.003319 Acc: 13.2344\n",
      " |~~ train@31552  Loss: 0.003889 Acc: 13.0312\n",
      " |~~ train@31616  Loss: 0.002923 Acc: 13.3906\n",
      " |~~ train@31680  Loss: 0.003085 Acc: 13.3750\n",
      " |~~ train@31744  Loss: 0.003469 Acc: 13.1875\n",
      " |~~ train@31808  Loss: 0.002641 Acc: 13.5469\n",
      " |~~ train@31872  Loss: 0.003309 Acc: 13.2188\n",
      " |~~ train@31936  Loss: 0.003531 Acc: 13.1406\n",
      " |~~ train@32000  Loss: 0.003339 Acc: 13.2188\n",
      " |~~ train@32064  Loss: 0.003183 Acc: 13.2969\n",
      " |~~ train@32128  Loss: 0.002610 Acc: 13.5625\n",
      " |~~ train@32192  Loss: 0.003056 Acc: 13.3438\n",
      " |~~ train@32256  Loss: 0.003695 Acc: 13.0625\n",
      " |~~ train@32320  Loss: 0.003622 Acc: 13.1094\n",
      " |~~ train@32384  Loss: 0.002921 Acc: 13.4062\n",
      " |~~ train@32448  Loss: 0.003357 Acc: 13.1875\n",
      " |~~ train@32512  Loss: 0.003318 Acc: 13.2031\n",
      " |~~ train@32576  Loss: 0.003061 Acc: 13.3281\n",
      " |~~ train@32640  Loss: 0.003467 Acc: 13.1562\n",
      " |~~ train@32704  Loss: 0.003259 Acc: 13.2500\n",
      " |~~ train@32768  Loss: 0.003136 Acc: 13.2812\n",
      " |~~ train@32832  Loss: 0.003371 Acc: 13.2031\n",
      " |~~ train@32896  Loss: 0.002926 Acc: 13.3438\n",
      " |~~ train@32960  Loss: 0.002986 Acc: 13.3750\n",
      " |~~ train@33024  Loss: 0.003703 Acc: 13.0156\n",
      " |~~ train@33088  Loss: 0.003347 Acc: 13.1719\n",
      " |~~ train@33152  Loss: 0.003086 Acc: 13.3281\n",
      " |~~ train@33216  Loss: 0.003619 Acc: 13.0312\n",
      " |~~ train@33280  Loss: 0.003416 Acc: 13.1875\n",
      " |~~ train@33344  Loss: 0.003202 Acc: 13.2656\n",
      " |~~ train@33408  Loss: 0.003532 Acc: 13.1094\n",
      " |~~ train@33472  Loss: 0.003320 Acc: 13.2188\n",
      " |~~ train@33536  Loss: 0.003216 Acc: 13.2656\n",
      " |~~ train@33600  Loss: 0.003230 Acc: 13.2500\n",
      " |~~ train@33664  Loss: 0.003119 Acc: 13.3438\n",
      " |~~ train@33728  Loss: 0.003333 Acc: 13.2188\n",
      " |~~ train@33792  Loss: 0.003309 Acc: 13.2188\n",
      " |~~ train@33856  Loss: 0.002869 Acc: 13.4062\n",
      " |~~ train@33920  Loss: 0.003103 Acc: 13.3125\n",
      " |~~ train@33984  Loss: 0.003548 Acc: 13.1250\n",
      " |~~ train@34048  Loss: 0.002931 Acc: 13.3594\n",
      " |~~ train@34112  Loss: 0.003475 Acc: 13.1562\n",
      " |~~ train@34176  Loss: 0.002771 Acc: 13.4531\n",
      " |~~ train@34240  Loss: 0.003692 Acc: 13.0781\n",
      " |~~ train@34304  Loss: 0.003021 Acc: 13.3281\n",
      " |~~ train@34368  Loss: 0.003332 Acc: 13.1875\n",
      " |~~ train@34432  Loss: 0.002900 Acc: 13.3750\n",
      " |~~ train@34496  Loss: 0.003718 Acc: 13.0469\n",
      " |~~ train@34560  Loss: 0.003456 Acc: 13.1719\n",
      " |~~ train@34624  Loss: 0.003078 Acc: 13.3438\n",
      " |~~ train@34688  Loss: 0.002999 Acc: 13.3281\n",
      " |~~ train@34752  Loss: 0.003067 Acc: 13.3125\n",
      " |~~ train@34816  Loss: 0.003774 Acc: 13.0781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@34880  Loss: 0.002868 Acc: 13.3906\n",
      " |~~ train@34944  Loss: 0.003156 Acc: 13.2969\n",
      " |~~ train@35008  Loss: 0.002904 Acc: 13.3438\n",
      " |~~ train@35072  Loss: 0.003107 Acc: 13.2969\n",
      " |~~ train@35136  Loss: 0.003541 Acc: 13.1250\n",
      " |~~ train@35200  Loss: 0.003317 Acc: 13.1875\n",
      " |~~ train@35264  Loss: 0.003671 Acc: 13.0938\n",
      " |~~ train@35328  Loss: 0.003226 Acc: 13.2656\n",
      " |~~ train@35392  Loss: 0.003062 Acc: 13.2812\n",
      " |~~ train@35456  Loss: 0.002995 Acc: 13.3438\n",
      " |~~ train@35520  Loss: 0.002912 Acc: 13.3906\n",
      " |~~ train@35584  Loss: 0.003215 Acc: 13.2812\n",
      " |~~ train@35648  Loss: 0.003285 Acc: 13.2031\n",
      " |~~ train@35712  Loss: 0.003546 Acc: 13.1250\n",
      " |~~ train@35776  Loss: 0.003116 Acc: 13.3125\n",
      " |~~ train@35840  Loss: 0.002921 Acc: 13.3594\n",
      " |~~ train@35904  Loss: 0.003045 Acc: 13.2969\n",
      " |~~ train@35968  Loss: 0.003299 Acc: 13.2031\n",
      " |~~ train@36032  Loss: 0.003260 Acc: 13.1875\n",
      " |~~ train@36096  Loss: 0.003170 Acc: 13.2812\n",
      " |~~ train@36160  Loss: 0.003251 Acc: 13.2188\n",
      " |~~ train@36224  Loss: 0.003692 Acc: 13.0000\n",
      " |~~ train@36288  Loss: 0.002866 Acc: 13.4219\n",
      " |~~ train@36352  Loss: 0.002899 Acc: 13.3750\n",
      " |~~ train@36416  Loss: 0.003080 Acc: 13.3438\n",
      " |~~ train@36480  Loss: 0.003482 Acc: 13.0938\n",
      " |~~ train@36544  Loss: 0.003468 Acc: 13.1250\n",
      " |~~ train@36608  Loss: 0.003469 Acc: 13.1562\n",
      " |~~ train@36672  Loss: 0.003260 Acc: 13.1719\n",
      " |~~ train@36736  Loss: 0.003361 Acc: 13.1719\n",
      " |~~ train@36800  Loss: 0.002939 Acc: 13.3594\n",
      " |~~ train@36864  Loss: 0.002938 Acc: 13.3438\n",
      " |~~ train@36928  Loss: 0.003243 Acc: 13.2344\n",
      " |~~ train@36992  Loss: 0.002954 Acc: 13.3594\n",
      " |~~ train@37056  Loss: 0.002997 Acc: 13.4062\n",
      " |~~ train@37120  Loss: 0.002973 Acc: 13.3750\n",
      " |~~ train@37184  Loss: 0.003640 Acc: 13.0625\n",
      " |~~ train@37248  Loss: 0.002792 Acc: 13.3906\n",
      " |~~ train@37312  Loss: 0.003084 Acc: 13.3125\n",
      " |~~ train@37376  Loss: 0.002981 Acc: 13.3438\n",
      " |~~ train@37440  Loss: 0.003352 Acc: 13.1562\n",
      " |~~ train@37504  Loss: 0.003078 Acc: 13.2812\n",
      " |~~ train@37568  Loss: 0.003349 Acc: 13.2188\n",
      " |~~ train@37632  Loss: 0.002858 Acc: 13.4062\n",
      " |~~ train@37696  Loss: 0.003414 Acc: 13.1719\n",
      " |~~ train@37760  Loss: 0.002977 Acc: 13.3750\n",
      " |~~ train@37824  Loss: 0.003207 Acc: 13.2344\n",
      " |~~ train@37888  Loss: 0.003264 Acc: 13.1875\n",
      " |~~ train@37952  Loss: 0.003042 Acc: 13.2969\n",
      " |~~ train@38016  Loss: 0.002702 Acc: 13.4688\n",
      " |~~ train@38080  Loss: 0.002578 Acc: 13.4688\n",
      " |~~ train@38144  Loss: 0.002662 Acc: 13.4531\n",
      " |~~ train@38208  Loss: 0.003181 Acc: 13.2500\n",
      " |~~ train@38272  Loss: 0.002903 Acc: 13.3594\n",
      " |~~ train@38336  Loss: 0.003152 Acc: 13.2812\n",
      " |~~ train@38400  Loss: 0.002696 Acc: 13.4219\n",
      " |~~ train@38464  Loss: 0.003117 Acc: 13.2969\n",
      " |~~ train@38528  Loss: 0.002911 Acc: 13.3438\n",
      " |~~ train@38592  Loss: 0.003063 Acc: 13.2812\n",
      " |~~ train@38656  Loss: 0.003047 Acc: 13.3125\n",
      " |~~ train@38720  Loss: 0.002797 Acc: 13.4062\n",
      " |~~ train@38784  Loss: 0.003213 Acc: 13.2344\n",
      " |~~ train@38848  Loss: 0.003060 Acc: 13.2812\n",
      " |~~ train@38912  Loss: 0.003499 Acc: 13.1094\n",
      " |~~ train@38976  Loss: 0.003127 Acc: 13.2656\n",
      " |~~ train@39040  Loss: 0.003046 Acc: 13.3125\n",
      " |~~ train@39104  Loss: 0.002871 Acc: 13.3594\n",
      " |~~ train@39168  Loss: 0.003438 Acc: 13.1250\n",
      " |~~ train@39232  Loss: 0.003087 Acc: 13.2969\n",
      " |~~ train@39296  Loss: 0.002985 Acc: 13.3594\n",
      " |~~ train@39360  Loss: 0.003068 Acc: 13.3125\n",
      " |~~ train@39424  Loss: 0.002624 Acc: 13.4688\n",
      " |~~ train@39488  Loss: 0.003023 Acc: 13.3281\n",
      " |~~ train@39552  Loss: 0.003355 Acc: 13.1875\n",
      " |~~ train@39616  Loss: 0.002577 Acc: 13.4688\n",
      " |~~ train@39680  Loss: 0.003297 Acc: 13.2031\n",
      " |~~ train@39744  Loss: 0.003349 Acc: 13.1562\n",
      " |~~ train@39808  Loss: 0.002949 Acc: 13.2969\n",
      " |~~ train@39872  Loss: 0.002771 Acc: 13.3750\n",
      " |~~ train@39936  Loss: 0.003154 Acc: 13.2188\n",
      " |~~ train@40000  Loss: 0.003012 Acc: 13.2344\n",
      " |~~ train@40064  Loss: 0.003323 Acc: 13.1719\n",
      " |~~ train@40128  Loss: 0.002865 Acc: 13.3594\n",
      " |~~ train@40192  Loss: 0.003608 Acc: 13.0938\n",
      " |~~ train@40256  Loss: 0.003579 Acc: 13.0312\n",
      " |~~ train@40320  Loss: 0.003016 Acc: 13.2969\n",
      " |~~ train@40384  Loss: 0.002852 Acc: 13.3906\n",
      " |~~ train@40448  Loss: 0.003161 Acc: 13.2344\n",
      " |~~ train@40512  Loss: 0.003178 Acc: 13.2344\n",
      " |~~ train@40576  Loss: 0.003290 Acc: 13.1875\n",
      " |~~ train@40640  Loss: 0.002983 Acc: 13.2969\n",
      " |~~ train@40704  Loss: 0.003175 Acc: 13.2500\n",
      " |~~ train@40768  Loss: 0.003135 Acc: 13.2500\n",
      " |~~ train@40832  Loss: 0.002988 Acc: 13.2969\n",
      " |~~ train@40896  Loss: 0.002768 Acc: 13.4062\n",
      " |~~ train@40960  Loss: 0.003096 Acc: 13.2812\n",
      " |~~ train@41024  Loss: 0.003530 Acc: 13.0469\n",
      " |~~ train@41088  Loss: 0.003376 Acc: 13.1406\n",
      " |~~ train@41152  Loss: 0.002989 Acc: 13.2969\n",
      " |~~ train@41216  Loss: 0.002996 Acc: 13.2969\n",
      " |~~ train@41280  Loss: 0.003260 Acc: 13.2188\n",
      " |~~ train@41344  Loss: 0.002977 Acc: 13.2969\n",
      " |~~ train@41408  Loss: 0.002764 Acc: 13.3906\n",
      " |~~ train@41472  Loss: 0.003585 Acc: 13.0625\n",
      " |~~ train@41536  Loss: 0.002977 Acc: 13.3125\n",
      " |~~ train@41600  Loss: 0.003329 Acc: 13.1719\n",
      " |~~ train@41664  Loss: 0.002650 Acc: 13.4688\n",
      " |~~ train@41728  Loss: 0.002971 Acc: 13.3438\n",
      " |~~ train@41792  Loss: 0.003116 Acc: 13.2500\n",
      " |~~ train@41856  Loss: 0.002419 Acc: 13.5469\n",
      " |~~ train@41920  Loss: 0.003082 Acc: 13.2969\n",
      " |~~ train@41984  Loss: 0.003214 Acc: 13.2188\n",
      " |~~ train@42048  Loss: 0.003451 Acc: 13.1250\n",
      " |~~ train@42112  Loss: 0.002974 Acc: 13.3281\n",
      " |~~ train@42176  Loss: 0.002610 Acc: 13.4531\n",
      " |~~ train@42240  Loss: 0.002882 Acc: 13.3438\n",
      " |~~ train@42304  Loss: 0.003095 Acc: 13.2969\n",
      " |~~ train@42368  Loss: 0.003030 Acc: 13.3125\n",
      " |~~ train@42432  Loss: 0.002788 Acc: 13.4062\n",
      " |~~ train@42496  Loss: 0.003354 Acc: 13.1562\n",
      " |~~ train@42560  Loss: 0.003903 Acc: 13.0000\n",
      " |~~ train@42624  Loss: 0.003682 Acc: 13.0156\n",
      " |~~ train@42688  Loss: 0.002542 Acc: 13.5156\n",
      " |~~ train@42752  Loss: 0.003083 Acc: 13.2812\n",
      " |~~ train@42816  Loss: 0.002693 Acc: 13.4688\n",
      " |~~ train@42880  Loss: 0.003422 Acc: 13.1875\n",
      " |~~ train@42944  Loss: 0.002746 Acc: 13.3906\n",
      " |~~ train@43008  Loss: 0.003121 Acc: 13.2344\n",
      " |~~ train@43072  Loss: 0.002945 Acc: 13.2969\n",
      " |~~ train@43136  Loss: 0.003601 Acc: 13.0938\n",
      " |~~ train@43200  Loss: 0.003354 Acc: 13.1875\n",
      " |~~ train@43264  Loss: 0.003127 Acc: 13.2500\n",
      " |~~ train@43328  Loss: 0.003956 Acc: 12.9062\n",
      " |~~ train@43392  Loss: 0.003144 Acc: 13.2031\n",
      " |~~ train@43456  Loss: 0.002503 Acc: 13.5000\n",
      " |~~ train@43520  Loss: 0.003001 Acc: 13.3281\n",
      " |~~ train@43584  Loss: 0.002808 Acc: 13.3906\n",
      " |~~ train@43648  Loss: 0.002897 Acc: 13.3594\n",
      " |~~ train@43712  Loss: 0.003380 Acc: 13.1406\n",
      " |~~ train@43776  Loss: 0.003002 Acc: 13.2969\n",
      " |~~ train@43840  Loss: 0.003421 Acc: 13.1094\n",
      " |~~ train@43904  Loss: 0.003195 Acc: 13.2188\n",
      " |~~ train@43968  Loss: 0.003276 Acc: 13.2188\n",
      " |~~ train@44032  Loss: 0.003199 Acc: 13.2812\n",
      " |~~ train@44096  Loss: 0.003083 Acc: 13.2969\n",
      " |~~ train@44160  Loss: 0.002750 Acc: 13.3906\n",
      " |~~ train@44224  Loss: 0.003242 Acc: 13.1875\n",
      " |~~ train@44288  Loss: 0.003105 Acc: 13.2656\n",
      " |~~ train@44352  Loss: 0.003072 Acc: 13.2812\n",
      " |~~ train@44416  Loss: 0.002990 Acc: 13.2969\n",
      " |~~ train@44480  Loss: 0.003253 Acc: 13.2031\n",
      " |~~ train@44544  Loss: 0.002984 Acc: 13.2969\n",
      " |~~ train@44608  Loss: 0.003604 Acc: 13.0625\n",
      " |~~ train@44672  Loss: 0.003273 Acc: 13.1875\n",
      " |~~ train@44736  Loss: 0.002690 Acc: 13.4219\n",
      " |~~ train@44800  Loss: 0.003036 Acc: 13.2344\n",
      " |~~ train@44864  Loss: 0.003278 Acc: 13.1406\n",
      " |~~ train@44928  Loss: 0.003318 Acc: 13.2031\n",
      " |~~ train@44992  Loss: 0.003106 Acc: 13.2656\n",
      " |~~ train@45056  Loss: 0.003163 Acc: 13.2188\n",
      " |~~ train@45120  Loss: 0.003390 Acc: 13.1406\n",
      " |~~ train@45184  Loss: 0.003075 Acc: 13.2969\n",
      " |~~ train@45248  Loss: 0.003064 Acc: 13.2969\n",
      " |~~ train@45312  Loss: 0.003123 Acc: 13.2656\n",
      " |~~ train@45376  Loss: 0.003054 Acc: 13.2812\n",
      " |~~ train@45440  Loss: 0.002576 Acc: 13.4688\n",
      " |~~ train@45504  Loss: 0.003054 Acc: 13.2969\n",
      " |~~ train@45568  Loss: 0.002811 Acc: 13.3750\n",
      " |~~ train@45632  Loss: 0.003982 Acc: 12.9531\n",
      " |~~ train@45696  Loss: 0.003356 Acc: 13.1562\n",
      " |~~ train@45760  Loss: 0.002734 Acc: 13.3438\n",
      " |~~ train@45824  Loss: 0.003231 Acc: 13.2188\n",
      " |~~ train@45888  Loss: 0.002260 Acc: 13.5781\n",
      " |~~ train@45952  Loss: 0.003407 Acc: 13.1250\n",
      " |~~ train@46016  Loss: 0.003029 Acc: 13.2969\n",
      " |~~ train@46080  Loss: 0.002869 Acc: 13.3281\n",
      " |~~ train@46144  Loss: 0.003678 Acc: 12.9844\n",
      " |~~ train@46208  Loss: 0.003263 Acc: 13.1875\n",
      " |~~ train@46272  Loss: 0.002905 Acc: 13.3281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@46336  Loss: 0.003073 Acc: 13.2812\n",
      " |~~ train@46400  Loss: 0.002964 Acc: 13.2656\n",
      " |~~ train@46464  Loss: 0.003406 Acc: 13.1250\n",
      " |~~ train@46528  Loss: 0.003187 Acc: 13.2344\n",
      " |~~ train@46592  Loss: 0.002999 Acc: 13.2969\n",
      " |~~ train@46656  Loss: 0.003401 Acc: 13.1250\n",
      " |~~ train@46720  Loss: 0.002942 Acc: 13.2812\n",
      " |~~ train@46784  Loss: 0.002903 Acc: 13.2969\n",
      " |~~ train@46848  Loss: 0.003158 Acc: 13.2188\n",
      " |~~ train@46912  Loss: 0.002815 Acc: 13.3750\n",
      " |~~ train@46976  Loss: 0.003032 Acc: 13.3281\n",
      " |~~ train@47040  Loss: 0.003378 Acc: 13.1406\n",
      " |~~ train@47104  Loss: 0.003201 Acc: 13.2344\n",
      " |~~ train@47168  Loss: 0.002480 Acc: 13.4688\n",
      " |~~ train@47232  Loss: 0.002708 Acc: 13.3906\n",
      " |~~ train@47296  Loss: 0.002890 Acc: 13.3125\n",
      " |~~ train@47360  Loss: 0.002728 Acc: 13.4062\n",
      " |~~ train@47424  Loss: 0.002930 Acc: 13.2812\n",
      " |~~ train@47488  Loss: 0.002798 Acc: 13.3594\n",
      " |~~ train@47552  Loss: 0.002798 Acc: 13.3281\n",
      " |~~ train@47616  Loss: 0.002974 Acc: 13.2969\n",
      " |~~ train@47680  Loss: 0.002793 Acc: 13.3594\n",
      " |~~ train@47744  Loss: 0.003480 Acc: 13.1562\n",
      " |~~ train@47808  Loss: 0.003213 Acc: 13.1875\n",
      " |~~ train@47872  Loss: 0.003045 Acc: 13.2812\n",
      " |~~ train@47936  Loss: 0.003497 Acc: 13.0625\n",
      " |~~ train@48000  Loss: 0.002755 Acc: 13.3594\n",
      " |~~ train@48064  Loss: 0.002521 Acc: 13.4844\n",
      " |~~ train@48128  Loss: 0.003289 Acc: 13.2031\n",
      " |~~ train@48192  Loss: 0.003675 Acc: 13.0156\n",
      " |~~ train@48256  Loss: 0.003110 Acc: 13.2656\n",
      " |~~ train@48320  Loss: 0.003421 Acc: 13.1406\n",
      " |~~ train@48384  Loss: 0.003238 Acc: 13.2188\n",
      " |~~ train@48448  Loss: 0.002011 Acc: 13.6719\n",
      " |~~ train@48512  Loss: 0.003168 Acc: 13.2500\n",
      " |~~ train@48576  Loss: 0.002622 Acc: 13.4688\n",
      " |~~ train@48640  Loss: 0.002617 Acc: 13.4375\n",
      " |~~ train@48704  Loss: 0.002818 Acc: 13.3594\n",
      " |~~ train@48768  Loss: 0.002818 Acc: 13.3594\n",
      " |~~ train@48832  Loss: 0.003114 Acc: 13.2344\n",
      " |~~ train@48896  Loss: 0.003445 Acc: 13.1719\n",
      " |~~ train@48960  Loss: 0.003267 Acc: 13.1094\n",
      " |~~ train@49024  Loss: 0.003682 Acc: 13.0000\n",
      " |~~ train@49088  Loss: 0.002773 Acc: 13.4062\n",
      " |~~ train@49152  Loss: 0.003111 Acc: 13.2500\n",
      " |~~ train@49216  Loss: 0.002776 Acc: 13.3750\n",
      " |~~ train@49280  Loss: 0.003119 Acc: 13.2500\n",
      " |~~ train@49344  Loss: 0.003589 Acc: 13.0781\n",
      " |~~ train@49408  Loss: 0.002637 Acc: 13.4531\n",
      " |~~ train@49472  Loss: 0.002993 Acc: 13.2500\n",
      " |~~ train@49536  Loss: 0.003085 Acc: 13.2188\n",
      " |~~ train@49600  Loss: 0.003360 Acc: 13.1406\n",
      " |~~ train@49664  Loss: 0.002663 Acc: 13.3906\n",
      " |~~ train@49728  Loss: 0.003259 Acc: 13.2031\n",
      " |~~ train@49792  Loss: 0.003576 Acc: 13.0781\n",
      " |~~ train@49856  Loss: 0.003108 Acc: 13.2344\n",
      " |~~ train@49920  Loss: 0.003157 Acc: 13.2344\n",
      " |~~ train@49984  Loss: 0.002800 Acc: 13.3906\n",
      " |~~ train@50048  Loss: 0.003132 Acc: 13.2500\n",
      " |~~ train@50112  Loss: 0.002981 Acc: 13.2969\n",
      " |~~ train@50176  Loss: 0.003947 Acc: 12.9219\n",
      " |~~ train@50240  Loss: 0.002819 Acc: 13.3750\n",
      " |~~ train@50304  Loss: 0.003470 Acc: 13.0469\n",
      " |~~ train@50368  Loss: 0.003178 Acc: 13.2188\n",
      " |~~ train@50432  Loss: 0.003364 Acc: 13.1094\n",
      " |~~ train@50496  Loss: 0.002613 Acc: 13.3906\n",
      " |~~ train@50560  Loss: 0.003314 Acc: 13.1719\n",
      " |~~ train@50624  Loss: 0.002903 Acc: 13.3125\n",
      " |~~ train@50688  Loss: 0.002509 Acc: 13.4531\n",
      " |~~ train@50752  Loss: 0.003510 Acc: 13.1094\n",
      " |~~ train@50816  Loss: 0.002625 Acc: 13.4219\n",
      " |~~ train@50880  Loss: 0.002963 Acc: 13.2500\n",
      " |~~ train@50944  Loss: 0.003538 Acc: 13.0625\n",
      " |~~ train@51008  Loss: 0.002868 Acc: 13.3750\n",
      " |~~ train@51072  Loss: 0.003241 Acc: 13.1875\n",
      " |~~ train@51136  Loss: 0.003488 Acc: 13.0938\n",
      " |~~ train@51200  Loss: 0.002752 Acc: 13.3594\n",
      " |~~ train@51264  Loss: 0.002659 Acc: 13.4219\n",
      " |~~ train@51328  Loss: 0.003534 Acc: 13.0312\n",
      " |~~ train@51392  Loss: 0.002958 Acc: 13.3281\n",
      " |~~ train@51456  Loss: 0.002949 Acc: 13.3125\n",
      " |~~ train@51520  Loss: 0.002798 Acc: 13.3594\n",
      " |~~ train@51584  Loss: 0.003340 Acc: 13.1562\n",
      " |~~ train@51648  Loss: 0.002985 Acc: 13.3281\n",
      " |~~ train@51712  Loss: 0.002935 Acc: 13.3281\n",
      " |~~ train@51776  Loss: 0.003260 Acc: 13.1562\n",
      " |~~ train@51840  Loss: 0.002906 Acc: 13.3281\n",
      " |~~ train@51904  Loss: 0.002832 Acc: 13.3906\n",
      " |~~ train@51968  Loss: 0.002532 Acc: 13.4531\n",
      " |~~ train@52032  Loss: 0.003328 Acc: 13.1719\n",
      " |~~ train@52096  Loss: 0.003011 Acc: 13.3125\n",
      " |~~ train@52160  Loss: 0.002941 Acc: 13.2969\n",
      " |~~ train@52224  Loss: 0.002997 Acc: 13.2500\n",
      " |~~ train@52288  Loss: 0.003119 Acc: 13.2656\n",
      " |~~ train@52352  Loss: 0.003521 Acc: 13.0625\n",
      " |~~ train@52416  Loss: 0.003038 Acc: 13.2500\n",
      " |~~ train@52480  Loss: 0.002988 Acc: 13.3125\n",
      " |~~ train@52544  Loss: 0.002991 Acc: 13.2812\n",
      " |~~ train@52608  Loss: 0.002876 Acc: 13.3438\n",
      " |~~ train@52672  Loss: 0.003072 Acc: 13.2500\n",
      " |~~ train@52736  Loss: 0.004093 Acc: 12.8438\n",
      " |~~ train@52800  Loss: 0.003030 Acc: 13.2500\n",
      " |~~ train@52864  Loss: 0.002685 Acc: 13.3594\n",
      " |~~ train@52928  Loss: 0.003259 Acc: 13.1562\n",
      " |~~ train@52992  Loss: 0.003135 Acc: 13.2188\n",
      " |~~ train@53056  Loss: 0.003243 Acc: 13.2031\n",
      " |~~ train@53120  Loss: 0.002988 Acc: 13.2969\n",
      " |~~ train@53184  Loss: 0.003835 Acc: 12.9688\n",
      " |~~ train@53248  Loss: 0.002778 Acc: 13.3594\n",
      " |~~ train@53312  Loss: 0.002787 Acc: 13.3906\n",
      " |~~ train@53376  Loss: 0.003010 Acc: 13.3125\n",
      " |~~ train@53440  Loss: 0.002952 Acc: 13.2812\n",
      " |~~ train@53504  Loss: 0.002966 Acc: 13.2656\n",
      " |~~ train@53568  Loss: 0.002963 Acc: 13.2969\n",
      " |~~ train@53632  Loss: 0.003062 Acc: 13.3125\n",
      " |~~ train@53696  Loss: 0.003189 Acc: 13.2188\n",
      " |~~ train@53760  Loss: 0.002800 Acc: 13.3594\n",
      " |~~ train@53824  Loss: 0.002668 Acc: 13.4062\n",
      " |~~ train@53888  Loss: 0.002684 Acc: 13.4219\n",
      " |~~ train@53952  Loss: 0.002916 Acc: 13.3438\n",
      " |~~ train@54016  Loss: 0.003362 Acc: 13.1094\n",
      " |~~ train@54080  Loss: 0.002630 Acc: 13.3906\n",
      " |~~ train@54144  Loss: 0.003198 Acc: 13.2031\n",
      " |~~ train@54208  Loss: 0.002637 Acc: 13.4062\n",
      " |~~ train@54272  Loss: 0.003798 Acc: 12.9531\n",
      " |~~ train@54336  Loss: 0.002865 Acc: 13.3125\n",
      " |~~ train@54400  Loss: 0.003010 Acc: 13.2344\n",
      " |~~ train@54464  Loss: 0.003051 Acc: 13.2500\n",
      " |~~ train@54528  Loss: 0.003047 Acc: 13.2656\n",
      " |~~ train@54592  Loss: 0.003080 Acc: 13.2344\n",
      " |~~ train@54656  Loss: 0.002905 Acc: 13.3281\n",
      " |~~ train@54720  Loss: 0.002475 Acc: 13.4531\n",
      " |~~ train@54784  Loss: 0.003631 Acc: 12.9844\n",
      " |~~ train@54848  Loss: 0.002527 Acc: 13.5000\n",
      " |~~ train@54912  Loss: 0.003332 Acc: 13.0781\n",
      " |~~ train@54976  Loss: 0.002998 Acc: 13.2656\n",
      " |~~ train@55040  Loss: 0.003301 Acc: 13.1719\n",
      " |~~ train@55104  Loss: 0.002851 Acc: 13.3125\n",
      " |~~ train@55168  Loss: 0.003042 Acc: 13.2344\n",
      " |~~ train@55232  Loss: 0.002743 Acc: 13.3750\n",
      " |~~ train@55296  Loss: 0.003051 Acc: 13.2344\n",
      " |~~ train@55360  Loss: 0.002549 Acc: 13.4375\n",
      " |~~ train@55424  Loss: 0.002846 Acc: 13.3125\n",
      " |~~ train@55488  Loss: 0.002811 Acc: 13.3750\n",
      " |~~ train@55552  Loss: 0.003284 Acc: 13.1719\n",
      " |~~ train@55616  Loss: 0.003284 Acc: 13.2031\n",
      " |~~ train@55680  Loss: 0.002644 Acc: 13.4062\n",
      " |~~ train@55744  Loss: 0.003036 Acc: 13.2188\n",
      " |~~ train@55808  Loss: 0.002926 Acc: 13.3125\n",
      " |~~ train@55872  Loss: 0.002502 Acc: 13.4688\n",
      " |~~ train@55936  Loss: 0.003163 Acc: 13.2188\n",
      " |~~ train@56000  Loss: 0.002616 Acc: 13.4219\n",
      " |~~ train@56064  Loss: 0.004048 Acc: 12.8906\n",
      " |~~ train@56128  Loss: 0.002837 Acc: 13.3594\n",
      " |~~ train@56192  Loss: 0.002492 Acc: 13.5000\n",
      " |~~ train@56256  Loss: 0.002814 Acc: 13.3438\n",
      " |~~ train@56320  Loss: 0.002446 Acc: 13.5000\n",
      " |~~ train@56384  Loss: 0.003277 Acc: 13.1562\n",
      " |~~ train@56448  Loss: 0.003367 Acc: 13.1094\n",
      " |~~ train@56512  Loss: 0.002819 Acc: 13.3281\n",
      " |~~ train@56576  Loss: 0.003290 Acc: 13.1406\n",
      " |~~ train@56640  Loss: 0.003320 Acc: 13.1719\n",
      " |~~ train@56704  Loss: 0.003120 Acc: 13.2656\n",
      " |~~ train@56768  Loss: 0.003343 Acc: 13.1875\n",
      " |~~ train@56832  Loss: 0.003278 Acc: 13.1406\n",
      " |~~ train@56896  Loss: 0.003216 Acc: 13.1875\n",
      " |~~ train@56960  Loss: 0.003115 Acc: 13.2188\n",
      " |~~ train@57024  Loss: 0.002964 Acc: 13.3125\n",
      " |~~ train@57088  Loss: 0.003157 Acc: 13.2344\n",
      " |~~ train@57152  Loss: 0.003087 Acc: 13.2188\n",
      " |~~ train@57216  Loss: 0.002561 Acc: 13.4375\n",
      " |~~ train@57280  Loss: 0.003109 Acc: 13.2344\n",
      " |~~ train@57344  Loss: 0.002850 Acc: 13.2812\n",
      " |~~ train@57408  Loss: 0.003183 Acc: 13.2344\n",
      " |~~ train@57472  Loss: 0.003694 Acc: 12.9688\n",
      " |~~ train@57536  Loss: 0.002908 Acc: 13.3125\n",
      " |~~ train@57600  Loss: 0.003191 Acc: 13.1875\n",
      " |~~ train@57664  Loss: 0.002716 Acc: 13.3750\n",
      " |~~ train@57728  Loss: 0.003147 Acc: 13.1406\n",
      " |~~ train@57792  Loss: 0.003033 Acc: 13.2656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@57856  Loss: 0.002822 Acc: 13.3125\n",
      " |~~ train@57920  Loss: 0.003229 Acc: 13.1719\n",
      " |~~ train@57984  Loss: 0.003024 Acc: 13.2812\n",
      " |~~ train@58048  Loss: 0.002842 Acc: 13.3438\n",
      " |~~ train@58112  Loss: 0.002679 Acc: 13.3750\n",
      " |~~ train@58176  Loss: 0.003626 Acc: 13.0469\n",
      " |~~ train@58240  Loss: 0.003292 Acc: 13.1719\n",
      " |~~ train@58304  Loss: 0.002568 Acc: 13.4062\n",
      " |~~ train@58368  Loss: 0.002966 Acc: 13.3125\n",
      " |~~ train@58432  Loss: 0.002741 Acc: 13.3594\n",
      " |~~ train@58496  Loss: 0.002553 Acc: 13.4688\n",
      " |~~ train@58560  Loss: 0.002773 Acc: 13.3750\n",
      " |~~ train@58624  Loss: 0.002846 Acc: 13.3438\n",
      " |~~ train@58688  Loss: 0.002332 Acc: 13.5156\n",
      " |~~ train@58752  Loss: 0.002604 Acc: 13.3906\n",
      " |~~ train@58816  Loss: 0.002806 Acc: 13.3594\n",
      " |~~ train@58880  Loss: 0.003055 Acc: 13.2031\n",
      " |~~ train@58944  Loss: 0.002711 Acc: 13.4062\n",
      " |~~ train@59008  Loss: 0.002532 Acc: 13.4688\n",
      " |~~ train@59072  Loss: 0.002835 Acc: 13.3594\n",
      " |~~ train@59136  Loss: 0.003600 Acc: 13.0000\n",
      " |~~ train@59200  Loss: 0.003511 Acc: 13.0312\n",
      " |~~ train@59264  Loss: 0.002972 Acc: 13.2812\n",
      " |~~ train@59328  Loss: 0.002904 Acc: 13.3281\n",
      " |~~ train@59392  Loss: 0.002921 Acc: 13.2812\n",
      " |~~ train@59456  Loss: 0.002994 Acc: 13.2344\n",
      " |~~ train@59520  Loss: 0.002967 Acc: 13.2344\n",
      " |~~ train@59584  Loss: 0.002968 Acc: 13.2969\n",
      " |~~ train@59648  Loss: 0.003147 Acc: 13.2031\n",
      " |~~ train@59712  Loss: 0.003194 Acc: 13.1250\n",
      " |~~ train@59776  Loss: 0.003411 Acc: 13.0781\n",
      " |~~ train@59840  Loss: 0.002833 Acc: 13.3281\n",
      " |~~ train@59904  Loss: 0.003416 Acc: 13.1094\n",
      " |~~ train@59968  Loss: 0.003045 Acc: 13.2344\n",
      " |~~ train@60032  Loss: 0.002565 Acc: 13.4688\n",
      " |~~ train@60096  Loss: 0.002892 Acc: 13.2500\n",
      " |~~ train@60160  Loss: 0.003171 Acc: 13.1250\n",
      " |~~ train@60224  Loss: 0.002433 Acc: 13.4844\n",
      " |~~ train@60288  Loss: 0.002878 Acc: 13.3125\n",
      " |~~ train@60352  Loss: 0.002924 Acc: 13.2812\n",
      " |~~ train@60416  Loss: 0.002362 Acc: 13.4844\n",
      " |~~ train@60480  Loss: 0.003144 Acc: 13.2344\n",
      " |~~ train@60544  Loss: 0.003240 Acc: 13.1875\n",
      " |~~ train@60608  Loss: 0.002748 Acc: 13.3438\n",
      " |~~ train@60672  Loss: 0.003387 Acc: 13.1094\n",
      " |~~ train@60736  Loss: 0.002793 Acc: 13.3438\n",
      " |~~ train@60800  Loss: 0.002643 Acc: 13.3750\n",
      " |~~ train@60864  Loss: 0.002745 Acc: 13.3438\n",
      " |~~ train@60928  Loss: 0.002556 Acc: 13.4219\n",
      " |~~ train@60992  Loss: 0.003135 Acc: 13.2344\n",
      " |~~ train@61056  Loss: 0.003052 Acc: 13.2188\n",
      " |~~ train@61120  Loss: 0.002898 Acc: 13.2969\n",
      " |~~ train@61184  Loss: 0.002564 Acc: 13.4375\n",
      " |~~ train@61248  Loss: 0.002639 Acc: 13.3906\n",
      " |~~ train@61312  Loss: 0.002832 Acc: 13.3438\n",
      " |~~ train@61376  Loss: 0.002811 Acc: 13.2812\n",
      " |~~ train@61440  Loss: 0.003204 Acc: 13.2188\n",
      " |~~ train@61504  Loss: 0.003200 Acc: 13.1562\n",
      " |~~ train@61568  Loss: 0.002745 Acc: 13.3594\n",
      " |~~ train@61632  Loss: 0.002797 Acc: 13.3125\n",
      " |~~ train@61696  Loss: 0.002765 Acc: 13.3594\n",
      " |~~ train@61760  Loss: 0.002723 Acc: 13.3594\n",
      " |~~ train@61824  Loss: 0.002813 Acc: 13.3438\n",
      " |~~ train@61888  Loss: 0.002697 Acc: 13.3750\n",
      " |~~ train@61952  Loss: 0.003179 Acc: 13.1562\n",
      " |~~ train@62016  Loss: 0.003515 Acc: 13.0781\n",
      " |~~ train@62080  Loss: 0.003091 Acc: 13.2031\n",
      " |~~ train@62144  Loss: 0.003443 Acc: 13.1875\n",
      " |~~ train@62208  Loss: 0.002512 Acc: 13.4219\n",
      " |~~ train@62272  Loss: 0.002998 Acc: 13.2969\n",
      " |~~ train@62336  Loss: 0.003163 Acc: 13.1875\n",
      " |~~ train@62400  Loss: 0.003083 Acc: 13.2188\n",
      " |~~ train@62464  Loss: 0.002900 Acc: 13.2969\n",
      " |~~ train@62528  Loss: 0.002896 Acc: 13.3281\n",
      " |~~ train@62592  Loss: 0.002813 Acc: 13.3281\n",
      " |~~ train@62656  Loss: 0.002616 Acc: 13.3750\n",
      " |~~ train@62720  Loss: 0.002656 Acc: 13.3750\n",
      " |~~ train@62784  Loss: 0.003399 Acc: 13.1250\n",
      " |~~ train@62848  Loss: 0.003447 Acc: 13.0781\n",
      " |~~ train@62912  Loss: 0.002636 Acc: 13.4219\n",
      " |~~ train@62976  Loss: 0.002694 Acc: 13.3438\n",
      " |~~ train@63040  Loss: 0.002734 Acc: 13.3438\n",
      " |~~ train@63104  Loss: 0.003037 Acc: 13.2344\n",
      " |~~ train@63168  Loss: 0.003341 Acc: 13.1406\n",
      " |~~ train@63232  Loss: 0.003279 Acc: 13.1719\n",
      " |~~ train@63296  Loss: 0.003006 Acc: 13.2656\n",
      " |~~ train@63360  Loss: 0.002706 Acc: 13.3438\n",
      " |~~ train@63424  Loss: 0.002976 Acc: 13.2656\n",
      " |~~ train@63488  Loss: 0.002901 Acc: 13.3125\n",
      " |~~ train@63552  Loss: 0.003108 Acc: 13.2344\n",
      " |~~ train@63616  Loss: 0.002187 Acc: 13.5781\n",
      " |~~ train@63680  Loss: 0.002952 Acc: 13.2656\n",
      " |~~ train@63744  Loss: 0.003116 Acc: 13.1562\n",
      " |~~ train@63808  Loss: 0.002620 Acc: 13.4531\n",
      " |~~ train@63872  Loss: 0.002618 Acc: 13.4062\n",
      " |~~ train@63936  Loss: 0.003335 Acc: 13.1094\n",
      " |~~ train@64000  Loss: 0.002652 Acc: 13.3594\n",
      " |~~ train@64064  Loss: 0.002756 Acc: 13.3125\n",
      " |~~ train@64128  Loss: 0.003109 Acc: 13.2031\n",
      " |~~ train@64192  Loss: 0.003270 Acc: 13.1406\n",
      " |~~ train@64256  Loss: 0.003111 Acc: 13.2188\n",
      " |~~ train@64320  Loss: 0.002586 Acc: 13.4688\n",
      " |~~ train@64384  Loss: 0.002905 Acc: 13.2656\n",
      " |~~ train@64448  Loss: 0.002878 Acc: 13.2969\n",
      " |~~ train@64512  Loss: 0.002818 Acc: 13.3281\n",
      " |~~ train@64576  Loss: 0.002812 Acc: 13.3125\n",
      " |~~ train@64640  Loss: 0.002716 Acc: 13.3438\n",
      " |~~ train@64704  Loss: 0.002785 Acc: 13.3906\n",
      " |~~ train@64768  Loss: 0.002887 Acc: 13.3281\n",
      " |~~ train@64832  Loss: 0.003226 Acc: 13.2031\n",
      " |~~ train@64896  Loss: 0.003662 Acc: 13.0469\n",
      " |~~ train@64960  Loss: 0.002771 Acc: 13.3750\n",
      " |~~ train@65024  Loss: 0.002791 Acc: 13.2969\n",
      " |~~ train@65088  Loss: 0.003742 Acc: 13.0000\n",
      " |~~ train@65152  Loss: 0.003260 Acc: 13.1875\n",
      " |~~ train@65216  Loss: 0.002752 Acc: 13.3281\n",
      " |~~ train@65280  Loss: 0.002333 Acc: 13.5156\n",
      " |~~ train@65344  Loss: 0.003068 Acc: 13.2188\n",
      " |~~ train@65408  Loss: 0.002790 Acc: 13.3594\n",
      " |~~ train@65472  Loss: 0.002949 Acc: 13.2812\n",
      " |~~ train@65536  Loss: 0.003106 Acc: 13.2344\n",
      " |~~ train@65600  Loss: 0.002930 Acc: 13.2812\n",
      " |~~ train@65664  Loss: 0.002987 Acc: 13.2344\n",
      " |~~ train@65728  Loss: 0.003145 Acc: 13.1719\n",
      " |~~ train@65792  Loss: 0.002996 Acc: 13.2500\n",
      " |~~ train@65856  Loss: 0.002791 Acc: 13.3750\n",
      " |~~ train@65920  Loss: 0.003516 Acc: 13.0312\n",
      " |~~ train@65984  Loss: 0.002814 Acc: 13.3594\n",
      " |~~ train@66048  Loss: 0.003494 Acc: 13.0781\n",
      " |~~ train@66112  Loss: 0.003912 Acc: 12.9375\n",
      " |~~ train@66176  Loss: 0.002509 Acc: 13.4219\n",
      " |~~ train@66240  Loss: 0.002740 Acc: 13.3281\n",
      " |~~ train@66304  Loss: 0.002767 Acc: 13.3125\n",
      " |~~ train@66368  Loss: 0.003531 Acc: 13.0781\n",
      " |~~ train@66432  Loss: 0.003102 Acc: 13.2031\n",
      " |~~ train@66496  Loss: 0.003366 Acc: 13.1719\n",
      " |~~ train@66560  Loss: 0.002338 Acc: 13.5000\n",
      " |~~ train@66624  Loss: 0.002731 Acc: 13.2969\n",
      " |~~ train@66688  Loss: 0.003459 Acc: 13.0938\n",
      " |~~ train@66752  Loss: 0.003003 Acc: 13.2031\n",
      " |~~ train@66816  Loss: 0.003894 Acc: 12.9375\n",
      " |~~ train@66880  Loss: 0.003113 Acc: 13.2031\n",
      " |~~ train@66944  Loss: 0.003061 Acc: 13.2344\n",
      " |~~ train@67008  Loss: 0.002790 Acc: 13.3125\n",
      " |~~ train@67072  Loss: 0.002867 Acc: 13.3281\n",
      " |~~ train@67136  Loss: 0.002824 Acc: 13.2812\n",
      " |~~ train@67200  Loss: 0.002598 Acc: 13.4219\n",
      " |~~ train@67264  Loss: 0.003169 Acc: 13.1562\n",
      " |~~ train@67328  Loss: 0.003826 Acc: 12.9531\n",
      " |~~ train@67392  Loss: 0.002912 Acc: 13.2969\n",
      " |~~ train@67456  Loss: 0.002663 Acc: 13.3906\n",
      " |~~ train@67520  Loss: 0.002537 Acc: 13.4219\n",
      " |~~ train@67584  Loss: 0.003228 Acc: 13.1406\n",
      " |~~ train@67648  Loss: 0.002947 Acc: 13.2656\n",
      " |~~ train@67712  Loss: 0.003130 Acc: 13.1875\n",
      " |~~ train@67776  Loss: 0.002546 Acc: 13.4375\n",
      " |~~ train@67840  Loss: 0.002401 Acc: 13.4688\n",
      " |~~ train@67904  Loss: 0.003019 Acc: 13.1719\n",
      " |~~ train@67968  Loss: 0.003251 Acc: 13.1094\n",
      " |~~ train@68032  Loss: 0.002562 Acc: 13.3906\n",
      " |~~ train@68096  Loss: 0.002424 Acc: 13.4844\n",
      " |~~ train@68160  Loss: 0.002693 Acc: 13.3906\n",
      " |~~ train@68224  Loss: 0.002928 Acc: 13.2812\n",
      " |~~ train@68288  Loss: 0.002363 Acc: 13.5312\n",
      " |~~ train@68352  Loss: 0.003019 Acc: 13.2656\n",
      " |~~ train@68416  Loss: 0.002817 Acc: 13.3438\n",
      " |~~ train@68480  Loss: 0.003156 Acc: 13.1719\n",
      " |~~ train@68544  Loss: 0.002882 Acc: 13.2812\n",
      " |~~ train@68608  Loss: 0.003151 Acc: 13.2031\n",
      " |~~ train@68672  Loss: 0.002744 Acc: 13.3594\n",
      " |~~ train@68736  Loss: 0.002874 Acc: 13.3438\n",
      " |~~ train@68800  Loss: 0.002548 Acc: 13.4688\n",
      " |~~ train@68864  Loss: 0.003175 Acc: 13.1719\n",
      " |~~ train@68928  Loss: 0.002924 Acc: 13.3125\n",
      " |~~ train@68992  Loss: 0.003128 Acc: 13.2344\n",
      " |~~ train@69056  Loss: 0.002681 Acc: 13.3594\n",
      " |~~ train@69120  Loss: 0.002555 Acc: 13.4375\n",
      " |~~ train@69184  Loss: 0.003060 Acc: 13.2344\n",
      " |~~ train@69248  Loss: 0.002866 Acc: 13.2969\n",
      " |~~ train@69312  Loss: 0.003087 Acc: 13.2344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@69376  Loss: 0.002749 Acc: 13.3125\n",
      " |~~ train@69440  Loss: 0.003235 Acc: 13.1719\n",
      " |~~ train@69504  Loss: 0.003091 Acc: 13.2188\n",
      " |~~ train@69568  Loss: 0.003170 Acc: 13.2031\n",
      " |~~ train@69632  Loss: 0.003762 Acc: 12.9688\n",
      " |~~ train@69696  Loss: 0.003394 Acc: 13.0625\n",
      " |~~ train@69760  Loss: 0.002674 Acc: 13.3594\n",
      " |~~ train@69824  Loss: 0.002755 Acc: 13.3594\n",
      " |~~ train@69888  Loss: 0.002828 Acc: 13.3594\n",
      " |~~ train@69952  Loss: 0.002872 Acc: 13.3438\n",
      " |~~ train@70016  Loss: 0.003557 Acc: 13.0625\n",
      " |~~ train@70080  Loss: 0.002375 Acc: 13.4688\n",
      " |~~ train@70144  Loss: 0.002984 Acc: 13.2500\n",
      " |~~ train@70208  Loss: 0.002893 Acc: 13.2812\n",
      " |~~ train@70272  Loss: 0.002602 Acc: 13.4062\n",
      " |~~ train@70336  Loss: 0.002375 Acc: 13.4844\n",
      " |~~ train@70400  Loss: 0.003375 Acc: 13.1250\n",
      " |~~ train@70464  Loss: 0.003323 Acc: 13.1562\n",
      " |~~ train@70528  Loss: 0.003370 Acc: 13.0781\n",
      " |~~ train@70592  Loss: 0.002608 Acc: 13.4062\n",
      " |~~ train@70656  Loss: 0.002736 Acc: 13.3281\n",
      " |~~ train@70720  Loss: 0.002903 Acc: 13.2969\n",
      " |~~ train@70784  Loss: 0.003005 Acc: 13.2344\n",
      " |~~ train@70848  Loss: 0.002988 Acc: 13.2656\n",
      " |~~ train@70912  Loss: 0.002900 Acc: 13.2031\n",
      " |~~ train@70976  Loss: 0.003326 Acc: 13.1406\n",
      " |~~ train@71040  Loss: 0.003264 Acc: 13.1406\n",
      " |~~ train@71104  Loss: 0.003144 Acc: 13.1719\n",
      " |~~ train@71168  Loss: 0.002610 Acc: 13.3594\n",
      " |~~ train@71232  Loss: 0.002724 Acc: 13.3906\n",
      " |~~ train@71296  Loss: 0.002651 Acc: 13.4219\n",
      " |~~ train@71360  Loss: 0.002872 Acc: 13.2812\n",
      " |~~ train@71424  Loss: 0.002675 Acc: 13.3594\n",
      " |~~ train@71488  Loss: 0.003164 Acc: 13.1562\n",
      " |~~ train@71552  Loss: 0.002512 Acc: 13.4062\n",
      " |~~ train@71616  Loss: 0.002608 Acc: 13.3906\n",
      " |~~ train@71680  Loss: 0.002682 Acc: 13.3906\n",
      " |~~ train@71744  Loss: 0.002957 Acc: 13.2656\n",
      " |~~ train@71808  Loss: 0.002925 Acc: 13.2969\n",
      " |~~ train@71872  Loss: 0.002362 Acc: 13.5312\n",
      " |~~ train@71936  Loss: 0.002841 Acc: 13.3125\n",
      " |~~ train@72000  Loss: 0.003060 Acc: 13.2500\n",
      " |~~ train@72064  Loss: 0.002810 Acc: 13.3906\n",
      " |~~ train@72128  Loss: 0.002886 Acc: 13.3125\n",
      " |~~ train@72192  Loss: 0.002576 Acc: 13.3750\n",
      " |~~ train@72256  Loss: 0.003192 Acc: 13.2031\n",
      " |~~ train@72320  Loss: 0.002638 Acc: 13.3750\n",
      " |~~ train@72384  Loss: 0.003294 Acc: 13.1250\n",
      " |~~ train@72448  Loss: 0.002568 Acc: 13.4219\n",
      " |~~ train@72512  Loss: 0.003115 Acc: 13.2344\n",
      " |~~ train@72576  Loss: 0.002869 Acc: 13.2812\n",
      " |~~ train@72640  Loss: 0.002974 Acc: 13.2031\n",
      " |~~ train@72704  Loss: 0.003303 Acc: 13.1250\n",
      " |~~ train@72768  Loss: 0.002824 Acc: 13.3125\n",
      " |~~ train@72832  Loss: 0.002430 Acc: 13.4844\n",
      " |~~ train@72896  Loss: 0.002478 Acc: 13.4688\n",
      " |~~ train@72960  Loss: 0.003113 Acc: 13.2031\n",
      " |~~ train@73024  Loss: 0.002597 Acc: 13.4375\n",
      " |~~ train@73088  Loss: 0.002869 Acc: 13.3125\n",
      " |~~ train@73152  Loss: 0.002475 Acc: 13.4219\n",
      " |~~ train@73216  Loss: 0.003044 Acc: 13.2344\n",
      " |~~ train@73280  Loss: 0.002895 Acc: 13.3281\n",
      " |~~ train@73344  Loss: 0.002942 Acc: 13.2344\n",
      " |~~ train@73408  Loss: 0.002686 Acc: 13.3438\n",
      " |~~ train@73472  Loss: 0.002657 Acc: 13.3906\n",
      " |~~ train@73536  Loss: 0.002789 Acc: 13.3125\n",
      " |~~ train@73600  Loss: 0.002523 Acc: 13.4219\n",
      " |~~ train@73664  Loss: 0.003369 Acc: 13.1250\n",
      " |~~ train@73728  Loss: 0.002817 Acc: 13.2969\n",
      " |~~ train@73792  Loss: 0.002189 Acc: 13.5156\n",
      " |~~ train@73856  Loss: 0.002619 Acc: 13.3750\n",
      " |~~ train@73920  Loss: 0.002826 Acc: 13.3125\n",
      " |~~ train@73984  Loss: 0.003039 Acc: 13.2188\n",
      " |~~ train@74048  Loss: 0.002637 Acc: 13.4062\n",
      " |~~ train@74112  Loss: 0.002814 Acc: 13.3438\n",
      " |~~ train@74176  Loss: 0.003156 Acc: 13.2344\n",
      " |~~ train@74240  Loss: 0.002816 Acc: 13.3125\n",
      " |~~ train@74304  Loss: 0.003212 Acc: 13.1250\n",
      " |~~ train@74368  Loss: 0.002668 Acc: 13.3906\n",
      " |~~ train@74432  Loss: 0.002800 Acc: 13.2969\n",
      " |~~ train@74496  Loss: 0.003051 Acc: 13.1875\n",
      " |~~ train@74560  Loss: 0.002849 Acc: 13.2500\n",
      " |~~ train@74624  Loss: 0.003251 Acc: 13.1719\n",
      " |~~ train@74688  Loss: 0.003383 Acc: 13.1719\n",
      " |~~ train@74752  Loss: 0.002968 Acc: 13.3125\n",
      " |~~ train@74816  Loss: 0.002830 Acc: 13.3281\n",
      " |~~ train@74880  Loss: 0.003212 Acc: 13.2031\n",
      " |~~ train@74944  Loss: 0.003104 Acc: 13.2500\n",
      " |~~ train@75008  Loss: 0.002482 Acc: 13.4219\n",
      " |~~ train@75072  Loss: 0.003042 Acc: 13.2344\n",
      " |~~ train@75136  Loss: 0.003289 Acc: 13.1875\n",
      " |~~ train@75200  Loss: 0.002811 Acc: 13.3438\n",
      " |~~ train@75264  Loss: 0.003060 Acc: 13.2656\n",
      " |~~ train@75328  Loss: 0.002920 Acc: 13.2656\n",
      " |~~ train@75392  Loss: 0.002372 Acc: 13.4219\n",
      " |~~ train@75456  Loss: 0.002752 Acc: 13.3281\n",
      " |~~ train@75520  Loss: 0.003055 Acc: 13.2188\n",
      " |~~ train@75584  Loss: 0.002722 Acc: 13.3906\n",
      " |~~ train@75648  Loss: 0.002876 Acc: 13.2812\n",
      " |~~ train@75712  Loss: 0.003137 Acc: 13.2031\n",
      " |~~ train@75776  Loss: 0.003027 Acc: 13.2344\n",
      " |~~ train@75840  Loss: 0.002803 Acc: 13.3438\n",
      " |~~ train@75904  Loss: 0.002728 Acc: 13.3594\n",
      " |~~ train@75968  Loss: 0.002971 Acc: 13.2031\n",
      " |~~ train@76032  Loss: 0.003486 Acc: 13.0938\n",
      " |~~ train@76096  Loss: 0.003424 Acc: 13.1250\n",
      " |~~ train@76160  Loss: 0.002726 Acc: 13.3750\n",
      " |~~ train@76224  Loss: 0.003066 Acc: 13.2188\n",
      " |~~ train@76288  Loss: 0.002914 Acc: 13.3125\n",
      " |~~ train@76352  Loss: 0.002762 Acc: 13.3125\n",
      " |~~ train@76416  Loss: 0.002467 Acc: 13.3906\n",
      " |~~ train@76480  Loss: 0.002159 Acc: 13.5938\n",
      " |~~ train@76544  Loss: 0.003047 Acc: 13.2500\n",
      " |~~ train@76608  Loss: 0.002204 Acc: 13.5156\n",
      " |~~ train@76672  Loss: 0.002428 Acc: 13.4531\n",
      " |~~ train@76736  Loss: 0.002294 Acc: 13.4844\n",
      " |~~ train@76800  Loss: 0.002543 Acc: 13.4062\n",
      " |~~ train@76864  Loss: 0.003114 Acc: 13.2344\n",
      " |~~ train@76928  Loss: 0.002938 Acc: 13.2500\n",
      " |~~ train@76992  Loss: 0.003180 Acc: 13.1875\n",
      " |~~ train@77056  Loss: 0.002617 Acc: 13.4062\n",
      " |~~ train@77120  Loss: 0.003114 Acc: 13.1875\n",
      " |~~ train@77184  Loss: 0.003181 Acc: 13.2031\n",
      " |~~ train@77248  Loss: 0.002940 Acc: 13.3125\n",
      " |~~ train@77312  Loss: 0.003046 Acc: 13.2500\n",
      " |~~ train@77376  Loss: 0.002980 Acc: 13.2656\n",
      " |~~ train@77440  Loss: 0.003323 Acc: 13.1406\n",
      " |~~ train@77504  Loss: 0.003091 Acc: 13.2188\n",
      " |~~ train@77568  Loss: 0.002277 Acc: 13.5312\n",
      " |~~ train@77632  Loss: 0.003108 Acc: 13.1562\n",
      " |~~ train@77696  Loss: 0.002786 Acc: 13.3125\n",
      " |~~ train@77760  Loss: 0.003295 Acc: 13.1562\n",
      " |~~ train@77824  Loss: 0.002961 Acc: 13.2031\n",
      " |~~ train@77888  Loss: 0.002596 Acc: 13.3750\n",
      " |~~ train@77952  Loss: 0.002546 Acc: 13.4062\n",
      " |~~ train@78016  Loss: 0.002881 Acc: 13.2656\n",
      " |~~ train@78080  Loss: 0.002725 Acc: 13.2969\n",
      " |~~ train@78144  Loss: 0.002669 Acc: 13.3906\n",
      " |~~ train@78208  Loss: 0.002473 Acc: 13.4219\n",
      " |~~ train@78272  Loss: 0.002819 Acc: 13.3125\n",
      " |~~ train@78336  Loss: 0.003018 Acc: 13.2188\n",
      " |~~ train@78400  Loss: 0.002932 Acc: 13.2656\n",
      " |~~ train@78464  Loss: 0.003424 Acc: 13.0781\n",
      " |~~ train@78528  Loss: 0.003307 Acc: 13.1094\n",
      " |~~ train@78592  Loss: 0.002209 Acc: 13.5625\n",
      " |~~ train@78656  Loss: 0.003514 Acc: 13.0938\n",
      " |~~ train@78720  Loss: 0.003091 Acc: 13.2031\n",
      " |~~ train@78784  Loss: 0.002391 Acc: 13.4688\n",
      " |~~ train@78848  Loss: 0.002915 Acc: 13.2656\n",
      " |~~ train@78912  Loss: 0.002967 Acc: 13.2969\n",
      " |~~ train@78976  Loss: 0.002570 Acc: 13.4062\n",
      " |~~ train@79040  Loss: 0.002705 Acc: 13.3906\n",
      " |~~ train@79104  Loss: 0.002970 Acc: 13.2188\n",
      " |~~ train@79168  Loss: 0.002931 Acc: 13.2812\n",
      " |~~ train@79232  Loss: 0.003146 Acc: 13.2031\n",
      " |~~ train@79296  Loss: 0.003393 Acc: 13.1094\n",
      " |~~ train@79360  Loss: 0.003040 Acc: 13.2188\n",
      " |~~ train@79424  Loss: 0.002825 Acc: 13.2969\n",
      " |~~ train@79488  Loss: 0.002813 Acc: 13.3438\n",
      " |~~ train@79552  Loss: 0.003468 Acc: 13.0469\n",
      " |~~ train@79616  Loss: 0.002904 Acc: 13.2812\n",
      " |~~ train@79680  Loss: 0.003413 Acc: 13.1250\n",
      " |~~ train@79744  Loss: 0.003167 Acc: 13.2188\n",
      " |~~ train@79808  Loss: 0.003017 Acc: 13.2656\n",
      " |~~ train@79872  Loss: 0.003003 Acc: 13.2500\n",
      " |~~ train@79936  Loss: 0.002404 Acc: 13.4844\n",
      " |~~ train@80000  Loss: 0.002652 Acc: 13.3906\n",
      " |~~ train@80064  Loss: 0.002767 Acc: 13.3438\n",
      " |~~ train@80128  Loss: 0.002992 Acc: 13.2500\n",
      " |~~ train@80192  Loss: 0.002852 Acc: 13.2812\n",
      " |~~ train@80256  Loss: 0.003306 Acc: 13.1094\n",
      " |~~ train@80320  Loss: 0.002749 Acc: 13.3594\n",
      " |~~ train@80384  Loss: 0.002971 Acc: 13.3125\n",
      " |~~ train@80448  Loss: 0.003016 Acc: 13.1875\n",
      " |~~ train@80512  Loss: 0.003825 Acc: 12.9531\n",
      " |~~ train@80576  Loss: 0.002762 Acc: 13.3125\n",
      " |~~ train@80640  Loss: 0.002360 Acc: 13.4375\n",
      " |~~ train@80704  Loss: 0.002628 Acc: 13.3750\n",
      " |~~ train@80768  Loss: 0.003101 Acc: 13.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@80832  Loss: 0.002668 Acc: 13.3750\n",
      " |~~ train@80896  Loss: 0.002742 Acc: 13.2969\n",
      " |~~ train@80960  Loss: 0.002658 Acc: 13.3438\n",
      " |~~ train@81024  Loss: 0.002748 Acc: 13.3906\n",
      " |~~ train@81088  Loss: 0.002590 Acc: 13.4375\n",
      " |~~ train@81152  Loss: 0.002855 Acc: 13.3281\n",
      " |~~ train@81216  Loss: 0.002936 Acc: 13.2344\n",
      " |~~ train@81280  Loss: 0.003042 Acc: 13.2656\n",
      " |~~ train@81344  Loss: 0.002615 Acc: 13.3750\n",
      " |~~ train@81408  Loss: 0.003126 Acc: 13.1562\n",
      " |~~ train@81472  Loss: 0.002641 Acc: 13.3594\n",
      " |~~ train@81536  Loss: 0.003505 Acc: 12.9844\n",
      " |~~ train@81600  Loss: 0.002687 Acc: 13.3438\n",
      " |~~ train@81664  Loss: 0.002804 Acc: 13.3281\n",
      " |~~ train@81728  Loss: 0.002558 Acc: 13.3750\n",
      " |~~ train@81792  Loss: 0.002632 Acc: 13.3906\n",
      " |~~ train@81856  Loss: 0.002586 Acc: 13.3750\n",
      " |~~ train@81920  Loss: 0.003406 Acc: 13.1250\n",
      " |~~ train@81984  Loss: 0.002570 Acc: 13.3750\n",
      " |~~ train@82048  Loss: 0.002988 Acc: 13.2656\n",
      " |~~ train@82112  Loss: 0.002706 Acc: 13.3906\n",
      " |~~ train@82176  Loss: 0.003005 Acc: 13.2500\n",
      " |~~ train@82240  Loss: 0.002675 Acc: 13.3750\n",
      " |~~ train@82304  Loss: 0.003115 Acc: 13.2344\n",
      " |~~ train@82368  Loss: 0.002763 Acc: 13.3125\n",
      " |~~ train@82432  Loss: 0.002281 Acc: 13.4844\n",
      " |~~ train@82496  Loss: 0.002830 Acc: 13.3281\n",
      " |~~ train@82560  Loss: 0.002922 Acc: 13.2656\n",
      " |~~ train@82624  Loss: 0.003323 Acc: 13.1406\n",
      " |~~ train@82688  Loss: 0.002608 Acc: 13.3750\n",
      " |~~ train@82752  Loss: 0.002458 Acc: 13.5000\n",
      " |~~ train@82816  Loss: 0.003041 Acc: 13.2344\n",
      " |~~ train@82880  Loss: 0.003364 Acc: 13.1406\n",
      " |~~ train@82944  Loss: 0.003260 Acc: 13.1406\n",
      " |~~ train@83008  Loss: 0.003049 Acc: 13.2188\n",
      " |~~ train@83072  Loss: 0.003213 Acc: 13.1406\n",
      " |~~ train@83136  Loss: 0.003228 Acc: 13.1250\n",
      " |~~ train@83200  Loss: 0.002194 Acc: 13.5781\n",
      " |~~ train@83264  Loss: 0.002900 Acc: 13.2969\n",
      " |~~ train@83328  Loss: 0.003301 Acc: 13.1250\n",
      " |~~ train@83392  Loss: 0.003307 Acc: 13.1719\n",
      " |~~ train@83456  Loss: 0.003560 Acc: 13.0781\n",
      " |~~ train@83520  Loss: 0.002648 Acc: 13.3438\n",
      " |~~ train@83584  Loss: 0.002664 Acc: 13.3750\n",
      " |~~ train@83648  Loss: 0.002521 Acc: 13.4062\n",
      " |~~ train@83712  Loss: 0.002995 Acc: 13.2031\n",
      " |~~ train@83776  Loss: 0.002653 Acc: 13.3281\n",
      " |~~ train@83840  Loss: 0.002909 Acc: 13.2656\n",
      " |~~ train@83904  Loss: 0.003165 Acc: 13.1719\n",
      " |~~ train@83968  Loss: 0.003133 Acc: 13.1719\n",
      " |~~ train@84032  Loss: 0.002848 Acc: 13.3125\n",
      " |~~ train@84096  Loss: 0.002450 Acc: 13.4219\n",
      " |~~ train@84160  Loss: 0.002579 Acc: 13.3906\n",
      " |~~ train@84224  Loss: 0.002927 Acc: 13.3125\n",
      " |~~ train@84288  Loss: 0.003185 Acc: 13.1875\n",
      " |~~ train@84352  Loss: 0.003015 Acc: 13.2656\n",
      " |~~ train@84416  Loss: 0.002967 Acc: 13.2500\n",
      " |~~ train@84480  Loss: 0.003016 Acc: 13.2656\n",
      " |~~ train@84544  Loss: 0.003069 Acc: 13.2188\n",
      " |~~ train@84608  Loss: 0.003385 Acc: 13.1250\n",
      " |~~ train@84672  Loss: 0.002746 Acc: 13.2969\n",
      " |~~ train@84736  Loss: 0.002991 Acc: 13.2656\n",
      " |~~ train@84800  Loss: 0.003177 Acc: 13.1250\n",
      " |~~ train@84864  Loss: 0.002765 Acc: 13.3281\n",
      " |~~ train@84928  Loss: 0.002417 Acc: 13.5000\n",
      " |~~ train@84992  Loss: 0.002767 Acc: 13.2969\n",
      " |~~ train@85056  Loss: 0.002857 Acc: 13.2969\n",
      " |~~ train@85120  Loss: 0.003687 Acc: 12.9688\n",
      " |~~ train@85184  Loss: 0.002698 Acc: 13.3438\n",
      " |~~ train@85248  Loss: 0.002974 Acc: 13.2344\n",
      " |~~ train@85312  Loss: 0.003191 Acc: 13.1406\n",
      " |~~ train@85376  Loss: 0.003197 Acc: 13.1406\n",
      " |~~ train@85440  Loss: 0.002835 Acc: 13.2656\n",
      " |~~ train@85504  Loss: 0.002860 Acc: 13.2969\n",
      " |~~ train@85568  Loss: 0.002403 Acc: 13.4375\n",
      " |~~ train@85632  Loss: 0.002840 Acc: 13.2812\n",
      " |~~ train@85696  Loss: 0.002750 Acc: 13.3125\n",
      " |~~ train@85760  Loss: 0.002840 Acc: 13.2969\n",
      " |~~ train@85824  Loss: 0.002526 Acc: 13.4375\n",
      " |~~ train@85888  Loss: 0.003073 Acc: 13.2812\n",
      " |~~ train@85952  Loss: 0.003057 Acc: 13.2031\n",
      " |~~ train@86016  Loss: 0.002907 Acc: 13.2500\n",
      " |~~ train@86080  Loss: 0.002582 Acc: 13.4062\n",
      " |~~ train@86144  Loss: 0.003029 Acc: 13.2031\n",
      " |~~ train@86208  Loss: 0.002712 Acc: 13.3281\n",
      " |~~ train@86272  Loss: 0.002697 Acc: 13.3438\n",
      " |~~ train@86336  Loss: 0.003702 Acc: 12.9375\n",
      " |~~ train@86400  Loss: 0.003385 Acc: 13.0938\n",
      " |~~ train@86464  Loss: 0.003444 Acc: 13.0625\n",
      " |~~ train@86528  Loss: 0.003102 Acc: 13.2500\n",
      " |~~ train@86592  Loss: 0.003387 Acc: 13.0938\n",
      " |~~ train@86656  Loss: 0.003013 Acc: 13.1719\n",
      " |~~ train@86720  Loss: 0.002486 Acc: 13.4531\n",
      " |~~ train@86784  Loss: 0.002415 Acc: 13.4844\n",
      " |~~ train@86848  Loss: 0.003267 Acc: 13.1094\n",
      " |~~ train@86912  Loss: 0.003423 Acc: 13.1406\n",
      " |~~ train@86976  Loss: 0.003054 Acc: 13.2500\n",
      " |~~ train@87040  Loss: 0.002408 Acc: 13.5000\n",
      " |~~ train@87104  Loss: 0.003267 Acc: 13.1250\n",
      " |~~ train@87168  Loss: 0.003015 Acc: 13.2344\n",
      " |~~ train@87232  Loss: 0.002898 Acc: 13.2500\n",
      " |~~ train@87296  Loss: 0.002904 Acc: 13.2812\n",
      " |~~ train@87360  Loss: 0.002397 Acc: 13.4375\n",
      " |~~ train@87424  Loss: 0.003004 Acc: 13.2812\n",
      " |~~ train@87488  Loss: 0.002939 Acc: 13.2500\n",
      " |~~ train@87552  Loss: 0.003158 Acc: 13.1875\n",
      " |~~ train@87616  Loss: 0.003087 Acc: 13.2188\n",
      " |~~ train@87680  Loss: 0.002793 Acc: 13.3125\n",
      " |~~ train@87744  Loss: 0.003085 Acc: 13.2031\n",
      " |~~ train@87808  Loss: 0.002616 Acc: 13.4062\n",
      " |~~ train@87872  Loss: 0.002830 Acc: 13.3438\n",
      " |~~ train@87936  Loss: 0.003674 Acc: 12.9844\n",
      " |~~ train@88000  Loss: 0.002925 Acc: 13.2500\n",
      " |~~ train@88064  Loss: 0.002705 Acc: 13.3125\n",
      " |~~ train@88128  Loss: 0.002996 Acc: 13.2500\n",
      " |~~ train@88192  Loss: 0.002395 Acc: 13.4375\n",
      " |~~ train@88256  Loss: 0.003292 Acc: 13.1719\n",
      " |~~ train@88320  Loss: 0.002946 Acc: 13.2656\n",
      " |~~ train@88384  Loss: 0.002763 Acc: 13.3281\n",
      " |~~ train@88448  Loss: 0.002857 Acc: 13.2656\n",
      " |~~ train@88512  Loss: 0.002597 Acc: 13.3750\n",
      " |~~ train@88576  Loss: 0.002928 Acc: 13.2656\n",
      " |~~ train@88640  Loss: 0.002601 Acc: 13.4062\n",
      " |~~ train@88704  Loss: 0.002732 Acc: 13.3438\n",
      " |~~ train@88768  Loss: 0.002852 Acc: 13.2969\n",
      " |~~ train@88832  Loss: 0.002746 Acc: 13.3594\n",
      " |~~ train@88896  Loss: 0.002788 Acc: 13.2812\n",
      " |~~ train@88960  Loss: 0.002641 Acc: 13.4062\n",
      " |~~ train@89024  Loss: 0.002732 Acc: 13.3594\n",
      " |~~ train@89088  Loss: 0.002710 Acc: 13.3594\n",
      " |~~ train@89152  Loss: 0.002244 Acc: 13.5000\n",
      " |~~ train@89216  Loss: 0.002988 Acc: 13.2344\n",
      " |~~ train@89280  Loss: 0.003425 Acc: 13.1094\n",
      " |~~ train@89344  Loss: 0.002902 Acc: 13.2969\n",
      " |~~ train@89408  Loss: 0.002403 Acc: 13.4844\n",
      " |~~ train@89472  Loss: 0.002793 Acc: 13.3438\n",
      " |~~ train@89536  Loss: 0.003034 Acc: 13.2344\n",
      " |~~ train@89600  Loss: 0.002587 Acc: 13.3594\n",
      " |~~ train@89664  Loss: 0.002976 Acc: 13.2344\n",
      " |~~ train@89728  Loss: 0.003780 Acc: 12.9531\n",
      " |~~ train@89792  Loss: 0.002636 Acc: 13.3438\n",
      " |~~ train@89856  Loss: 0.002653 Acc: 13.3750\n",
      " |~~ train@89920  Loss: 0.003171 Acc: 13.1719\n",
      " |~~ train@89984  Loss: 0.003171 Acc: 13.1406\n",
      " |~~ train@90048  Loss: 0.002897 Acc: 13.2656\n",
      " |~~ train@90112  Loss: 0.002412 Acc: 13.4688\n",
      " |~~ train@90176  Loss: 0.002327 Acc: 13.4844\n",
      " |~~ train@90240  Loss: 0.003148 Acc: 13.2500\n",
      " |~~ train@90304  Loss: 0.003508 Acc: 13.0625\n",
      " |~~ train@90368  Loss: 0.002982 Acc: 13.2500\n",
      " |~~ train@90432  Loss: 0.003139 Acc: 13.1875\n",
      " |~~ train@90496  Loss: 0.002652 Acc: 13.3594\n",
      " |~~ train@90560  Loss: 0.002807 Acc: 13.3125\n",
      " |~~ train@90624  Loss: 0.002592 Acc: 13.3594\n",
      " |~~ train@90688  Loss: 0.003502 Acc: 13.0938\n",
      " |~~ train@90752  Loss: 0.002710 Acc: 13.3281\n",
      " |~~ train@90816  Loss: 0.002813 Acc: 13.3125\n",
      " |~~ train@90880  Loss: 0.002471 Acc: 13.4375\n",
      " |~~ train@90944  Loss: 0.003333 Acc: 13.1250\n",
      " |~~ train@91008  Loss: 0.002251 Acc: 13.5312\n",
      " |~~ train@91072  Loss: 0.003674 Acc: 12.9844\n",
      " |~~ train@91136  Loss: 0.002770 Acc: 13.3594\n",
      " |~~ train@91200  Loss: 0.002862 Acc: 13.3125\n",
      " |~~ train@91264  Loss: 0.002571 Acc: 13.3750\n",
      " |~~ train@91328  Loss: 0.002445 Acc: 13.4375\n",
      " |~~ train@91392  Loss: 0.003019 Acc: 13.2344\n",
      " |~~ train@91456  Loss: 0.002379 Acc: 13.4219\n",
      " |~~ train@91520  Loss: 0.003014 Acc: 13.2188\n",
      " |~~ train@91584  Loss: 0.003126 Acc: 13.1875\n",
      " |~~ train@91648  Loss: 0.002865 Acc: 13.3125\n",
      " |~~ train@91712  Loss: 0.003610 Acc: 13.0156\n",
      " |~~ train@91776  Loss: 0.003196 Acc: 13.2188\n",
      " |~~ train@91840  Loss: 0.003079 Acc: 13.2031\n",
      " |~~ train@91904  Loss: 0.002943 Acc: 13.2656\n",
      " |~~ train@91968  Loss: 0.002963 Acc: 13.2969\n",
      " |~~ train@92032  Loss: 0.003330 Acc: 13.1250\n",
      " |~~ train@92096  Loss: 0.002926 Acc: 13.2344\n",
      " |~~ train@92160  Loss: 0.002888 Acc: 13.2969\n",
      " |~~ train@92224  Loss: 0.002794 Acc: 13.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@92288  Loss: 0.003208 Acc: 13.1875\n",
      " |~~ train@92352  Loss: 0.003464 Acc: 13.0781\n",
      " |~~ train@92416  Loss: 0.002061 Acc: 13.5938\n",
      " |~~ train@92480  Loss: 0.003187 Acc: 13.1562\n",
      " |~~ train@92544  Loss: 0.003186 Acc: 13.1562\n",
      " |~~ train@92608  Loss: 0.002611 Acc: 13.3438\n",
      " |~~ train@92672  Loss: 0.002455 Acc: 13.5000\n",
      " |~~ train@92736  Loss: 0.002459 Acc: 13.4219\n",
      " |~~ train@92800  Loss: 0.002273 Acc: 13.5312\n",
      " |~~ train@92864  Loss: 0.002999 Acc: 13.2656\n",
      " |~~ train@92928  Loss: 0.003265 Acc: 13.1250\n",
      " |~~ train@92992  Loss: 0.003329 Acc: 13.1719\n",
      " |~~ train@93056  Loss: 0.002873 Acc: 13.2812\n",
      " |~~ train@93120  Loss: 0.003168 Acc: 13.1875\n",
      " |~~ train@93184  Loss: 0.002999 Acc: 13.2344\n",
      " |~~ train@93248  Loss: 0.002987 Acc: 13.2344\n",
      " |~~ train@93312  Loss: 0.002683 Acc: 13.3594\n",
      " |~~ train@93376  Loss: 0.003090 Acc: 13.2188\n",
      " |~~ train@93440  Loss: 0.002778 Acc: 13.3125\n",
      " |~~ train@93504  Loss: 0.002908 Acc: 13.2812\n",
      " |~~ train@93568  Loss: 0.002661 Acc: 13.3594\n",
      " |~~ train@93632  Loss: 0.002705 Acc: 13.3438\n",
      " |~~ train@93696  Loss: 0.003286 Acc: 13.1562\n",
      " |~~ train@93760  Loss: 0.002553 Acc: 13.4219\n",
      " |~~ train@93824  Loss: 0.003383 Acc: 13.1250\n",
      " |~~ train@93888  Loss: 0.002671 Acc: 13.3594\n",
      " |~~ train@93952  Loss: 0.002495 Acc: 13.4219\n",
      " |~~ train@94016  Loss: 0.003027 Acc: 13.2188\n",
      " |~~ train@94080  Loss: 0.003195 Acc: 13.1250\n",
      " |~~ train@94144  Loss: 0.002914 Acc: 13.3125\n",
      " |~~ train@94208  Loss: 0.002842 Acc: 13.2812\n",
      " |~~ train@94272  Loss: 0.003035 Acc: 13.2344\n",
      " |~~ train@94336  Loss: 0.003303 Acc: 13.1094\n",
      " |~~ train@94400  Loss: 0.003026 Acc: 13.1719\n",
      " |~~ train@94464  Loss: 0.003141 Acc: 13.1875\n",
      " |~~ train@94528  Loss: 0.002820 Acc: 13.2969\n",
      " |~~ train@94592  Loss: 0.003114 Acc: 13.1562\n",
      " |~~ train@94656  Loss: 0.002661 Acc: 13.3906\n",
      " |~~ train@94720  Loss: 0.002693 Acc: 13.3281\n",
      " |~~ train@94784  Loss: 0.003401 Acc: 13.0938\n",
      " |~~ train@94848  Loss: 0.003011 Acc: 13.2188\n",
      " |~~ train@94912  Loss: 0.003102 Acc: 13.1875\n",
      " |~~ train@94976  Loss: 0.003183 Acc: 13.1562\n",
      " |~~ train@95040  Loss: 0.002644 Acc: 13.3438\n",
      " |~~ train@95104  Loss: 0.002652 Acc: 13.3281\n",
      " |~~ train@95168  Loss: 0.003497 Acc: 13.0938\n",
      " |~~ train@95232  Loss: 0.002703 Acc: 13.3750\n",
      " |~~ train@95296  Loss: 0.002109 Acc: 13.5938\n",
      " |~~ train@95360  Loss: 0.002821 Acc: 13.3125\n",
      " |~~ train@95424  Loss: 0.003189 Acc: 13.1562\n",
      " |~~ train@95488  Loss: 0.002959 Acc: 13.2812\n",
      " |~~ train@95552  Loss: 0.002906 Acc: 13.2500\n",
      " |~~ train@95616  Loss: 0.003470 Acc: 13.1094\n",
      " |~~ train@95680  Loss: 0.002684 Acc: 13.3750\n",
      " |~~ train@95744  Loss: 0.003276 Acc: 13.1719\n",
      " |~~ train@95808  Loss: 0.003116 Acc: 13.2031\n",
      " |~~ train@95872  Loss: 0.002263 Acc: 13.5156\n",
      " |~~ train@95936  Loss: 0.002596 Acc: 13.4375\n",
      " |~~ train@96000  Loss: 0.002462 Acc: 13.4375\n",
      " |~~ train@96064  Loss: 0.003146 Acc: 13.1562\n",
      " |~~ train@96128  Loss: 0.002463 Acc: 13.4062\n",
      " |~~ train@96192  Loss: 0.002742 Acc: 13.3438\n",
      " |~~ train@96256  Loss: 0.002752 Acc: 13.3281\n",
      " |~~ train@96320  Loss: 0.003195 Acc: 13.1875\n",
      " |~~ train@96384  Loss: 0.002988 Acc: 13.2031\n",
      " |~~ train@96448  Loss: 0.002705 Acc: 13.3281\n",
      " |~~ train@96512  Loss: 0.002757 Acc: 13.2969\n",
      " |~~ train@96576  Loss: 0.002920 Acc: 13.3125\n",
      " |~~ train@96640  Loss: 0.002983 Acc: 13.2812\n",
      " |~~ train@96704  Loss: 0.003299 Acc: 13.1406\n",
      " |~~ train@96768  Loss: 0.002641 Acc: 13.3906\n",
      " |~~ train@96832  Loss: 0.003281 Acc: 13.1094\n",
      " |~~ train@96896  Loss: 0.002974 Acc: 13.2500\n",
      " |~~ train@96960  Loss: 0.002552 Acc: 13.3906\n",
      " |~~ train@97024  Loss: 0.002613 Acc: 13.3594\n",
      " |~~ train@97088  Loss: 0.002979 Acc: 13.2188\n",
      " |~~ train@97152  Loss: 0.003096 Acc: 13.1719\n",
      " |~~ train@97216  Loss: 0.002905 Acc: 13.2656\n",
      " |~~ train@97280  Loss: 0.002776 Acc: 13.3281\n",
      " |~~ train@97344  Loss: 0.003382 Acc: 13.0938\n",
      " |~~ train@97408  Loss: 0.003264 Acc: 13.1406\n",
      " |~~ train@97472  Loss: 0.002785 Acc: 13.3438\n",
      " |~~ train@97536  Loss: 0.002579 Acc: 13.4062\n",
      " |~~ train@97600  Loss: 0.002281 Acc: 13.4844\n",
      " |~~ train@97664  Loss: 0.002500 Acc: 13.4531\n",
      " |~~ train@97728  Loss: 0.003401 Acc: 13.0781\n",
      " |~~ train@97792  Loss: 0.003517 Acc: 13.0469\n",
      " |~~ train@97856  Loss: 0.002886 Acc: 13.2656\n",
      " |~~ train@97920  Loss: 0.003125 Acc: 13.2344\n",
      " |~~ train@97984  Loss: 0.002428 Acc: 13.4219\n",
      " |~~ train@98048  Loss: 0.003087 Acc: 13.2031\n",
      " |~~ train@98112  Loss: 0.002422 Acc: 13.4062\n",
      " |~~ train@98176  Loss: 0.002655 Acc: 13.3750\n",
      " |~~ train@98240  Loss: 0.002901 Acc: 13.2656\n",
      " |~~ train@98304  Loss: 0.002575 Acc: 13.3594\n",
      " |~~ train@98368  Loss: 0.003070 Acc: 13.1562\n",
      " |~~ train@98432  Loss: 0.002797 Acc: 13.2812\n",
      " |~~ train@98496  Loss: 0.002862 Acc: 13.3125\n",
      " |~~ train@98560  Loss: 0.003440 Acc: 13.0938\n",
      " |~~ train@98624  Loss: 0.002782 Acc: 13.3594\n",
      " |~~ train@98688  Loss: 0.002712 Acc: 13.3594\n",
      " |~~ train@98752  Loss: 0.002900 Acc: 13.2344\n",
      " |~~ train@98816  Loss: 0.003190 Acc: 13.1562\n",
      " |~~ train@98880  Loss: 0.003129 Acc: 13.1875\n",
      " |~~ train@98944  Loss: 0.002880 Acc: 13.3125\n",
      " |~~ train@99008  Loss: 0.003013 Acc: 13.2188\n",
      " |~~ train@99072  Loss: 0.002980 Acc: 13.2656\n",
      " |~~ train@99136  Loss: 0.002362 Acc: 13.4531\n",
      " |~~ train@99200  Loss: 0.003565 Acc: 13.0312\n",
      " |~~ train@99264  Loss: 0.002935 Acc: 13.2500\n",
      " |~~ train@99328  Loss: 0.002435 Acc: 13.4375\n",
      " |~~ train@99392  Loss: 0.002787 Acc: 13.2812\n",
      " |~~ train@99456  Loss: 0.001965 Acc: 13.6094\n",
      " |~~ train@99520  Loss: 0.002942 Acc: 13.2656\n",
      " |~~ train@99584  Loss: 0.003278 Acc: 13.1094\n",
      " |~~ train@99648  Loss: 0.002714 Acc: 13.2969\n",
      " |~~ train@99712  Loss: 0.002184 Acc: 13.5781\n",
      " |~~ train@99776  Loss: 0.002565 Acc: 13.3594\n",
      " |~~ train@99840  Loss: 0.002491 Acc: 13.4219\n",
      " |~~ train@99904  Loss: 0.002943 Acc: 13.2969\n",
      " |~~ train@99968  Loss: 0.002962 Acc: 13.2500\n",
      " |~~ train@100032  Loss: 0.002331 Acc: 13.4844\n",
      " |~~ train@100096  Loss: 0.003456 Acc: 13.0312\n",
      " |~~ train@100160  Loss: 0.003047 Acc: 13.2344\n",
      " |~~ train@100224  Loss: 0.002863 Acc: 13.3125\n",
      " |~~ train@100288  Loss: 0.002660 Acc: 13.3906\n",
      " |~~ train@100352  Loss: 0.002325 Acc: 13.4375\n",
      " |~~ train@100416  Loss: 0.002314 Acc: 13.5000\n",
      " |~~ train@100480  Loss: 0.003265 Acc: 13.1094\n",
      " |~~ train@100544  Loss: 0.002753 Acc: 13.3750\n",
      " |~~ train@100608  Loss: 0.002894 Acc: 13.2812\n",
      " |~~ train@100672  Loss: 0.003019 Acc: 13.2344\n",
      " |~~ train@100736  Loss: 0.003153 Acc: 13.1562\n",
      " |~~ train@100800  Loss: 0.003370 Acc: 13.1094\n",
      " |~~ train@100864  Loss: 0.002671 Acc: 13.3594\n",
      " |~~ train@100908  Loss: 0.003685 Acc: 13.4318\n",
      "train  Loss: 0.003511 Acc: 13.2042\n",
      " |~~ val@64  Loss: 0.003350 Acc: 13.1094\n",
      " |~~ val@128  Loss: 0.003331 Acc: 13.0938\n",
      " |~~ val@192  Loss: 0.002781 Acc: 13.3125\n",
      " |~~ val@256  Loss: 0.003160 Acc: 13.1250\n",
      " |~~ val@320  Loss: 0.002912 Acc: 13.2500\n",
      " |~~ val@384  Loss: 0.003440 Acc: 13.0469\n",
      " |~~ val@448  Loss: 0.002998 Acc: 13.1562\n",
      " |~~ val@512  Loss: 0.002975 Acc: 13.2500\n",
      " |~~ val@576  Loss: 0.003001 Acc: 13.2656\n",
      " |~~ val@640  Loss: 0.003451 Acc: 13.0312\n",
      " |~~ val@704  Loss: 0.002552 Acc: 13.3906\n",
      " |~~ val@768  Loss: 0.002986 Acc: 13.2656\n",
      " |~~ val@832  Loss: 0.002243 Acc: 13.5000\n",
      " |~~ val@896  Loss: 0.002864 Acc: 13.3125\n",
      " |~~ val@960  Loss: 0.002270 Acc: 13.5156\n",
      " |~~ val@1024  Loss: 0.003098 Acc: 13.2344\n",
      " |~~ val@1088  Loss: 0.003024 Acc: 13.2188\n",
      " |~~ val@1152  Loss: 0.002754 Acc: 13.2969\n",
      " |~~ val@1216  Loss: 0.002963 Acc: 13.2500\n",
      " |~~ val@1280  Loss: 0.002842 Acc: 13.3281\n",
      " |~~ val@1344  Loss: 0.002446 Acc: 13.4688\n",
      " |~~ val@1408  Loss: 0.002913 Acc: 13.2500\n",
      " |~~ val@1472  Loss: 0.003203 Acc: 13.1719\n",
      " |~~ val@1536  Loss: 0.003089 Acc: 13.2188\n",
      " |~~ val@1600  Loss: 0.002734 Acc: 13.2969\n",
      " |~~ val@1664  Loss: 0.002860 Acc: 13.2500\n",
      " |~~ val@1728  Loss: 0.003073 Acc: 13.2031\n",
      " |~~ val@1792  Loss: 0.003167 Acc: 13.1875\n",
      " |~~ val@1856  Loss: 0.002996 Acc: 13.1875\n",
      " |~~ val@1920  Loss: 0.002339 Acc: 13.4062\n",
      " |~~ val@1984  Loss: 0.002634 Acc: 13.3125\n",
      " |~~ val@2048  Loss: 0.002823 Acc: 13.3281\n",
      " |~~ val@2112  Loss: 0.002644 Acc: 13.3750\n",
      " |~~ val@2176  Loss: 0.002906 Acc: 13.2656\n",
      " |~~ val@2240  Loss: 0.002824 Acc: 13.2656\n",
      " |~~ val@2304  Loss: 0.003269 Acc: 13.1406\n",
      " |~~ val@2368  Loss: 0.002594 Acc: 13.3281\n",
      " |~~ val@2432  Loss: 0.003090 Acc: 13.1875\n",
      " |~~ val@2496  Loss: 0.002859 Acc: 13.2500\n",
      " |~~ val@2560  Loss: 0.002531 Acc: 13.3906\n",
      " |~~ val@2624  Loss: 0.002663 Acc: 13.3750\n",
      " |~~ val@2688  Loss: 0.003395 Acc: 13.0625\n",
      " |~~ val@2752  Loss: 0.002713 Acc: 13.3281\n",
      " |~~ val@2816  Loss: 0.002633 Acc: 13.3438\n",
      " |~~ val@2880  Loss: 0.002915 Acc: 13.3125\n",
      " |~~ val@2944  Loss: 0.003475 Acc: 13.0469\n",
      " |~~ val@3008  Loss: 0.002707 Acc: 13.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ val@3072  Loss: 0.003217 Acc: 13.0938\n",
      " |~~ val@3136  Loss: 0.002727 Acc: 13.2969\n",
      " |~~ val@3200  Loss: 0.002768 Acc: 13.3438\n",
      " |~~ val@3264  Loss: 0.002715 Acc: 13.3281\n",
      " |~~ val@3328  Loss: 0.002056 Acc: 13.5625\n",
      " |~~ val@3392  Loss: 0.002720 Acc: 13.3438\n",
      " |~~ val@3456  Loss: 0.002851 Acc: 13.3281\n",
      " |~~ val@3520  Loss: 0.003078 Acc: 13.1719\n",
      " |~~ val@3584  Loss: 0.003512 Acc: 13.0781\n",
      " |~~ val@3648  Loss: 0.002821 Acc: 13.3125\n",
      " |~~ val@3712  Loss: 0.002552 Acc: 13.4219\n",
      " |~~ val@3776  Loss: 0.002367 Acc: 13.4531\n",
      " |~~ val@3840  Loss: 0.003201 Acc: 13.1250\n",
      " |~~ val@3904  Loss: 0.002286 Acc: 13.5000\n",
      " |~~ val@3968  Loss: 0.003011 Acc: 13.1875\n",
      " |~~ val@4032  Loss: 0.002700 Acc: 13.3750\n",
      " |~~ val@4096  Loss: 0.002742 Acc: 13.3906\n",
      " |~~ val@4160  Loss: 0.002694 Acc: 13.3438\n",
      " |~~ val@4224  Loss: 0.002514 Acc: 13.3750\n",
      " |~~ val@4288  Loss: 0.002944 Acc: 13.2969\n",
      " |~~ val@4352  Loss: 0.003286 Acc: 13.1562\n",
      " |~~ val@4416  Loss: 0.002758 Acc: 13.3125\n",
      " |~~ val@4480  Loss: 0.002522 Acc: 13.3906\n",
      " |~~ val@4544  Loss: 0.003140 Acc: 13.2031\n",
      " |~~ val@4608  Loss: 0.003162 Acc: 13.2031\n",
      " |~~ val@4672  Loss: 0.002591 Acc: 13.3750\n",
      " |~~ val@4736  Loss: 0.002357 Acc: 13.4688\n",
      " |~~ val@4800  Loss: 0.003133 Acc: 13.1562\n",
      " |~~ val@4864  Loss: 0.002890 Acc: 13.2656\n",
      " |~~ val@4928  Loss: 0.002970 Acc: 13.2344\n",
      " |~~ val@4992  Loss: 0.003184 Acc: 13.2031\n",
      " |~~ val@5056  Loss: 0.002609 Acc: 13.3906\n",
      " |~~ val@5120  Loss: 0.002662 Acc: 13.3906\n",
      " |~~ val@5184  Loss: 0.003026 Acc: 13.2188\n",
      " |~~ val@5248  Loss: 0.003293 Acc: 13.0938\n",
      " |~~ val@5312  Loss: 0.003148 Acc: 13.1875\n",
      " |~~ val@5376  Loss: 0.003273 Acc: 13.0938\n",
      " |~~ val@5440  Loss: 0.003130 Acc: 13.1719\n",
      " |~~ val@5504  Loss: 0.003029 Acc: 13.2031\n",
      " |~~ val@5568  Loss: 0.002589 Acc: 13.3906\n",
      " |~~ val@5632  Loss: 0.003200 Acc: 13.1562\n",
      " |~~ val@5696  Loss: 0.002792 Acc: 13.3125\n",
      " |~~ val@5760  Loss: 0.002817 Acc: 13.3125\n",
      " |~~ val@5824  Loss: 0.002936 Acc: 13.2500\n",
      " |~~ val@5888  Loss: 0.003378 Acc: 13.1719\n",
      " |~~ val@5952  Loss: 0.002945 Acc: 13.2656\n",
      " |~~ val@6016  Loss: 0.003137 Acc: 13.2031\n",
      " |~~ val@6080  Loss: 0.002852 Acc: 13.2969\n",
      " |~~ val@6144  Loss: 0.002681 Acc: 13.3281\n",
      " |~~ val@6208  Loss: 0.002731 Acc: 13.3438\n",
      " |~~ val@6272  Loss: 0.002937 Acc: 13.2656\n",
      " |~~ val@6336  Loss: 0.002893 Acc: 13.3438\n",
      " |~~ val@6400  Loss: 0.003469 Acc: 13.1094\n",
      " |~~ val@6464  Loss: 0.002794 Acc: 13.2812\n",
      " |~~ val@6528  Loss: 0.003138 Acc: 13.1562\n",
      " |~~ val@6592  Loss: 0.002995 Acc: 13.2500\n",
      " |~~ val@6656  Loss: 0.002702 Acc: 13.3281\n",
      " |~~ val@6720  Loss: 0.002656 Acc: 13.3750\n",
      " |~~ val@6784  Loss: 0.002410 Acc: 13.4062\n",
      " |~~ val@6848  Loss: 0.003114 Acc: 13.2500\n",
      " |~~ val@6912  Loss: 0.003020 Acc: 13.1875\n",
      " |~~ val@6976  Loss: 0.003124 Acc: 13.1719\n",
      " |~~ val@7040  Loss: 0.002537 Acc: 13.3906\n",
      " |~~ val@7104  Loss: 0.002894 Acc: 13.3125\n",
      " |~~ val@7168  Loss: 0.003255 Acc: 13.1719\n",
      " |~~ val@7232  Loss: 0.002700 Acc: 13.3438\n",
      " |~~ val@7296  Loss: 0.002567 Acc: 13.3594\n",
      " |~~ val@7360  Loss: 0.002923 Acc: 13.2969\n",
      " |~~ val@7424  Loss: 0.002337 Acc: 13.4375\n",
      " |~~ val@7488  Loss: 0.003185 Acc: 13.2031\n",
      " |~~ val@7552  Loss: 0.002402 Acc: 13.4688\n",
      " |~~ val@7616  Loss: 0.002406 Acc: 13.4375\n",
      " |~~ val@7680  Loss: 0.002802 Acc: 13.3594\n",
      " |~~ val@7744  Loss: 0.002434 Acc: 13.4062\n",
      " |~~ val@7808  Loss: 0.002181 Acc: 13.5469\n",
      " |~~ val@7872  Loss: 0.002991 Acc: 13.2500\n",
      " |~~ val@7936  Loss: 0.003190 Acc: 13.1719\n",
      " |~~ val@8000  Loss: 0.002849 Acc: 13.2344\n",
      " |~~ val@8064  Loss: 0.003061 Acc: 13.2188\n",
      " |~~ val@8128  Loss: 0.002458 Acc: 13.4531\n",
      " |~~ val@8192  Loss: 0.002456 Acc: 13.4375\n",
      " |~~ val@8256  Loss: 0.002295 Acc: 13.4844\n",
      " |~~ val@8320  Loss: 0.002789 Acc: 13.2969\n",
      " |~~ val@8384  Loss: 0.002934 Acc: 13.2188\n",
      " |~~ val@8448  Loss: 0.003008 Acc: 13.2031\n",
      " |~~ val@8512  Loss: 0.002809 Acc: 13.3125\n",
      " |~~ val@8576  Loss: 0.003101 Acc: 13.2188\n",
      " |~~ val@8640  Loss: 0.002532 Acc: 13.3750\n",
      " |~~ val@8704  Loss: 0.003210 Acc: 13.1719\n",
      " |~~ val@8768  Loss: 0.002711 Acc: 13.2969\n",
      " |~~ val@8832  Loss: 0.002647 Acc: 13.3750\n",
      " |~~ val@8896  Loss: 0.002608 Acc: 13.3906\n",
      " |~~ val@8960  Loss: 0.003192 Acc: 13.1875\n",
      " |~~ val@9024  Loss: 0.002654 Acc: 13.2969\n",
      " |~~ val@9088  Loss: 0.002521 Acc: 13.3438\n",
      " |~~ val@9152  Loss: 0.002744 Acc: 13.3750\n",
      " |~~ val@9216  Loss: 0.002517 Acc: 13.4062\n",
      " |~~ val@9280  Loss: 0.003086 Acc: 13.2344\n",
      " |~~ val@9344  Loss: 0.003280 Acc: 13.0781\n",
      " |~~ val@9408  Loss: 0.002871 Acc: 13.2812\n",
      " |~~ val@9472  Loss: 0.003261 Acc: 13.1719\n",
      " |~~ val@9536  Loss: 0.002561 Acc: 13.4062\n",
      " |~~ val@9600  Loss: 0.002749 Acc: 13.3281\n",
      " |~~ val@9664  Loss: 0.003128 Acc: 13.1875\n",
      " |~~ val@9728  Loss: 0.002477 Acc: 13.4062\n",
      " |~~ val@9792  Loss: 0.002367 Acc: 13.5000\n",
      " |~~ val@9856  Loss: 0.002905 Acc: 13.2500\n",
      " |~~ val@9920  Loss: 0.002828 Acc: 13.2656\n",
      " |~~ val@9984  Loss: 0.002966 Acc: 13.2656\n",
      " |~~ val@10048  Loss: 0.003219 Acc: 13.1719\n",
      " |~~ val@10112  Loss: 0.002656 Acc: 13.3750\n",
      " |~~ val@10176  Loss: 0.003129 Acc: 13.1562\n",
      " |~~ val@10240  Loss: 0.002515 Acc: 13.3438\n",
      " |~~ val@10304  Loss: 0.002919 Acc: 13.2812\n",
      " |~~ val@10368  Loss: 0.002463 Acc: 13.4062\n",
      " |~~ val@10432  Loss: 0.002978 Acc: 13.2031\n",
      " |~~ val@10496  Loss: 0.002024 Acc: 13.5781\n",
      " |~~ val@10560  Loss: 0.003046 Acc: 13.2188\n",
      " |~~ val@10624  Loss: 0.003235 Acc: 13.1719\n",
      " |~~ val@10688  Loss: 0.002838 Acc: 13.2812\n",
      " |~~ val@10752  Loss: 0.002115 Acc: 13.5781\n",
      " |~~ val@10816  Loss: 0.002784 Acc: 13.3125\n",
      " |~~ val@10880  Loss: 0.002891 Acc: 13.2812\n",
      " |~~ val@10944  Loss: 0.002848 Acc: 13.2656\n",
      " |~~ val@11008  Loss: 0.002790 Acc: 13.2969\n",
      " |~~ val@11072  Loss: 0.003490 Acc: 13.1250\n",
      " |~~ val@11136  Loss: 0.002251 Acc: 13.5312\n",
      " |~~ val@11200  Loss: 0.003443 Acc: 13.1250\n",
      " |~~ val@11212  Loss: 0.020183 Acc: 13.0000\n",
      "val  Loss: 0.002873 Acc: 13.2853\n",
      "Epoch 1/7\n",
      "----------\n",
      " |~~ train@64  Loss: 0.002563 Acc: 13.3594\n",
      " |~~ train@128  Loss: 0.003028 Acc: 13.2500\n",
      " |~~ train@192  Loss: 0.002990 Acc: 13.2188\n",
      " |~~ train@256  Loss: 0.002936 Acc: 13.2812\n",
      " |~~ train@320  Loss: 0.003394 Acc: 13.1406\n",
      " |~~ train@384  Loss: 0.002756 Acc: 13.2969\n",
      " |~~ train@448  Loss: 0.002675 Acc: 13.3906\n",
      " |~~ train@512  Loss: 0.002966 Acc: 13.2656\n",
      " |~~ train@576  Loss: 0.002549 Acc: 13.3750\n",
      " |~~ train@640  Loss: 0.002917 Acc: 13.2812\n",
      " |~~ train@704  Loss: 0.003450 Acc: 13.0938\n",
      " |~~ train@768  Loss: 0.002121 Acc: 13.5469\n",
      " |~~ train@832  Loss: 0.002656 Acc: 13.3750\n",
      " |~~ train@896  Loss: 0.002829 Acc: 13.2500\n",
      " |~~ train@960  Loss: 0.003142 Acc: 13.1719\n",
      " |~~ train@1024  Loss: 0.003082 Acc: 13.2500\n",
      " |~~ train@1088  Loss: 0.002409 Acc: 13.4375\n",
      " |~~ train@1152  Loss: 0.002164 Acc: 13.5156\n",
      " |~~ train@1216  Loss: 0.002628 Acc: 13.3281\n",
      " |~~ train@1280  Loss: 0.002924 Acc: 13.2969\n",
      " |~~ train@1344  Loss: 0.003072 Acc: 13.1719\n",
      " |~~ train@1408  Loss: 0.003158 Acc: 13.1719\n",
      " |~~ train@1472  Loss: 0.003039 Acc: 13.2656\n",
      " |~~ train@1536  Loss: 0.003264 Acc: 13.1562\n",
      " |~~ train@1600  Loss: 0.002781 Acc: 13.3125\n",
      " |~~ train@1664  Loss: 0.003239 Acc: 13.1719\n",
      " |~~ train@1728  Loss: 0.003242 Acc: 13.1562\n",
      " |~~ train@1792  Loss: 0.002913 Acc: 13.2500\n",
      " |~~ train@1856  Loss: 0.003869 Acc: 12.9219\n",
      " |~~ train@1920  Loss: 0.002973 Acc: 13.1875\n",
      " |~~ train@1984  Loss: 0.002835 Acc: 13.2812\n",
      " |~~ train@2048  Loss: 0.002271 Acc: 13.4844\n",
      " |~~ train@2112  Loss: 0.002701 Acc: 13.3750\n",
      " |~~ train@2176  Loss: 0.002604 Acc: 13.3750\n",
      " |~~ train@2240  Loss: 0.002736 Acc: 13.3125\n",
      " |~~ train@2304  Loss: 0.002870 Acc: 13.2500\n",
      " |~~ train@2368  Loss: 0.003221 Acc: 13.1562\n",
      " |~~ train@2432  Loss: 0.002710 Acc: 13.3281\n",
      " |~~ train@2496  Loss: 0.002558 Acc: 13.4062\n",
      " |~~ train@2560  Loss: 0.002346 Acc: 13.4062\n",
      " |~~ train@2624  Loss: 0.002789 Acc: 13.2812\n",
      " |~~ train@2688  Loss: 0.003197 Acc: 13.2344\n",
      " |~~ train@2752  Loss: 0.003298 Acc: 13.1406\n",
      " |~~ train@2816  Loss: 0.003085 Acc: 13.2344\n",
      " |~~ train@2880  Loss: 0.003564 Acc: 13.0312\n",
      " |~~ train@2944  Loss: 0.002879 Acc: 13.2969\n",
      " |~~ train@3008  Loss: 0.002842 Acc: 13.2344\n",
      " |~~ train@3072  Loss: 0.002727 Acc: 13.3438\n",
      " |~~ train@3136  Loss: 0.002535 Acc: 13.3750\n",
      " |~~ train@3200  Loss: 0.002795 Acc: 13.2656\n",
      " |~~ train@3264  Loss: 0.002905 Acc: 13.2500\n",
      " |~~ train@3328  Loss: 0.003026 Acc: 13.2500\n",
      " |~~ train@3392  Loss: 0.003168 Acc: 13.2031\n",
      " |~~ train@3456  Loss: 0.003320 Acc: 13.1406\n",
      " |~~ train@3520  Loss: 0.003024 Acc: 13.1719\n",
      " |~~ train@3584  Loss: 0.002664 Acc: 13.3125\n",
      " |~~ train@3648  Loss: 0.002702 Acc: 13.3438\n",
      " |~~ train@3712  Loss: 0.003014 Acc: 13.2500\n",
      " |~~ train@3776  Loss: 0.002781 Acc: 13.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@3840  Loss: 0.003365 Acc: 13.1250\n",
      " |~~ train@3904  Loss: 0.003110 Acc: 13.2188\n",
      " |~~ train@3968  Loss: 0.002774 Acc: 13.3125\n",
      " |~~ train@4032  Loss: 0.003160 Acc: 13.1875\n",
      " |~~ train@4096  Loss: 0.003139 Acc: 13.2031\n",
      " |~~ train@4160  Loss: 0.003170 Acc: 13.2188\n",
      " |~~ train@4224  Loss: 0.002813 Acc: 13.3125\n",
      " |~~ train@4288  Loss: 0.003263 Acc: 13.1406\n",
      " |~~ train@4352  Loss: 0.002438 Acc: 13.4531\n",
      " |~~ train@4416  Loss: 0.002895 Acc: 13.2812\n",
      " |~~ train@4480  Loss: 0.002719 Acc: 13.3438\n",
      " |~~ train@4544  Loss: 0.003131 Acc: 13.1562\n",
      " |~~ train@4608  Loss: 0.002888 Acc: 13.2969\n",
      " |~~ train@4672  Loss: 0.003092 Acc: 13.2188\n",
      " |~~ train@4736  Loss: 0.002472 Acc: 13.4219\n",
      " |~~ train@4800  Loss: 0.002834 Acc: 13.2656\n",
      " |~~ train@4864  Loss: 0.002815 Acc: 13.2812\n",
      " |~~ train@4928  Loss: 0.003252 Acc: 13.1250\n",
      " |~~ train@4992  Loss: 0.002800 Acc: 13.2812\n",
      " |~~ train@5056  Loss: 0.002760 Acc: 13.3281\n",
      " |~~ train@5120  Loss: 0.002743 Acc: 13.3125\n",
      " |~~ train@5184  Loss: 0.002398 Acc: 13.4062\n",
      " |~~ train@5248  Loss: 0.003097 Acc: 13.2656\n",
      " |~~ train@5312  Loss: 0.002567 Acc: 13.3594\n",
      " |~~ train@5376  Loss: 0.002763 Acc: 13.2969\n",
      " |~~ train@5440  Loss: 0.003126 Acc: 13.2031\n",
      " |~~ train@5504  Loss: 0.003362 Acc: 13.1250\n",
      " |~~ train@5568  Loss: 0.003310 Acc: 13.0938\n",
      " |~~ train@5632  Loss: 0.003250 Acc: 13.1406\n",
      " |~~ train@5696  Loss: 0.002965 Acc: 13.2188\n",
      " |~~ train@5760  Loss: 0.002059 Acc: 13.5781\n",
      " |~~ train@5824  Loss: 0.003164 Acc: 13.2188\n",
      " |~~ train@5888  Loss: 0.003311 Acc: 13.1406\n",
      " |~~ train@5952  Loss: 0.003024 Acc: 13.2344\n",
      " |~~ train@6016  Loss: 0.003151 Acc: 13.1875\n",
      " |~~ train@6080  Loss: 0.002415 Acc: 13.4531\n",
      " |~~ train@6144  Loss: 0.003102 Acc: 13.1719\n",
      " |~~ train@6208  Loss: 0.002580 Acc: 13.3438\n",
      " |~~ train@6272  Loss: 0.002233 Acc: 13.5000\n",
      " |~~ train@6336  Loss: 0.003134 Acc: 13.1562\n",
      " |~~ train@6400  Loss: 0.003454 Acc: 13.1094\n",
      " |~~ train@6464  Loss: 0.002622 Acc: 13.3906\n",
      " |~~ train@6528  Loss: 0.003324 Acc: 13.0938\n",
      " |~~ train@6592  Loss: 0.002690 Acc: 13.3594\n",
      " |~~ train@6656  Loss: 0.003167 Acc: 13.1719\n",
      " |~~ train@6720  Loss: 0.002732 Acc: 13.2969\n",
      " |~~ train@6784  Loss: 0.003359 Acc: 13.1406\n",
      " |~~ train@6848  Loss: 0.002749 Acc: 13.3281\n",
      " |~~ train@6912  Loss: 0.003252 Acc: 13.1406\n",
      " |~~ train@6976  Loss: 0.002810 Acc: 13.2500\n",
      " |~~ train@7040  Loss: 0.002784 Acc: 13.3125\n",
      " |~~ train@7104  Loss: 0.003318 Acc: 13.0938\n",
      " |~~ train@7168  Loss: 0.002786 Acc: 13.2812\n",
      " |~~ train@7232  Loss: 0.003071 Acc: 13.2031\n",
      " |~~ train@7296  Loss: 0.003530 Acc: 13.0000\n",
      " |~~ train@7360  Loss: 0.003009 Acc: 13.2812\n",
      " |~~ train@7424  Loss: 0.002922 Acc: 13.2500\n",
      " |~~ train@7488  Loss: 0.003210 Acc: 13.1406\n",
      " |~~ train@7552  Loss: 0.003156 Acc: 13.1562\n",
      " |~~ train@7616  Loss: 0.003180 Acc: 13.2031\n",
      " |~~ train@7680  Loss: 0.003115 Acc: 13.2031\n",
      " |~~ train@7744  Loss: 0.002953 Acc: 13.2812\n",
      " |~~ train@7808  Loss: 0.002729 Acc: 13.2812\n",
      " |~~ train@7872  Loss: 0.002882 Acc: 13.2500\n",
      " |~~ train@7936  Loss: 0.003000 Acc: 13.2500\n",
      " |~~ train@8000  Loss: 0.002814 Acc: 13.3125\n",
      " |~~ train@8064  Loss: 0.003482 Acc: 13.0312\n",
      " |~~ train@8128  Loss: 0.002328 Acc: 13.4688\n",
      " |~~ train@8192  Loss: 0.002946 Acc: 13.2656\n",
      " |~~ train@8256  Loss: 0.002531 Acc: 13.3750\n",
      " |~~ train@8320  Loss: 0.003257 Acc: 13.1562\n",
      " |~~ train@8384  Loss: 0.002405 Acc: 13.4219\n",
      " |~~ train@8448  Loss: 0.002996 Acc: 13.2656\n",
      " |~~ train@8512  Loss: 0.003142 Acc: 13.2031\n",
      " |~~ train@8576  Loss: 0.003191 Acc: 13.1875\n",
      " |~~ train@8640  Loss: 0.002861 Acc: 13.2812\n",
      " |~~ train@8704  Loss: 0.001946 Acc: 13.5625\n",
      " |~~ train@8768  Loss: 0.002647 Acc: 13.3906\n",
      " |~~ train@8832  Loss: 0.003309 Acc: 13.0938\n",
      " |~~ train@8896  Loss: 0.002311 Acc: 13.5000\n",
      " |~~ train@8960  Loss: 0.002958 Acc: 13.2656\n",
      " |~~ train@9024  Loss: 0.002996 Acc: 13.2656\n",
      " |~~ train@9088  Loss: 0.002417 Acc: 13.4531\n",
      " |~~ train@9152  Loss: 0.002856 Acc: 13.2656\n",
      " |~~ train@9216  Loss: 0.002426 Acc: 13.4375\n",
      " |~~ train@9280  Loss: 0.002576 Acc: 13.4062\n",
      " |~~ train@9344  Loss: 0.002746 Acc: 13.2656\n",
      " |~~ train@9408  Loss: 0.002850 Acc: 13.3281\n",
      " |~~ train@9472  Loss: 0.002500 Acc: 13.3750\n",
      " |~~ train@9536  Loss: 0.002639 Acc: 13.3125\n",
      " |~~ train@9600  Loss: 0.002331 Acc: 13.4844\n",
      " |~~ train@9664  Loss: 0.002802 Acc: 13.3281\n",
      " |~~ train@9728  Loss: 0.002847 Acc: 13.2656\n",
      " |~~ train@9792  Loss: 0.002811 Acc: 13.3125\n",
      " |~~ train@9856  Loss: 0.003438 Acc: 13.0625\n",
      " |~~ train@9920  Loss: 0.003356 Acc: 13.0781\n",
      " |~~ train@9984  Loss: 0.003247 Acc: 13.1719\n",
      " |~~ train@10048  Loss: 0.003149 Acc: 13.2031\n",
      " |~~ train@10112  Loss: 0.002506 Acc: 13.3906\n",
      " |~~ train@10176  Loss: 0.003170 Acc: 13.1094\n",
      " |~~ train@10240  Loss: 0.003738 Acc: 13.0156\n",
      " |~~ train@10304  Loss: 0.003016 Acc: 13.2188\n",
      " |~~ train@10368  Loss: 0.002422 Acc: 13.4219\n",
      " |~~ train@10432  Loss: 0.002798 Acc: 13.3594\n",
      " |~~ train@10496  Loss: 0.002684 Acc: 13.3750\n",
      " |~~ train@10560  Loss: 0.003072 Acc: 13.1875\n",
      " |~~ train@10624  Loss: 0.002875 Acc: 13.2656\n",
      " |~~ train@10688  Loss: 0.002674 Acc: 13.2969\n",
      " |~~ train@10752  Loss: 0.003235 Acc: 13.1562\n",
      " |~~ train@10816  Loss: 0.003087 Acc: 13.2344\n",
      " |~~ train@10880  Loss: 0.003039 Acc: 13.2656\n",
      " |~~ train@10944  Loss: 0.002514 Acc: 13.4062\n",
      " |~~ train@11008  Loss: 0.002893 Acc: 13.2656\n",
      " |~~ train@11072  Loss: 0.002270 Acc: 13.5000\n",
      " |~~ train@11136  Loss: 0.002772 Acc: 13.3281\n",
      " |~~ train@11200  Loss: 0.003169 Acc: 13.1719\n",
      " |~~ train@11264  Loss: 0.002661 Acc: 13.3438\n",
      " |~~ train@11328  Loss: 0.003287 Acc: 13.1406\n",
      " |~~ train@11392  Loss: 0.002456 Acc: 13.4219\n",
      " |~~ train@11456  Loss: 0.002351 Acc: 13.4688\n",
      " |~~ train@11520  Loss: 0.002894 Acc: 13.2656\n",
      " |~~ train@11584  Loss: 0.002749 Acc: 13.2969\n",
      " |~~ train@11648  Loss: 0.003273 Acc: 13.1719\n",
      " |~~ train@11712  Loss: 0.003137 Acc: 13.1406\n",
      " |~~ train@11776  Loss: 0.003101 Acc: 13.2031\n",
      " |~~ train@11840  Loss: 0.002355 Acc: 13.4688\n",
      " |~~ train@11904  Loss: 0.002893 Acc: 13.2656\n",
      " |~~ train@11968  Loss: 0.002615 Acc: 13.3438\n",
      " |~~ train@12032  Loss: 0.002974 Acc: 13.2031\n",
      " |~~ train@12096  Loss: 0.003274 Acc: 13.0938\n",
      " |~~ train@12160  Loss: 0.002836 Acc: 13.2969\n",
      " |~~ train@12224  Loss: 0.002337 Acc: 13.4375\n",
      " |~~ train@12288  Loss: 0.002457 Acc: 13.4219\n",
      " |~~ train@12352  Loss: 0.002615 Acc: 13.3906\n",
      " |~~ train@12416  Loss: 0.003074 Acc: 13.1562\n",
      " |~~ train@12480  Loss: 0.003312 Acc: 13.1250\n",
      " |~~ train@12544  Loss: 0.002851 Acc: 13.2656\n",
      " |~~ train@12608  Loss: 0.002616 Acc: 13.3906\n",
      " |~~ train@12672  Loss: 0.002734 Acc: 13.3125\n",
      " |~~ train@12736  Loss: 0.003096 Acc: 13.2188\n",
      " |~~ train@12800  Loss: 0.002658 Acc: 13.3281\n",
      " |~~ train@12864  Loss: 0.002953 Acc: 13.2188\n",
      " |~~ train@12928  Loss: 0.002809 Acc: 13.3125\n",
      " |~~ train@12992  Loss: 0.002310 Acc: 13.4688\n",
      " |~~ train@13056  Loss: 0.002875 Acc: 13.2188\n",
      " |~~ train@13120  Loss: 0.002996 Acc: 13.2031\n",
      " |~~ train@13184  Loss: 0.002743 Acc: 13.3125\n",
      " |~~ train@13248  Loss: 0.003052 Acc: 13.2188\n",
      " |~~ train@13312  Loss: 0.002423 Acc: 13.4062\n",
      " |~~ train@13376  Loss: 0.002730 Acc: 13.3281\n",
      " |~~ train@13440  Loss: 0.003161 Acc: 13.1719\n",
      " |~~ train@13504  Loss: 0.002387 Acc: 13.4531\n",
      " |~~ train@13568  Loss: 0.003092 Acc: 13.1875\n",
      " |~~ train@13632  Loss: 0.003296 Acc: 13.1094\n",
      " |~~ train@13696  Loss: 0.002884 Acc: 13.2344\n",
      " |~~ train@13760  Loss: 0.002697 Acc: 13.3281\n",
      " |~~ train@13824  Loss: 0.002984 Acc: 13.2500\n",
      " |~~ train@13888  Loss: 0.003206 Acc: 13.1719\n",
      " |~~ train@13952  Loss: 0.002796 Acc: 13.2969\n",
      " |~~ train@14016  Loss: 0.003079 Acc: 13.2031\n",
      " |~~ train@14080  Loss: 0.003383 Acc: 13.1406\n",
      " |~~ train@14144  Loss: 0.003077 Acc: 13.2344\n",
      " |~~ train@14208  Loss: 0.002975 Acc: 13.2656\n",
      " |~~ train@14272  Loss: 0.002546 Acc: 13.3906\n",
      " |~~ train@14336  Loss: 0.003023 Acc: 13.1875\n",
      " |~~ train@14400  Loss: 0.002451 Acc: 13.3906\n",
      " |~~ train@14464  Loss: 0.003071 Acc: 13.2500\n",
      " |~~ train@14528  Loss: 0.003412 Acc: 13.1094\n",
      " |~~ train@14592  Loss: 0.002733 Acc: 13.3281\n",
      " |~~ train@14656  Loss: 0.002247 Acc: 13.5156\n",
      " |~~ train@14720  Loss: 0.002255 Acc: 13.4844\n",
      " |~~ train@14784  Loss: 0.002681 Acc: 13.3281\n",
      " |~~ train@14848  Loss: 0.002768 Acc: 13.3281\n",
      " |~~ train@14912  Loss: 0.003060 Acc: 13.1875\n",
      " |~~ train@14976  Loss: 0.002886 Acc: 13.3125\n",
      " |~~ train@15040  Loss: 0.002725 Acc: 13.3438\n",
      " |~~ train@15104  Loss: 0.003244 Acc: 13.2500\n",
      " |~~ train@15168  Loss: 0.002933 Acc: 13.2344\n",
      " |~~ train@15232  Loss: 0.002083 Acc: 13.5625\n",
      " |~~ train@15296  Loss: 0.002982 Acc: 13.2188\n",
      " |~~ train@15360  Loss: 0.002510 Acc: 13.3594\n",
      " |~~ train@15424  Loss: 0.003248 Acc: 13.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@15488  Loss: 0.002760 Acc: 13.3125\n",
      " |~~ train@15552  Loss: 0.002949 Acc: 13.2344\n",
      " |~~ train@15616  Loss: 0.002310 Acc: 13.4844\n",
      " |~~ train@15680  Loss: 0.002551 Acc: 13.3594\n",
      " |~~ train@15744  Loss: 0.003565 Acc: 13.0469\n",
      " |~~ train@15808  Loss: 0.002719 Acc: 13.3281\n",
      " |~~ train@15872  Loss: 0.002849 Acc: 13.3125\n",
      " |~~ train@15936  Loss: 0.002724 Acc: 13.3281\n",
      " |~~ train@16000  Loss: 0.003354 Acc: 13.1250\n",
      " |~~ train@16064  Loss: 0.002910 Acc: 13.2969\n",
      " |~~ train@16128  Loss: 0.002671 Acc: 13.3594\n",
      " |~~ train@16192  Loss: 0.002988 Acc: 13.2344\n",
      " |~~ train@16256  Loss: 0.003164 Acc: 13.2188\n",
      " |~~ train@16320  Loss: 0.002767 Acc: 13.3125\n",
      " |~~ train@16384  Loss: 0.002436 Acc: 13.4219\n",
      " |~~ train@16448  Loss: 0.002947 Acc: 13.2812\n",
      " |~~ train@16512  Loss: 0.002901 Acc: 13.2812\n",
      " |~~ train@16576  Loss: 0.002391 Acc: 13.4688\n",
      " |~~ train@16640  Loss: 0.003057 Acc: 13.2188\n",
      " |~~ train@16704  Loss: 0.002733 Acc: 13.3125\n",
      " |~~ train@16768  Loss: 0.002804 Acc: 13.3125\n",
      " |~~ train@16832  Loss: 0.002978 Acc: 13.2656\n",
      " |~~ train@16896  Loss: 0.002910 Acc: 13.2031\n",
      " |~~ train@16960  Loss: 0.002922 Acc: 13.2656\n",
      " |~~ train@17024  Loss: 0.002966 Acc: 13.3125\n",
      " |~~ train@17088  Loss: 0.003073 Acc: 13.2188\n",
      " |~~ train@17152  Loss: 0.002848 Acc: 13.2344\n",
      " |~~ train@17216  Loss: 0.002649 Acc: 13.3125\n",
      " |~~ train@17280  Loss: 0.002950 Acc: 13.2656\n",
      " |~~ train@17344  Loss: 0.002789 Acc: 13.3438\n",
      " |~~ train@17408  Loss: 0.002596 Acc: 13.3594\n",
      " |~~ train@17472  Loss: 0.002643 Acc: 13.4062\n",
      " |~~ train@17536  Loss: 0.003014 Acc: 13.2031\n",
      " |~~ train@17600  Loss: 0.002849 Acc: 13.3281\n",
      " |~~ train@17664  Loss: 0.002805 Acc: 13.3125\n",
      " |~~ train@17728  Loss: 0.002363 Acc: 13.4688\n",
      " |~~ train@17792  Loss: 0.002524 Acc: 13.4062\n",
      " |~~ train@17856  Loss: 0.002741 Acc: 13.3438\n",
      " |~~ train@17920  Loss: 0.002969 Acc: 13.2188\n",
      " |~~ train@17984  Loss: 0.003002 Acc: 13.2344\n",
      " |~~ train@18048  Loss: 0.002465 Acc: 13.4219\n",
      " |~~ train@18112  Loss: 0.002954 Acc: 13.2188\n",
      " |~~ train@18176  Loss: 0.002515 Acc: 13.4062\n",
      " |~~ train@18240  Loss: 0.003162 Acc: 13.1875\n",
      " |~~ train@18304  Loss: 0.003452 Acc: 13.0312\n",
      " |~~ train@18368  Loss: 0.003093 Acc: 13.2031\n",
      " |~~ train@18432  Loss: 0.002442 Acc: 13.4688\n",
      " |~~ train@18496  Loss: 0.002790 Acc: 13.3125\n",
      " |~~ train@18560  Loss: 0.003327 Acc: 13.1719\n",
      " |~~ train@18624  Loss: 0.003552 Acc: 13.0156\n",
      " |~~ train@18688  Loss: 0.002365 Acc: 13.4531\n",
      " |~~ train@18752  Loss: 0.003081 Acc: 13.2031\n",
      " |~~ train@18816  Loss: 0.003020 Acc: 13.2344\n",
      " |~~ train@18880  Loss: 0.003087 Acc: 13.2031\n",
      " |~~ train@18944  Loss: 0.002595 Acc: 13.3594\n",
      " |~~ train@19008  Loss: 0.002882 Acc: 13.2500\n",
      " |~~ train@19072  Loss: 0.002948 Acc: 13.2812\n",
      " |~~ train@19136  Loss: 0.002094 Acc: 13.5781\n",
      " |~~ train@19200  Loss: 0.003042 Acc: 13.2031\n",
      " |~~ train@19264  Loss: 0.003095 Acc: 13.2188\n",
      " |~~ train@19328  Loss: 0.002880 Acc: 13.3125\n",
      " |~~ train@19392  Loss: 0.002600 Acc: 13.4062\n",
      " |~~ train@19456  Loss: 0.002660 Acc: 13.3906\n",
      " |~~ train@19520  Loss: 0.002594 Acc: 13.3750\n",
      " |~~ train@19584  Loss: 0.003124 Acc: 13.1250\n",
      " |~~ train@19648  Loss: 0.002913 Acc: 13.2344\n",
      " |~~ train@19712  Loss: 0.002960 Acc: 13.2344\n",
      " |~~ train@19776  Loss: 0.002956 Acc: 13.2031\n",
      " |~~ train@19840  Loss: 0.003010 Acc: 13.2344\n",
      " |~~ train@19904  Loss: 0.002843 Acc: 13.2812\n",
      " |~~ train@19968  Loss: 0.002743 Acc: 13.2969\n",
      " |~~ train@20032  Loss: 0.003550 Acc: 13.0000\n",
      " |~~ train@20096  Loss: 0.003278 Acc: 13.1406\n",
      " |~~ train@20160  Loss: 0.002969 Acc: 13.2812\n",
      " |~~ train@20224  Loss: 0.002362 Acc: 13.4688\n",
      " |~~ train@20288  Loss: 0.002638 Acc: 13.3594\n",
      " |~~ train@20352  Loss: 0.003253 Acc: 13.1562\n",
      " |~~ train@20416  Loss: 0.002682 Acc: 13.2812\n",
      " |~~ train@20480  Loss: 0.003131 Acc: 13.1562\n",
      " |~~ train@20544  Loss: 0.003214 Acc: 13.2188\n",
      " |~~ train@20608  Loss: 0.002923 Acc: 13.3125\n",
      " |~~ train@20672  Loss: 0.002834 Acc: 13.3125\n",
      " |~~ train@20736  Loss: 0.002687 Acc: 13.3438\n",
      " |~~ train@20800  Loss: 0.002689 Acc: 13.3750\n",
      " |~~ train@20864  Loss: 0.002841 Acc: 13.2500\n",
      " |~~ train@20928  Loss: 0.002443 Acc: 13.4688\n",
      " |~~ train@20992  Loss: 0.002614 Acc: 13.3594\n",
      " |~~ train@21056  Loss: 0.002509 Acc: 13.3906\n",
      " |~~ train@21120  Loss: 0.003078 Acc: 13.2031\n",
      " |~~ train@21184  Loss: 0.002683 Acc: 13.3125\n",
      " |~~ train@21248  Loss: 0.002716 Acc: 13.3281\n",
      " |~~ train@21312  Loss: 0.003073 Acc: 13.2344\n",
      " |~~ train@21376  Loss: 0.003213 Acc: 13.0938\n",
      " |~~ train@21440  Loss: 0.002334 Acc: 13.4531\n",
      " |~~ train@21504  Loss: 0.002444 Acc: 13.4219\n",
      " |~~ train@21568  Loss: 0.002816 Acc: 13.2812\n",
      " |~~ train@21632  Loss: 0.002804 Acc: 13.2500\n",
      " |~~ train@21696  Loss: 0.002705 Acc: 13.3906\n",
      " |~~ train@21760  Loss: 0.002520 Acc: 13.3594\n",
      " |~~ train@21824  Loss: 0.002806 Acc: 13.3281\n",
      " |~~ train@21888  Loss: 0.002789 Acc: 13.3438\n",
      " |~~ train@21952  Loss: 0.002634 Acc: 13.3750\n",
      " |~~ train@22016  Loss: 0.002795 Acc: 13.2969\n",
      " |~~ train@22080  Loss: 0.002593 Acc: 13.3438\n",
      " |~~ train@22144  Loss: 0.003030 Acc: 13.2188\n",
      " |~~ train@22208  Loss: 0.002660 Acc: 13.3906\n",
      " |~~ train@22272  Loss: 0.003084 Acc: 13.1875\n",
      " |~~ train@22336  Loss: 0.003133 Acc: 13.1719\n",
      " |~~ train@22400  Loss: 0.003003 Acc: 13.2188\n",
      " |~~ train@22464  Loss: 0.002995 Acc: 13.2500\n",
      " |~~ train@22528  Loss: 0.002587 Acc: 13.4062\n",
      " |~~ train@22592  Loss: 0.002846 Acc: 13.2969\n",
      " |~~ train@22656  Loss: 0.002494 Acc: 13.4062\n",
      " |~~ train@22720  Loss: 0.003714 Acc: 12.9531\n",
      " |~~ train@22784  Loss: 0.003294 Acc: 13.1094\n",
      " |~~ train@22848  Loss: 0.002985 Acc: 13.2031\n",
      " |~~ train@22912  Loss: 0.003221 Acc: 13.1406\n",
      " |~~ train@22976  Loss: 0.002460 Acc: 13.4062\n",
      " |~~ train@23040  Loss: 0.003174 Acc: 13.1875\n",
      " |~~ train@23104  Loss: 0.002903 Acc: 13.2031\n",
      " |~~ train@23168  Loss: 0.002264 Acc: 13.5156\n",
      " |~~ train@23232  Loss: 0.003005 Acc: 13.2500\n",
      " |~~ train@23296  Loss: 0.003111 Acc: 13.1875\n",
      " |~~ train@23360  Loss: 0.002500 Acc: 13.4219\n",
      " |~~ train@23424  Loss: 0.003113 Acc: 13.1719\n",
      " |~~ train@23488  Loss: 0.003058 Acc: 13.2188\n",
      " |~~ train@23552  Loss: 0.002611 Acc: 13.3750\n",
      " |~~ train@23616  Loss: 0.003063 Acc: 13.2031\n",
      " |~~ train@23680  Loss: 0.003172 Acc: 13.2031\n",
      " |~~ train@23744  Loss: 0.002260 Acc: 13.5469\n",
      " |~~ train@23808  Loss: 0.002678 Acc: 13.3594\n",
      " |~~ train@23872  Loss: 0.002540 Acc: 13.3438\n",
      " |~~ train@23936  Loss: 0.002231 Acc: 13.4844\n",
      " |~~ train@24000  Loss: 0.003116 Acc: 13.2031\n",
      " |~~ train@24064  Loss: 0.002881 Acc: 13.1875\n",
      " |~~ train@24128  Loss: 0.002713 Acc: 13.3438\n",
      " |~~ train@24192  Loss: 0.002263 Acc: 13.4688\n",
      " |~~ train@24256  Loss: 0.002868 Acc: 13.2969\n",
      " |~~ train@24320  Loss: 0.002635 Acc: 13.3750\n",
      " |~~ train@24384  Loss: 0.003430 Acc: 13.0469\n",
      " |~~ train@24448  Loss: 0.003274 Acc: 13.1719\n",
      " |~~ train@24512  Loss: 0.002886 Acc: 13.2656\n",
      " |~~ train@24576  Loss: 0.002796 Acc: 13.3281\n",
      " |~~ train@24640  Loss: 0.002723 Acc: 13.2969\n",
      " |~~ train@24704  Loss: 0.002500 Acc: 13.4219\n",
      " |~~ train@24768  Loss: 0.002937 Acc: 13.2344\n",
      " |~~ train@24832  Loss: 0.003053 Acc: 13.2344\n",
      " |~~ train@24896  Loss: 0.002934 Acc: 13.2344\n",
      " |~~ train@24960  Loss: 0.003200 Acc: 13.1562\n",
      " |~~ train@25024  Loss: 0.003262 Acc: 13.1406\n",
      " |~~ train@25088  Loss: 0.002664 Acc: 13.3438\n",
      " |~~ train@25152  Loss: 0.002771 Acc: 13.3438\n",
      " |~~ train@25216  Loss: 0.002973 Acc: 13.2344\n",
      " |~~ train@25280  Loss: 0.002974 Acc: 13.2344\n",
      " |~~ train@25344  Loss: 0.002796 Acc: 13.3438\n",
      " |~~ train@25408  Loss: 0.003244 Acc: 13.2344\n",
      " |~~ train@25472  Loss: 0.002948 Acc: 13.2344\n",
      " |~~ train@25536  Loss: 0.003105 Acc: 13.2344\n",
      " |~~ train@25600  Loss: 0.003124 Acc: 13.2031\n",
      " |~~ train@25664  Loss: 0.003294 Acc: 13.1406\n",
      " |~~ train@25728  Loss: 0.002858 Acc: 13.2500\n",
      " |~~ train@25792  Loss: 0.002943 Acc: 13.3281\n",
      " |~~ train@25856  Loss: 0.002769 Acc: 13.3125\n",
      " |~~ train@25920  Loss: 0.003209 Acc: 13.1250\n",
      " |~~ train@25984  Loss: 0.002144 Acc: 13.5625\n",
      " |~~ train@26048  Loss: 0.002520 Acc: 13.3438\n",
      " |~~ train@26112  Loss: 0.003637 Acc: 13.0625\n",
      " |~~ train@26176  Loss: 0.003173 Acc: 13.1875\n",
      " |~~ train@26240  Loss: 0.003386 Acc: 13.0625\n",
      " |~~ train@26304  Loss: 0.002952 Acc: 13.2344\n",
      " |~~ train@26368  Loss: 0.002365 Acc: 13.4844\n",
      " |~~ train@26432  Loss: 0.002598 Acc: 13.3750\n",
      " |~~ train@26496  Loss: 0.003121 Acc: 13.1875\n",
      " |~~ train@26560  Loss: 0.002642 Acc: 13.3594\n",
      " |~~ train@26624  Loss: 0.003113 Acc: 13.2656\n",
      " |~~ train@26688  Loss: 0.002801 Acc: 13.3281\n",
      " |~~ train@26752  Loss: 0.002903 Acc: 13.2969\n",
      " |~~ train@26816  Loss: 0.003162 Acc: 13.1875\n",
      " |~~ train@26880  Loss: 0.003018 Acc: 13.2188\n",
      " |~~ train@26944  Loss: 0.003278 Acc: 13.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@27008  Loss: 0.002673 Acc: 13.3750\n",
      " |~~ train@27072  Loss: 0.002509 Acc: 13.4062\n",
      " |~~ train@27136  Loss: 0.002820 Acc: 13.3281\n",
      " |~~ train@27200  Loss: 0.002958 Acc: 13.2812\n",
      " |~~ train@27264  Loss: 0.003423 Acc: 13.0469\n",
      " |~~ train@27328  Loss: 0.002790 Acc: 13.2500\n",
      " |~~ train@27392  Loss: 0.002802 Acc: 13.2656\n",
      " |~~ train@27456  Loss: 0.002314 Acc: 13.4375\n",
      " |~~ train@27520  Loss: 0.002994 Acc: 13.2344\n",
      " |~~ train@27584  Loss: 0.002732 Acc: 13.3438\n",
      " |~~ train@27648  Loss: 0.002489 Acc: 13.4375\n",
      " |~~ train@27712  Loss: 0.002370 Acc: 13.4844\n",
      " |~~ train@27776  Loss: 0.003251 Acc: 13.1719\n",
      " |~~ train@27840  Loss: 0.002649 Acc: 13.3906\n",
      " |~~ train@27904  Loss: 0.002660 Acc: 13.3906\n",
      " |~~ train@27968  Loss: 0.002889 Acc: 13.2812\n",
      " |~~ train@28032  Loss: 0.002392 Acc: 13.4375\n",
      " |~~ train@28096  Loss: 0.002201 Acc: 13.5469\n",
      " |~~ train@28160  Loss: 0.002808 Acc: 13.3125\n",
      " |~~ train@28224  Loss: 0.003106 Acc: 13.1719\n",
      " |~~ train@28288  Loss: 0.003700 Acc: 13.0312\n",
      " |~~ train@28352  Loss: 0.002799 Acc: 13.3906\n",
      " |~~ train@28416  Loss: 0.002997 Acc: 13.2344\n",
      " |~~ train@28480  Loss: 0.002949 Acc: 13.2656\n",
      " |~~ train@28544  Loss: 0.002862 Acc: 13.2656\n",
      " |~~ train@28608  Loss: 0.003394 Acc: 13.0469\n",
      " |~~ train@28672  Loss: 0.002888 Acc: 13.3125\n",
      " |~~ train@28736  Loss: 0.002989 Acc: 13.2344\n",
      " |~~ train@28800  Loss: 0.003141 Acc: 13.1562\n",
      " |~~ train@28864  Loss: 0.002877 Acc: 13.2969\n",
      " |~~ train@28928  Loss: 0.002514 Acc: 13.4219\n",
      " |~~ train@28992  Loss: 0.002980 Acc: 13.2500\n",
      " |~~ train@29056  Loss: 0.002599 Acc: 13.3438\n",
      " |~~ train@29120  Loss: 0.003186 Acc: 13.0938\n",
      " |~~ train@29184  Loss: 0.003458 Acc: 13.1562\n",
      " |~~ train@29248  Loss: 0.002516 Acc: 13.3594\n",
      " |~~ train@29312  Loss: 0.003042 Acc: 13.2188\n",
      " |~~ train@29376  Loss: 0.002537 Acc: 13.3906\n",
      " |~~ train@29440  Loss: 0.002987 Acc: 13.2812\n",
      " |~~ train@29504  Loss: 0.002895 Acc: 13.2188\n",
      " |~~ train@29568  Loss: 0.002617 Acc: 13.4219\n",
      " |~~ train@29632  Loss: 0.002737 Acc: 13.3281\n",
      " |~~ train@29696  Loss: 0.002144 Acc: 13.5625\n",
      " |~~ train@29760  Loss: 0.003110 Acc: 13.2188\n",
      " |~~ train@29824  Loss: 0.002413 Acc: 13.4219\n",
      " |~~ train@29888  Loss: 0.002317 Acc: 13.4531\n",
      " |~~ train@29952  Loss: 0.003603 Acc: 13.0469\n",
      " |~~ train@30016  Loss: 0.002863 Acc: 13.2812\n",
      " |~~ train@30080  Loss: 0.002975 Acc: 13.2188\n",
      " |~~ train@30144  Loss: 0.002828 Acc: 13.3125\n",
      " |~~ train@30208  Loss: 0.002143 Acc: 13.5156\n",
      " |~~ train@30272  Loss: 0.002515 Acc: 13.3750\n",
      " |~~ train@30336  Loss: 0.002741 Acc: 13.2969\n",
      " |~~ train@30400  Loss: 0.002635 Acc: 13.3750\n",
      " |~~ train@30464  Loss: 0.003173 Acc: 13.1406\n",
      " |~~ train@30528  Loss: 0.002870 Acc: 13.2031\n",
      " |~~ train@30592  Loss: 0.002577 Acc: 13.3906\n",
      " |~~ train@30656  Loss: 0.002702 Acc: 13.3281\n",
      " |~~ train@30720  Loss: 0.002806 Acc: 13.2812\n",
      " |~~ train@30784  Loss: 0.002606 Acc: 13.3906\n",
      " |~~ train@30848  Loss: 0.002350 Acc: 13.4219\n",
      " |~~ train@30912  Loss: 0.002835 Acc: 13.2500\n",
      " |~~ train@30976  Loss: 0.003148 Acc: 13.1562\n",
      " |~~ train@31040  Loss: 0.002725 Acc: 13.3125\n",
      " |~~ train@31104  Loss: 0.003085 Acc: 13.1562\n",
      " |~~ train@31168  Loss: 0.002975 Acc: 13.2344\n",
      " |~~ train@31232  Loss: 0.003030 Acc: 13.2031\n",
      " |~~ train@31296  Loss: 0.002684 Acc: 13.2812\n",
      " |~~ train@31360  Loss: 0.002907 Acc: 13.2656\n",
      " |~~ train@31424  Loss: 0.003611 Acc: 13.0000\n",
      " |~~ train@31488  Loss: 0.003306 Acc: 13.0625\n",
      " |~~ train@31552  Loss: 0.002799 Acc: 13.2656\n",
      " |~~ train@31616  Loss: 0.002846 Acc: 13.2500\n",
      " |~~ train@31680  Loss: 0.003065 Acc: 13.2188\n",
      " |~~ train@31744  Loss: 0.002689 Acc: 13.3125\n",
      " |~~ train@31808  Loss: 0.002435 Acc: 13.4531\n",
      " |~~ train@31872  Loss: 0.002326 Acc: 13.4531\n",
      " |~~ train@31936  Loss: 0.002988 Acc: 13.2031\n",
      " |~~ train@32000  Loss: 0.003268 Acc: 13.1094\n",
      " |~~ train@32064  Loss: 0.003225 Acc: 13.1719\n",
      " |~~ train@32128  Loss: 0.003138 Acc: 13.2344\n",
      " |~~ train@32192  Loss: 0.002708 Acc: 13.3594\n",
      " |~~ train@32256  Loss: 0.002693 Acc: 13.3438\n",
      " |~~ train@32320  Loss: 0.003372 Acc: 13.1250\n",
      " |~~ train@32384  Loss: 0.002753 Acc: 13.3438\n",
      " |~~ train@32448  Loss: 0.002216 Acc: 13.5312\n",
      " |~~ train@32512  Loss: 0.002579 Acc: 13.3906\n",
      " |~~ train@32576  Loss: 0.002694 Acc: 13.3594\n",
      " |~~ train@32640  Loss: 0.002686 Acc: 13.3750\n",
      " |~~ train@32704  Loss: 0.002581 Acc: 13.3906\n",
      " |~~ train@32768  Loss: 0.003587 Acc: 13.0781\n",
      " |~~ train@32832  Loss: 0.002865 Acc: 13.2656\n",
      " |~~ train@32896  Loss: 0.002552 Acc: 13.4219\n",
      " |~~ train@32960  Loss: 0.003358 Acc: 13.1094\n",
      " |~~ train@33024  Loss: 0.002787 Acc: 13.3281\n",
      " |~~ train@33088  Loss: 0.002992 Acc: 13.3125\n",
      " |~~ train@33152  Loss: 0.002945 Acc: 13.2344\n",
      " |~~ train@33216  Loss: 0.002420 Acc: 13.4531\n",
      " |~~ train@33280  Loss: 0.002773 Acc: 13.3594\n",
      " |~~ train@33344  Loss: 0.002504 Acc: 13.4219\n",
      " |~~ train@33408  Loss: 0.002982 Acc: 13.2188\n",
      " |~~ train@33472  Loss: 0.002993 Acc: 13.2031\n",
      " |~~ train@33536  Loss: 0.003053 Acc: 13.2031\n",
      " |~~ train@33600  Loss: 0.002916 Acc: 13.2344\n",
      " |~~ train@33664  Loss: 0.003049 Acc: 13.2188\n",
      " |~~ train@33728  Loss: 0.002622 Acc: 13.3281\n",
      " |~~ train@33792  Loss: 0.002837 Acc: 13.2656\n",
      " |~~ train@33856  Loss: 0.002612 Acc: 13.3281\n",
      " |~~ train@33920  Loss: 0.002804 Acc: 13.2656\n",
      " |~~ train@33984  Loss: 0.002860 Acc: 13.3125\n",
      " |~~ train@34048  Loss: 0.002627 Acc: 13.3750\n",
      " |~~ train@34112  Loss: 0.002662 Acc: 13.3750\n",
      " |~~ train@34176  Loss: 0.002791 Acc: 13.3281\n",
      " |~~ train@34240  Loss: 0.003280 Acc: 13.0938\n",
      " |~~ train@34304  Loss: 0.002741 Acc: 13.3281\n",
      " |~~ train@34368  Loss: 0.002536 Acc: 13.3594\n",
      " |~~ train@34432  Loss: 0.003180 Acc: 13.1562\n",
      " |~~ train@34496  Loss: 0.003169 Acc: 13.1719\n",
      " |~~ train@34560  Loss: 0.002978 Acc: 13.2344\n",
      " |~~ train@34624  Loss: 0.002417 Acc: 13.3750\n",
      " |~~ train@34688  Loss: 0.002534 Acc: 13.4062\n",
      " |~~ train@34752  Loss: 0.003296 Acc: 13.0938\n",
      " |~~ train@34816  Loss: 0.002811 Acc: 13.3281\n",
      " |~~ train@34880  Loss: 0.003391 Acc: 13.1406\n",
      " |~~ train@34944  Loss: 0.002369 Acc: 13.4219\n",
      " |~~ train@35008  Loss: 0.002991 Acc: 13.2031\n",
      " |~~ train@35072  Loss: 0.002588 Acc: 13.3594\n",
      " |~~ train@35136  Loss: 0.002601 Acc: 13.3594\n",
      " |~~ train@35200  Loss: 0.001864 Acc: 13.6094\n",
      " |~~ train@35264  Loss: 0.003165 Acc: 13.1562\n",
      " |~~ train@35328  Loss: 0.002514 Acc: 13.4219\n",
      " |~~ train@35392  Loss: 0.002815 Acc: 13.2656\n",
      " |~~ train@35456  Loss: 0.003425 Acc: 13.0312\n",
      " |~~ train@35520  Loss: 0.002894 Acc: 13.2812\n",
      " |~~ train@35584  Loss: 0.003143 Acc: 13.1719\n",
      " |~~ train@35648  Loss: 0.003324 Acc: 13.1094\n",
      " |~~ train@35712  Loss: 0.002951 Acc: 13.2656\n",
      " |~~ train@35776  Loss: 0.002617 Acc: 13.3438\n",
      " |~~ train@35840  Loss: 0.002797 Acc: 13.2969\n",
      " |~~ train@35904  Loss: 0.002867 Acc: 13.2656\n",
      " |~~ train@35968  Loss: 0.002483 Acc: 13.4531\n",
      " |~~ train@36032  Loss: 0.002728 Acc: 13.2969\n",
      " |~~ train@36096  Loss: 0.003179 Acc: 13.2188\n",
      " |~~ train@36160  Loss: 0.003492 Acc: 13.0625\n",
      " |~~ train@36224  Loss: 0.003351 Acc: 13.0625\n",
      " |~~ train@36288  Loss: 0.002607 Acc: 13.3594\n",
      " |~~ train@36352  Loss: 0.002551 Acc: 13.3750\n",
      " |~~ train@36416  Loss: 0.002362 Acc: 13.4531\n",
      " |~~ train@36480  Loss: 0.001972 Acc: 13.5625\n",
      " |~~ train@36544  Loss: 0.003110 Acc: 13.1875\n",
      " |~~ train@36608  Loss: 0.002821 Acc: 13.3125\n",
      " |~~ train@36672  Loss: 0.003256 Acc: 13.1250\n",
      " |~~ train@36736  Loss: 0.003302 Acc: 13.0781\n",
      " |~~ train@36800  Loss: 0.002264 Acc: 13.4688\n",
      " |~~ train@36864  Loss: 0.003204 Acc: 13.1562\n",
      " |~~ train@36928  Loss: 0.002600 Acc: 13.3594\n",
      " |~~ train@36992  Loss: 0.003060 Acc: 13.2344\n",
      " |~~ train@37056  Loss: 0.002618 Acc: 13.3125\n",
      " |~~ train@37120  Loss: 0.002333 Acc: 13.4844\n",
      " |~~ train@37184  Loss: 0.002551 Acc: 13.4219\n",
      " |~~ train@37248  Loss: 0.003324 Acc: 13.1562\n",
      " |~~ train@37312  Loss: 0.002767 Acc: 13.2500\n",
      " |~~ train@37376  Loss: 0.003183 Acc: 13.1250\n",
      " |~~ train@37440  Loss: 0.003301 Acc: 13.1094\n",
      " |~~ train@37504  Loss: 0.003187 Acc: 13.1094\n",
      " |~~ train@37568  Loss: 0.003078 Acc: 13.2344\n",
      " |~~ train@37632  Loss: 0.002713 Acc: 13.3281\n",
      " |~~ train@37696  Loss: 0.002572 Acc: 13.3281\n",
      " |~~ train@37760  Loss: 0.002748 Acc: 13.3750\n",
      " |~~ train@37824  Loss: 0.003385 Acc: 13.0781\n",
      " |~~ train@37888  Loss: 0.002419 Acc: 13.4062\n",
      " |~~ train@37952  Loss: 0.002651 Acc: 13.3281\n",
      " |~~ train@38016  Loss: 0.002229 Acc: 13.4844\n",
      " |~~ train@38080  Loss: 0.003038 Acc: 13.1875\n",
      " |~~ train@38144  Loss: 0.002596 Acc: 13.4219\n",
      " |~~ train@38208  Loss: 0.003378 Acc: 13.0938\n",
      " |~~ train@38272  Loss: 0.002320 Acc: 13.5156\n",
      " |~~ train@38336  Loss: 0.002789 Acc: 13.2969\n",
      " |~~ train@38400  Loss: 0.002884 Acc: 13.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@38464  Loss: 0.002908 Acc: 13.2656\n",
      " |~~ train@38528  Loss: 0.003267 Acc: 13.0938\n",
      " |~~ train@38592  Loss: 0.002651 Acc: 13.3594\n",
      " |~~ train@38656  Loss: 0.003272 Acc: 13.1406\n",
      " |~~ train@38720  Loss: 0.003220 Acc: 13.1875\n",
      " |~~ train@38784  Loss: 0.002486 Acc: 13.4219\n",
      " |~~ train@38848  Loss: 0.002565 Acc: 13.3750\n",
      " |~~ train@38912  Loss: 0.002893 Acc: 13.2812\n",
      " |~~ train@38976  Loss: 0.002639 Acc: 13.3438\n",
      " |~~ train@39040  Loss: 0.003253 Acc: 13.1562\n",
      " |~~ train@39104  Loss: 0.002802 Acc: 13.3281\n",
      " |~~ train@39168  Loss: 0.003297 Acc: 13.0312\n",
      " |~~ train@39232  Loss: 0.003449 Acc: 13.0781\n",
      " |~~ train@39296  Loss: 0.002760 Acc: 13.3281\n",
      " |~~ train@39360  Loss: 0.002570 Acc: 13.3594\n",
      " |~~ train@39424  Loss: 0.002593 Acc: 13.3750\n",
      " |~~ train@39488  Loss: 0.002827 Acc: 13.2656\n",
      " |~~ train@39552  Loss: 0.003229 Acc: 13.1719\n",
      " |~~ train@39616  Loss: 0.002912 Acc: 13.2812\n",
      " |~~ train@39680  Loss: 0.002802 Acc: 13.2969\n",
      " |~~ train@39744  Loss: 0.002681 Acc: 13.3281\n",
      " |~~ train@39808  Loss: 0.002591 Acc: 13.3594\n",
      " |~~ train@39872  Loss: 0.002383 Acc: 13.4219\n",
      " |~~ train@39936  Loss: 0.002710 Acc: 13.2656\n",
      " |~~ train@40000  Loss: 0.002957 Acc: 13.2500\n",
      " |~~ train@40064  Loss: 0.002727 Acc: 13.2812\n",
      " |~~ train@40128  Loss: 0.002857 Acc: 13.2500\n",
      " |~~ train@40192  Loss: 0.002717 Acc: 13.3594\n",
      " |~~ train@40256  Loss: 0.002783 Acc: 13.2812\n",
      " |~~ train@40320  Loss: 0.002551 Acc: 13.3594\n",
      " |~~ train@40384  Loss: 0.002501 Acc: 13.3906\n",
      " |~~ train@40448  Loss: 0.002886 Acc: 13.2656\n",
      " |~~ train@40512  Loss: 0.002564 Acc: 13.4375\n",
      " |~~ train@40576  Loss: 0.003055 Acc: 13.2031\n",
      " |~~ train@40640  Loss: 0.002860 Acc: 13.3125\n",
      " |~~ train@40704  Loss: 0.003209 Acc: 13.1406\n",
      " |~~ train@40768  Loss: 0.002384 Acc: 13.4375\n",
      " |~~ train@40832  Loss: 0.002739 Acc: 13.3438\n",
      " |~~ train@40896  Loss: 0.002615 Acc: 13.3750\n",
      " |~~ train@40960  Loss: 0.002324 Acc: 13.4688\n",
      " |~~ train@41024  Loss: 0.002607 Acc: 13.4062\n",
      " |~~ train@41088  Loss: 0.003033 Acc: 13.1719\n",
      " |~~ train@41152  Loss: 0.002936 Acc: 13.2656\n",
      " |~~ train@41216  Loss: 0.002680 Acc: 13.3281\n",
      " |~~ train@41280  Loss: 0.003202 Acc: 13.1406\n",
      " |~~ train@41344  Loss: 0.003052 Acc: 13.1875\n",
      " |~~ train@41408  Loss: 0.002496 Acc: 13.3906\n",
      " |~~ train@41472  Loss: 0.002975 Acc: 13.2031\n",
      " |~~ train@41536  Loss: 0.003214 Acc: 13.2031\n",
      " |~~ train@41600  Loss: 0.003219 Acc: 13.1562\n",
      " |~~ train@41664  Loss: 0.002651 Acc: 13.3281\n",
      " |~~ train@41728  Loss: 0.002874 Acc: 13.2344\n",
      " |~~ train@41792  Loss: 0.003016 Acc: 13.2031\n",
      " |~~ train@41856  Loss: 0.002740 Acc: 13.3438\n",
      " |~~ train@41920  Loss: 0.003075 Acc: 13.1719\n",
      " |~~ train@41984  Loss: 0.002770 Acc: 13.2656\n",
      " |~~ train@42048  Loss: 0.002979 Acc: 13.2656\n",
      " |~~ train@42112  Loss: 0.002331 Acc: 13.4531\n",
      " |~~ train@42176  Loss: 0.002628 Acc: 13.3594\n",
      " |~~ train@42240  Loss: 0.002688 Acc: 13.3438\n",
      " |~~ train@42304  Loss: 0.003342 Acc: 13.1250\n",
      " |~~ train@42368  Loss: 0.002361 Acc: 13.4531\n",
      " |~~ train@42432  Loss: 0.002967 Acc: 13.2031\n",
      " |~~ train@42496  Loss: 0.002247 Acc: 13.4531\n",
      " |~~ train@42560  Loss: 0.002893 Acc: 13.2969\n",
      " |~~ train@42624  Loss: 0.003124 Acc: 13.2344\n",
      " |~~ train@42688  Loss: 0.003703 Acc: 13.0156\n",
      " |~~ train@42752  Loss: 0.003026 Acc: 13.1875\n",
      " |~~ train@42816  Loss: 0.002445 Acc: 13.4531\n",
      " |~~ train@42880  Loss: 0.002508 Acc: 13.4219\n",
      " |~~ train@42944  Loss: 0.002678 Acc: 13.2812\n",
      " |~~ train@43008  Loss: 0.003168 Acc: 13.1719\n",
      " |~~ train@43072  Loss: 0.002754 Acc: 13.2969\n",
      " |~~ train@43136  Loss: 0.002759 Acc: 13.3594\n",
      " |~~ train@43200  Loss: 0.003089 Acc: 13.1406\n",
      " |~~ train@43264  Loss: 0.002724 Acc: 13.3438\n",
      " |~~ train@43328  Loss: 0.002386 Acc: 13.4844\n",
      " |~~ train@43392  Loss: 0.002836 Acc: 13.2812\n",
      " |~~ train@43456  Loss: 0.003235 Acc: 13.1719\n",
      " |~~ train@43520  Loss: 0.002966 Acc: 13.2656\n",
      " |~~ train@43584  Loss: 0.003136 Acc: 13.1562\n",
      " |~~ train@43648  Loss: 0.002325 Acc: 13.5000\n",
      " |~~ train@43712  Loss: 0.003131 Acc: 13.1562\n",
      " |~~ train@43776  Loss: 0.002495 Acc: 13.4219\n",
      " |~~ train@43840  Loss: 0.002865 Acc: 13.3438\n",
      " |~~ train@43904  Loss: 0.003244 Acc: 13.1875\n",
      " |~~ train@43968  Loss: 0.002931 Acc: 13.2344\n",
      " |~~ train@44032  Loss: 0.002585 Acc: 13.3750\n",
      " |~~ train@44096  Loss: 0.002829 Acc: 13.2812\n",
      " |~~ train@44160  Loss: 0.002603 Acc: 13.3906\n",
      " |~~ train@44224  Loss: 0.003280 Acc: 13.1250\n",
      " |~~ train@44288  Loss: 0.002988 Acc: 13.2188\n",
      " |~~ train@44352  Loss: 0.002830 Acc: 13.2188\n",
      " |~~ train@44416  Loss: 0.003556 Acc: 13.0469\n",
      " |~~ train@44480  Loss: 0.002503 Acc: 13.3906\n",
      " |~~ train@44544  Loss: 0.002949 Acc: 13.2812\n",
      " |~~ train@44608  Loss: 0.003145 Acc: 13.1719\n",
      " |~~ train@44672  Loss: 0.002686 Acc: 13.3281\n",
      " |~~ train@44736  Loss: 0.002451 Acc: 13.3125\n",
      " |~~ train@44800  Loss: 0.003299 Acc: 13.0938\n",
      " |~~ train@44864  Loss: 0.002555 Acc: 13.3750\n",
      " |~~ train@44928  Loss: 0.003672 Acc: 13.0000\n",
      " |~~ train@44992  Loss: 0.003164 Acc: 13.2188\n",
      " |~~ train@45056  Loss: 0.002302 Acc: 13.5312\n",
      " |~~ train@45120  Loss: 0.002869 Acc: 13.2500\n",
      " |~~ train@45184  Loss: 0.002885 Acc: 13.2500\n",
      " |~~ train@45248  Loss: 0.002613 Acc: 13.3125\n",
      " |~~ train@45312  Loss: 0.002748 Acc: 13.3438\n",
      " |~~ train@45376  Loss: 0.002913 Acc: 13.2500\n",
      " |~~ train@45440  Loss: 0.002707 Acc: 13.3438\n",
      " |~~ train@45504  Loss: 0.002532 Acc: 13.3438\n",
      " |~~ train@45568  Loss: 0.002724 Acc: 13.3438\n",
      " |~~ train@45632  Loss: 0.002710 Acc: 13.3438\n",
      " |~~ train@45696  Loss: 0.002850 Acc: 13.3125\n",
      " |~~ train@45760  Loss: 0.002373 Acc: 13.4375\n",
      " |~~ train@45824  Loss: 0.002921 Acc: 13.2500\n",
      " |~~ train@45888  Loss: 0.003028 Acc: 13.2344\n",
      " |~~ train@45952  Loss: 0.002967 Acc: 13.1875\n",
      " |~~ train@46016  Loss: 0.002878 Acc: 13.2656\n",
      " |~~ train@46080  Loss: 0.003074 Acc: 13.2188\n",
      " |~~ train@46144  Loss: 0.003426 Acc: 13.0781\n",
      " |~~ train@46208  Loss: 0.002844 Acc: 13.2656\n",
      " |~~ train@46272  Loss: 0.002964 Acc: 13.2188\n",
      " |~~ train@46336  Loss: 0.003239 Acc: 13.1094\n",
      " |~~ train@46400  Loss: 0.003134 Acc: 13.1875\n",
      " |~~ train@46464  Loss: 0.002498 Acc: 13.3750\n",
      " |~~ train@46528  Loss: 0.002793 Acc: 13.3125\n",
      " |~~ train@46592  Loss: 0.002875 Acc: 13.2969\n",
      " |~~ train@46656  Loss: 0.003109 Acc: 13.1562\n",
      " |~~ train@46720  Loss: 0.002936 Acc: 13.2188\n",
      " |~~ train@46784  Loss: 0.003503 Acc: 13.0312\n",
      " |~~ train@46848  Loss: 0.002544 Acc: 13.3438\n",
      " |~~ train@46912  Loss: 0.002679 Acc: 13.3750\n",
      " |~~ train@46976  Loss: 0.002659 Acc: 13.3594\n",
      " |~~ train@47040  Loss: 0.002603 Acc: 13.3594\n",
      " |~~ train@47104  Loss: 0.003136 Acc: 13.1875\n",
      " |~~ train@47168  Loss: 0.003351 Acc: 13.1250\n",
      " |~~ train@47232  Loss: 0.003024 Acc: 13.1406\n",
      " |~~ train@47296  Loss: 0.002227 Acc: 13.5156\n",
      " |~~ train@47360  Loss: 0.002955 Acc: 13.2344\n",
      " |~~ train@47424  Loss: 0.002335 Acc: 13.4844\n",
      " |~~ train@47488  Loss: 0.002408 Acc: 13.4531\n",
      " |~~ train@47552  Loss: 0.003658 Acc: 13.0156\n",
      " |~~ train@47616  Loss: 0.002811 Acc: 13.3281\n",
      " |~~ train@47680  Loss: 0.002501 Acc: 13.4531\n",
      " |~~ train@47744  Loss: 0.003025 Acc: 13.2188\n",
      " |~~ train@47808  Loss: 0.003149 Acc: 13.1250\n",
      " |~~ train@47872  Loss: 0.002579 Acc: 13.3750\n",
      " |~~ train@47936  Loss: 0.002773 Acc: 13.3281\n",
      " |~~ train@48000  Loss: 0.002875 Acc: 13.2812\n",
      " |~~ train@48064  Loss: 0.003014 Acc: 13.2969\n",
      " |~~ train@48128  Loss: 0.002384 Acc: 13.3906\n",
      " |~~ train@48192  Loss: 0.002834 Acc: 13.2812\n",
      " |~~ train@48256  Loss: 0.002989 Acc: 13.2500\n",
      " |~~ train@48320  Loss: 0.003148 Acc: 13.2500\n",
      " |~~ train@48384  Loss: 0.002846 Acc: 13.3125\n",
      " |~~ train@48448  Loss: 0.002715 Acc: 13.3594\n",
      " |~~ train@48512  Loss: 0.003405 Acc: 13.1250\n",
      " |~~ train@48576  Loss: 0.002462 Acc: 13.4062\n",
      " |~~ train@48640  Loss: 0.003119 Acc: 13.2031\n",
      " |~~ train@48704  Loss: 0.002751 Acc: 13.3281\n",
      " |~~ train@48768  Loss: 0.003091 Acc: 13.1875\n",
      " |~~ train@48832  Loss: 0.003514 Acc: 13.0625\n",
      " |~~ train@48896  Loss: 0.003040 Acc: 13.2500\n",
      " |~~ train@48960  Loss: 0.002661 Acc: 13.3438\n",
      " |~~ train@49024  Loss: 0.003300 Acc: 13.2031\n",
      " |~~ train@49088  Loss: 0.003209 Acc: 13.1875\n",
      " |~~ train@49152  Loss: 0.002642 Acc: 13.3281\n",
      " |~~ train@49216  Loss: 0.002977 Acc: 13.2344\n",
      " |~~ train@49280  Loss: 0.003098 Acc: 13.2031\n",
      " |~~ train@49344  Loss: 0.003772 Acc: 12.9531\n",
      " |~~ train@49408  Loss: 0.002892 Acc: 13.3125\n",
      " |~~ train@49472  Loss: 0.002529 Acc: 13.3906\n",
      " |~~ train@49536  Loss: 0.002923 Acc: 13.1875\n",
      " |~~ train@49600  Loss: 0.003264 Acc: 13.1406\n",
      " |~~ train@49664  Loss: 0.002851 Acc: 13.2344\n",
      " |~~ train@49728  Loss: 0.002731 Acc: 13.2969\n",
      " |~~ train@49792  Loss: 0.003259 Acc: 13.1406\n",
      " |~~ train@49856  Loss: 0.002876 Acc: 13.2969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@49920  Loss: 0.002817 Acc: 13.2812\n",
      " |~~ train@49984  Loss: 0.003035 Acc: 13.2188\n",
      " |~~ train@50048  Loss: 0.002498 Acc: 13.3750\n",
      " |~~ train@50112  Loss: 0.002754 Acc: 13.3125\n",
      " |~~ train@50176  Loss: 0.002580 Acc: 13.3594\n",
      " |~~ train@50240  Loss: 0.002853 Acc: 13.2344\n",
      " |~~ train@50304  Loss: 0.002640 Acc: 13.3750\n",
      " |~~ train@50368  Loss: 0.003067 Acc: 13.2812\n",
      " |~~ train@50432  Loss: 0.002505 Acc: 13.4219\n",
      " |~~ train@50496  Loss: 0.002774 Acc: 13.3125\n",
      " |~~ train@50560  Loss: 0.003224 Acc: 13.1562\n",
      " |~~ train@50624  Loss: 0.002733 Acc: 13.3125\n",
      " |~~ train@50688  Loss: 0.003048 Acc: 13.2344\n",
      " |~~ train@50752  Loss: 0.003586 Acc: 13.0938\n",
      " |~~ train@50816  Loss: 0.003269 Acc: 13.1875\n",
      " |~~ train@50880  Loss: 0.002541 Acc: 13.3438\n",
      " |~~ train@50944  Loss: 0.002862 Acc: 13.2656\n",
      " |~~ train@51008  Loss: 0.003367 Acc: 13.1250\n",
      " |~~ train@51072  Loss: 0.002932 Acc: 13.3281\n",
      " |~~ train@51136  Loss: 0.002887 Acc: 13.2188\n",
      " |~~ train@51200  Loss: 0.002451 Acc: 13.4531\n",
      " |~~ train@51264  Loss: 0.002982 Acc: 13.2812\n",
      " |~~ train@51328  Loss: 0.002462 Acc: 13.4062\n",
      " |~~ train@51392  Loss: 0.003150 Acc: 13.1406\n",
      " |~~ train@51456  Loss: 0.002853 Acc: 13.2500\n",
      " |~~ train@51520  Loss: 0.002587 Acc: 13.3594\n",
      " |~~ train@51584  Loss: 0.002955 Acc: 13.2812\n",
      " |~~ train@51648  Loss: 0.002744 Acc: 13.3750\n",
      " |~~ train@51712  Loss: 0.003098 Acc: 13.1719\n",
      " |~~ train@51776  Loss: 0.003253 Acc: 13.1250\n",
      " |~~ train@51840  Loss: 0.002677 Acc: 13.3438\n",
      " |~~ train@51904  Loss: 0.002183 Acc: 13.5156\n",
      " |~~ train@51968  Loss: 0.003453 Acc: 13.0625\n",
      " |~~ train@52032  Loss: 0.002575 Acc: 13.3438\n",
      " |~~ train@52096  Loss: 0.002642 Acc: 13.3125\n",
      " |~~ train@52160  Loss: 0.002969 Acc: 13.3125\n",
      " |~~ train@52224  Loss: 0.002749 Acc: 13.3281\n",
      " |~~ train@52288  Loss: 0.002969 Acc: 13.1719\n",
      " |~~ train@52352  Loss: 0.002932 Acc: 13.2500\n",
      " |~~ train@52416  Loss: 0.002744 Acc: 13.3594\n",
      " |~~ train@52480  Loss: 0.002657 Acc: 13.2812\n",
      " |~~ train@52544  Loss: 0.002718 Acc: 13.3125\n",
      " |~~ train@52608  Loss: 0.003475 Acc: 13.0625\n",
      " |~~ train@52672  Loss: 0.002490 Acc: 13.3906\n",
      " |~~ train@52736  Loss: 0.003102 Acc: 13.1875\n",
      " |~~ train@52800  Loss: 0.002857 Acc: 13.2031\n",
      " |~~ train@52864  Loss: 0.002256 Acc: 13.4688\n",
      " |~~ train@52928  Loss: 0.002388 Acc: 13.4531\n",
      " |~~ train@52992  Loss: 0.002658 Acc: 13.3750\n",
      " |~~ train@53056  Loss: 0.002488 Acc: 13.3750\n",
      " |~~ train@53120  Loss: 0.003015 Acc: 13.2344\n",
      " |~~ train@53184  Loss: 0.002808 Acc: 13.2969\n",
      " |~~ train@53248  Loss: 0.003716 Acc: 12.9844\n",
      " |~~ train@53312  Loss: 0.002635 Acc: 13.3438\n",
      " |~~ train@53376  Loss: 0.003483 Acc: 13.1094\n",
      " |~~ train@53440  Loss: 0.002882 Acc: 13.2812\n",
      " |~~ train@53504  Loss: 0.002932 Acc: 13.2500\n",
      " |~~ train@53568  Loss: 0.003003 Acc: 13.2188\n",
      " |~~ train@53632  Loss: 0.002449 Acc: 13.3906\n",
      " |~~ train@53696  Loss: 0.002976 Acc: 13.2656\n",
      " |~~ train@53760  Loss: 0.002677 Acc: 13.3594\n",
      " |~~ train@53824  Loss: 0.003587 Acc: 13.0000\n",
      " |~~ train@53888  Loss: 0.002806 Acc: 13.3281\n",
      " |~~ train@53952  Loss: 0.003015 Acc: 13.1562\n",
      " |~~ train@54016  Loss: 0.002560 Acc: 13.3594\n",
      " |~~ train@54080  Loss: 0.002612 Acc: 13.4062\n",
      " |~~ train@54144  Loss: 0.002645 Acc: 13.3438\n",
      " |~~ train@54208  Loss: 0.003335 Acc: 13.1250\n",
      " |~~ train@54272  Loss: 0.002493 Acc: 13.4062\n",
      " |~~ train@54336  Loss: 0.002711 Acc: 13.2812\n",
      " |~~ train@54400  Loss: 0.002489 Acc: 13.3906\n",
      " |~~ train@54464  Loss: 0.002949 Acc: 13.2500\n",
      " |~~ train@54528  Loss: 0.002768 Acc: 13.3438\n",
      " |~~ train@54592  Loss: 0.002837 Acc: 13.2656\n",
      " |~~ train@54656  Loss: 0.002499 Acc: 13.3438\n",
      " |~~ train@54720  Loss: 0.002913 Acc: 13.3281\n",
      " |~~ train@54784  Loss: 0.002386 Acc: 13.4688\n",
      " |~~ train@54848  Loss: 0.002810 Acc: 13.2500\n",
      " |~~ train@54912  Loss: 0.003086 Acc: 13.2031\n",
      " |~~ train@54976  Loss: 0.003344 Acc: 13.0625\n",
      " |~~ train@55040  Loss: 0.002918 Acc: 13.2344\n",
      " |~~ train@55104  Loss: 0.003074 Acc: 13.2500\n",
      " |~~ train@55168  Loss: 0.003695 Acc: 12.9688\n",
      " |~~ train@55232  Loss: 0.003242 Acc: 13.1250\n",
      " |~~ train@55296  Loss: 0.003110 Acc: 13.1875\n",
      " |~~ train@55360  Loss: 0.002967 Acc: 13.2188\n",
      " |~~ train@55424  Loss: 0.002452 Acc: 13.3906\n",
      " |~~ train@55488  Loss: 0.003096 Acc: 13.2344\n",
      " |~~ train@55552  Loss: 0.002946 Acc: 13.2188\n",
      " |~~ train@55616  Loss: 0.002904 Acc: 13.2344\n",
      " |~~ train@55680  Loss: 0.003258 Acc: 13.1406\n",
      " |~~ train@55744  Loss: 0.002899 Acc: 13.2812\n",
      " |~~ train@55808  Loss: 0.002905 Acc: 13.2812\n",
      " |~~ train@55872  Loss: 0.002875 Acc: 13.2188\n",
      " |~~ train@55936  Loss: 0.003109 Acc: 13.1562\n",
      " |~~ train@56000  Loss: 0.002532 Acc: 13.4375\n",
      " |~~ train@56064  Loss: 0.003447 Acc: 13.0781\n",
      " |~~ train@56128  Loss: 0.002429 Acc: 13.4219\n",
      " |~~ train@56192  Loss: 0.003434 Acc: 13.0000\n",
      " |~~ train@56256  Loss: 0.002893 Acc: 13.2344\n",
      " |~~ train@56320  Loss: 0.003156 Acc: 13.1406\n",
      " |~~ train@56384  Loss: 0.002304 Acc: 13.4375\n",
      " |~~ train@56448  Loss: 0.002581 Acc: 13.3438\n",
      " |~~ train@56512  Loss: 0.003159 Acc: 13.1719\n",
      " |~~ train@56576  Loss: 0.003131 Acc: 13.1719\n",
      " |~~ train@56640  Loss: 0.003299 Acc: 13.1562\n",
      " |~~ train@56704  Loss: 0.003042 Acc: 13.1875\n",
      " |~~ train@56768  Loss: 0.003442 Acc: 13.0938\n",
      " |~~ train@56832  Loss: 0.002725 Acc: 13.3125\n",
      " |~~ train@56896  Loss: 0.002881 Acc: 13.2969\n",
      " |~~ train@56960  Loss: 0.003786 Acc: 12.9531\n",
      " |~~ train@57024  Loss: 0.002719 Acc: 13.3750\n",
      " |~~ train@57088  Loss: 0.002946 Acc: 13.2969\n",
      " |~~ train@57152  Loss: 0.002975 Acc: 13.2500\n",
      " |~~ train@57216  Loss: 0.002571 Acc: 13.3750\n",
      " |~~ train@57280  Loss: 0.002954 Acc: 13.2812\n",
      " |~~ train@57344  Loss: 0.002781 Acc: 13.2812\n",
      " |~~ train@57408  Loss: 0.002934 Acc: 13.2188\n",
      " |~~ train@57472  Loss: 0.003189 Acc: 13.1719\n",
      " |~~ train@57536  Loss: 0.002832 Acc: 13.2656\n",
      " |~~ train@57600  Loss: 0.003206 Acc: 13.1406\n",
      " |~~ train@57664  Loss: 0.002788 Acc: 13.3594\n",
      " |~~ train@57728  Loss: 0.002467 Acc: 13.4688\n",
      " |~~ train@57792  Loss: 0.003282 Acc: 13.0625\n",
      " |~~ train@57856  Loss: 0.003387 Acc: 13.1250\n",
      " |~~ train@57920  Loss: 0.002656 Acc: 13.3906\n",
      " |~~ train@57984  Loss: 0.002369 Acc: 13.4688\n",
      " |~~ train@58048  Loss: 0.002959 Acc: 13.2188\n",
      " |~~ train@58112  Loss: 0.002612 Acc: 13.4062\n",
      " |~~ train@58176  Loss: 0.002982 Acc: 13.2344\n",
      " |~~ train@58240  Loss: 0.003870 Acc: 12.9844\n",
      " |~~ train@58304  Loss: 0.003608 Acc: 13.0000\n",
      " |~~ train@58368  Loss: 0.002518 Acc: 13.3281\n",
      " |~~ train@58432  Loss: 0.003284 Acc: 13.1094\n",
      " |~~ train@58496  Loss: 0.002941 Acc: 13.2031\n",
      " |~~ train@58560  Loss: 0.002348 Acc: 13.4531\n",
      " |~~ train@58624  Loss: 0.003048 Acc: 13.2188\n",
      " |~~ train@58688  Loss: 0.003123 Acc: 13.1562\n",
      " |~~ train@58752  Loss: 0.002544 Acc: 13.3750\n",
      " |~~ train@58816  Loss: 0.002787 Acc: 13.3125\n",
      " |~~ train@58880  Loss: 0.003158 Acc: 13.1875\n",
      " |~~ train@58944  Loss: 0.002929 Acc: 13.2812\n",
      " |~~ train@59008  Loss: 0.003138 Acc: 13.1406\n",
      " |~~ train@59072  Loss: 0.002385 Acc: 13.4375\n",
      " |~~ train@59136  Loss: 0.003011 Acc: 13.2031\n",
      " |~~ train@59200  Loss: 0.002964 Acc: 13.2969\n",
      " |~~ train@59264  Loss: 0.003547 Acc: 13.0000\n",
      " |~~ train@59328  Loss: 0.002692 Acc: 13.3594\n",
      " |~~ train@59392  Loss: 0.003357 Acc: 13.1406\n",
      " |~~ train@59456  Loss: 0.002631 Acc: 13.3594\n",
      " |~~ train@59520  Loss: 0.003091 Acc: 13.2344\n",
      " |~~ train@59584  Loss: 0.003107 Acc: 13.2031\n",
      " |~~ train@59648  Loss: 0.002156 Acc: 13.4844\n",
      " |~~ train@59712  Loss: 0.002676 Acc: 13.3438\n",
      " |~~ train@59776  Loss: 0.002821 Acc: 13.2656\n",
      " |~~ train@59840  Loss: 0.002283 Acc: 13.4531\n",
      " |~~ train@59904  Loss: 0.002674 Acc: 13.3594\n",
      " |~~ train@59968  Loss: 0.002948 Acc: 13.2656\n",
      " |~~ train@60032  Loss: 0.003456 Acc: 13.0000\n",
      " |~~ train@60096  Loss: 0.004067 Acc: 12.8438\n",
      " |~~ train@60160  Loss: 0.002787 Acc: 13.3125\n",
      " |~~ train@60224  Loss: 0.002321 Acc: 13.4844\n",
      " |~~ train@60288  Loss: 0.002803 Acc: 13.2500\n",
      " |~~ train@60352  Loss: 0.003131 Acc: 13.1562\n",
      " |~~ train@60416  Loss: 0.003048 Acc: 13.2344\n",
      " |~~ train@60480  Loss: 0.002766 Acc: 13.3281\n",
      " |~~ train@60544  Loss: 0.002835 Acc: 13.2656\n",
      " |~~ train@60608  Loss: 0.003304 Acc: 13.0625\n",
      " |~~ train@60672  Loss: 0.002050 Acc: 13.5781\n",
      " |~~ train@60736  Loss: 0.002697 Acc: 13.3438\n",
      " |~~ train@60800  Loss: 0.003189 Acc: 13.1406\n",
      " |~~ train@60864  Loss: 0.003303 Acc: 13.0312\n",
      " |~~ train@60928  Loss: 0.003279 Acc: 13.1719\n",
      " |~~ train@60992  Loss: 0.002609 Acc: 13.3750\n",
      " |~~ train@61056  Loss: 0.003128 Acc: 13.2500\n",
      " |~~ train@61120  Loss: 0.002414 Acc: 13.4219\n",
      " |~~ train@61184  Loss: 0.003004 Acc: 13.2188\n",
      " |~~ train@61248  Loss: 0.002934 Acc: 13.2500\n",
      " |~~ train@61312  Loss: 0.002490 Acc: 13.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@61376  Loss: 0.002361 Acc: 13.4062\n",
      " |~~ train@61440  Loss: 0.002967 Acc: 13.2500\n",
      " |~~ train@61504  Loss: 0.003128 Acc: 13.2031\n",
      " |~~ train@61568  Loss: 0.002332 Acc: 13.4375\n",
      " |~~ train@61632  Loss: 0.002895 Acc: 13.2500\n",
      " |~~ train@61696  Loss: 0.003295 Acc: 13.2188\n",
      " |~~ train@61760  Loss: 0.002811 Acc: 13.3281\n",
      " |~~ train@61824  Loss: 0.003263 Acc: 13.1406\n",
      " |~~ train@61888  Loss: 0.003099 Acc: 13.2188\n",
      " |~~ train@61952  Loss: 0.003202 Acc: 13.1719\n",
      " |~~ train@62016  Loss: 0.002087 Acc: 13.5469\n",
      " |~~ train@62080  Loss: 0.002767 Acc: 13.2656\n",
      " |~~ train@62144  Loss: 0.003430 Acc: 13.0938\n",
      " |~~ train@62208  Loss: 0.002973 Acc: 13.2344\n",
      " |~~ train@62272  Loss: 0.002604 Acc: 13.3750\n",
      " |~~ train@62336  Loss: 0.002899 Acc: 13.2812\n",
      " |~~ train@62400  Loss: 0.003317 Acc: 13.1719\n",
      " |~~ train@62464  Loss: 0.002488 Acc: 13.3281\n",
      " |~~ train@62528  Loss: 0.002250 Acc: 13.4688\n",
      " |~~ train@62592  Loss: 0.003040 Acc: 13.2188\n",
      " |~~ train@62656  Loss: 0.002332 Acc: 13.4531\n",
      " |~~ train@62720  Loss: 0.002981 Acc: 13.2188\n",
      " |~~ train@62784  Loss: 0.003676 Acc: 13.0469\n",
      " |~~ train@62848  Loss: 0.003145 Acc: 13.2188\n",
      " |~~ train@62912  Loss: 0.002688 Acc: 13.3438\n",
      " |~~ train@62976  Loss: 0.002600 Acc: 13.3750\n",
      " |~~ train@63040  Loss: 0.002895 Acc: 13.3125\n",
      " |~~ train@63104  Loss: 0.003040 Acc: 13.2344\n",
      " |~~ train@63168  Loss: 0.002500 Acc: 13.4219\n",
      " |~~ train@63232  Loss: 0.002787 Acc: 13.2812\n",
      " |~~ train@63296  Loss: 0.002519 Acc: 13.3594\n",
      " |~~ train@63360  Loss: 0.003152 Acc: 13.2031\n",
      " |~~ train@63424  Loss: 0.002568 Acc: 13.3750\n",
      " |~~ train@63488  Loss: 0.003215 Acc: 13.1719\n",
      " |~~ train@63552  Loss: 0.002929 Acc: 13.2969\n",
      " |~~ train@63616  Loss: 0.002847 Acc: 13.3125\n",
      " |~~ train@63680  Loss: 0.003029 Acc: 13.1562\n",
      " |~~ train@63744  Loss: 0.003048 Acc: 13.2344\n",
      " |~~ train@63808  Loss: 0.003498 Acc: 13.0000\n",
      " |~~ train@63872  Loss: 0.002796 Acc: 13.3125\n",
      " |~~ train@63936  Loss: 0.002621 Acc: 13.3438\n",
      " |~~ train@64000  Loss: 0.002858 Acc: 13.2969\n",
      " |~~ train@64064  Loss: 0.002989 Acc: 13.2500\n",
      " |~~ train@64128  Loss: 0.003074 Acc: 13.1875\n",
      " |~~ train@64192  Loss: 0.003025 Acc: 13.2031\n",
      " |~~ train@64256  Loss: 0.003058 Acc: 13.2031\n",
      " |~~ train@64320  Loss: 0.002980 Acc: 13.2031\n",
      " |~~ train@64384  Loss: 0.002986 Acc: 13.2812\n",
      " |~~ train@64448  Loss: 0.002920 Acc: 13.2812\n",
      " |~~ train@64512  Loss: 0.002775 Acc: 13.3594\n",
      " |~~ train@64576  Loss: 0.003097 Acc: 13.2188\n",
      " |~~ train@64640  Loss: 0.002589 Acc: 13.3594\n",
      " |~~ train@64704  Loss: 0.003013 Acc: 13.2344\n",
      " |~~ train@64768  Loss: 0.003184 Acc: 13.2188\n",
      " |~~ train@64832  Loss: 0.002311 Acc: 13.4531\n",
      " |~~ train@64896  Loss: 0.003250 Acc: 13.1719\n",
      " |~~ train@64960  Loss: 0.002859 Acc: 13.2812\n",
      " |~~ train@65024  Loss: 0.001956 Acc: 13.6094\n",
      " |~~ train@65088  Loss: 0.002873 Acc: 13.2656\n",
      " |~~ train@65152  Loss: 0.003071 Acc: 13.2188\n",
      " |~~ train@65216  Loss: 0.002791 Acc: 13.3281\n",
      " |~~ train@65280  Loss: 0.003008 Acc: 13.2031\n",
      " |~~ train@65344  Loss: 0.002463 Acc: 13.4062\n",
      " |~~ train@65408  Loss: 0.002264 Acc: 13.4844\n",
      " |~~ train@65472  Loss: 0.003342 Acc: 13.0781\n",
      " |~~ train@65536  Loss: 0.002785 Acc: 13.2969\n",
      " |~~ train@65600  Loss: 0.002349 Acc: 13.4219\n",
      " |~~ train@65664  Loss: 0.003437 Acc: 13.0938\n",
      " |~~ train@65728  Loss: 0.003026 Acc: 13.2188\n",
      " |~~ train@65792  Loss: 0.003144 Acc: 13.1875\n",
      " |~~ train@65856  Loss: 0.003189 Acc: 13.1250\n",
      " |~~ train@65920  Loss: 0.002713 Acc: 13.3281\n",
      " |~~ train@65984  Loss: 0.002993 Acc: 13.2344\n",
      " |~~ train@66048  Loss: 0.002808 Acc: 13.3125\n",
      " |~~ train@66112  Loss: 0.003041 Acc: 13.1719\n",
      " |~~ train@66176  Loss: 0.002532 Acc: 13.3594\n",
      " |~~ train@66240  Loss: 0.002294 Acc: 13.4531\n",
      " |~~ train@66304  Loss: 0.002994 Acc: 13.1875\n",
      " |~~ train@66368  Loss: 0.002889 Acc: 13.2500\n",
      " |~~ train@66432  Loss: 0.002787 Acc: 13.3125\n",
      " |~~ train@66496  Loss: 0.002803 Acc: 13.2656\n",
      " |~~ train@66560  Loss: 0.003002 Acc: 13.2500\n",
      " |~~ train@66624  Loss: 0.002865 Acc: 13.2812\n",
      " |~~ train@66688  Loss: 0.002626 Acc: 13.3125\n",
      " |~~ train@66752  Loss: 0.002891 Acc: 13.2500\n",
      " |~~ train@66816  Loss: 0.002581 Acc: 13.3438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-166:\n",
      "Process Process-164:\n",
      "Process Process-165:\n",
      "Process Process-162:\n",
      "Process Process-161:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-167:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-169:\n",
      "Traceback (most recent call last):\n",
      "Process Process-170:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-168:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-163:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 43, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 42, in __getitem__\n",
      "    target = self.get_one_hot_labels(fname)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 45, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-4cc34eea5387>\", line 68, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 43, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 75, in get_one_hot_labels\n",
      "    labels = self.get_labels(fname)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-3-4cc34eea5387>\", line 68, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 42, in __getitem__\n",
      "    target = self.get_one_hot_labels(fname)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 57, in get_labels\n",
      "    return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/transforms/transforms.py\", line 147, in __call__\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 43, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 75, in get_one_hot_labels\n",
      "    labels = self.get_labels(fname)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/transforms/functional.py\", line 197, in resize\n",
      "    return img.resize((ow, oh), interpolation)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 57, in get_labels\n",
      "    return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 42, in __getitem__\n",
      "    target = self.get_one_hot_labels(fname)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 42, in __getitem__\n",
      "    target = self.get_one_hot_labels(fname)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 42, in __getitem__\n",
      "    target = self.get_one_hot_labels(fname)\n",
      "  File \"<ipython-input-3-4cc34eea5387>\", line 68, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 75, in get_one_hot_labels\n",
      "    labels = self.get_labels(fname)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 75, in get_one_hot_labels\n",
      "    labels = self.get_labels(fname)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 75, in get_one_hot_labels\n",
      "    labels = self.get_labels(fname)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 57, in get_labels\n",
      "    return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 57, in get_labels\n",
      "    return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 57, in get_labels\n",
      "    return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-05078c52fc86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             num_epochs=8)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-e3e23468b1d7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, outfolder)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# get the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 42, in __getitem__\n",
      "    target = self.get_one_hot_labels(fname)\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 75, in get_one_hot_labels\n",
      "    labels = self.get_labels(fname)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-21cc0ac35a56>\", line 57, in get_labels\n",
      "    return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 398, in values\n",
      "    return self._data.external_values()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4499, in external_values\n",
      "    return self._block.external_values()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 155, in external_values\n",
      "    def external_values(self, dtype=None):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_model(model_ft,\n",
    "            criterion,\n",
    "            optimizer_ft,\n",
    "            exp_lr_scheduler,\n",
    "            num_epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model results to S3\n",
    "\n",
    "aws s3 cp ResNet18PlusFlexibleFC_Epoch9.tar s3://bdh-xrayproj-modelparameters/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.list_buckets()\n",
    "\n",
    "S3 Commands: http://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html\n",
    "\n",
    "Boto3 QuickStart: http://boto3.readthedocs.io/en/latest/guide/quickstart.html\n",
    "\n",
    "Key Management: https://aws.amazon.com/blogs/security/a-safer-way-to-distribute-aws-credentials-to-ec2/\n",
    "\n",
    "AWS IAM Rules: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model results back from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_lr_scheduler.last_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ed2ea89b-99da-4fde-a3ac-91d4d4e3865a"
    }
   },
   "source": [
    "# Analysis of Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sums(model, dataset):\n",
    "    model.train(False)\n",
    "    \n",
    "    obs_counter = 0\n",
    "    total_pred = Variable(torch.FloatTensor(torch.zeros(14)), volatile=True)\n",
    "    total_act = Variable(torch.FloatTensor(torch.zeros(14)), volatile=True)\n",
    "\n",
    "    for data in dataset:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = Variable(inputs.cuda(), volatile=True)\n",
    "        labels = Variable(labels.cuda(), volatile=True)\n",
    "\n",
    "        outputs = model(inputs).sigmoid()\n",
    "\n",
    "        total_act += labels.sum(0).cpu()\n",
    "        total_pred += outputs.sum(0).cpu()\n",
    "\n",
    "        obs_counter += len(inputs)\n",
    "        \n",
    "    return {'pred': total_pred / obs_counter,\n",
    "            'act': total_act / obs_counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sums(model_ft, dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbpresent": {
     "id": "9235665b-a758-4c35-a922-55466a19bd44"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING ITERATION...\n",
      "PROCESSING FIRST 750 OBSERVATIONS\n",
      "STARTING ITERATION...\n",
      "PROCESSING FIRST 406 OBSERVATIONS\n"
     ]
    }
   ],
   "source": [
    "out_model_30.train(mode=False)\n",
    "\n",
    "obs_counter = 0\n",
    "total_pred = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "total_act = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "conf_a = {}\n",
    "conf_b = {}\n",
    "conf_c = {}\n",
    "conf_d = {}\n",
    "for i in range(1,10):\n",
    "    conf_a[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_b[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_c[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_d[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "for data in dataloaders['val']:\n",
    "    print(\"STARTING ITERATION...\")\n",
    "    inputs, labels = data\n",
    "    print(\"PROCESSING FIRST {} OBSERVATIONS\".format(len(inputs)))\n",
    "\n",
    "    inputs = Variable(inputs.cuda())\n",
    "    labels = Variable(labels.cuda())\n",
    "\n",
    "    outputs = out_model_30(inputs).sigmoid()\n",
    "    \n",
    "    total_act += labels.sum(0).cpu()\n",
    "    total_pred += outputs.sum(0).cpu()\n",
    "\n",
    "    # Store statistics (convert from autograd.Variable to float/int)\n",
    "    for i in range(1,10):\n",
    "        t = i/10\n",
    "        conf_a[i] += ((outputs.sigmoid()>t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_b[i] += ((outputs.sigmoid()<t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_c[i] += ((outputs.sigmoid()>t) == (labels<0.5)).sum(0).cpu().float()\n",
    "        conf_d[i] += ((outputs.sigmoid()<t) == (labels<0.5)).sum(0).cpu().float()\n",
    "\n",
    "    obs_counter += len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "nbpresent": {
     "id": "82758b84-e7ce-48d3-a3e1-f012ab124eea"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 -2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00 -2.1475e+09  0.0000e+00 -2.1475e+09  0.0000e+00\n",
      " 6.2634e+06 -1.0000e+00 -2.1475e+09  0.0000e+00 -2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "\n",
      "Columns 6 to 11 \n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 -6.5460e+04  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "-2.1475e+09  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "-2.1475e+09  0.0000e+00 -2.1475e+09 -2.1475e+09 -2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "\n",
      "Columns 12 to 13 \n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      "-2.1475e+09  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00\n",
      "[torch.IntTensor of size 9x14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comparison = Variable(torch.FloatTensor(9, 14))\n",
    "for i in range(9):\n",
    "    comparison[0] = conf_a[1] / obs_counter\n",
    "print(comparison.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "nbpresent": {
     "id": "b5bbfb54-d465-4c52-90ea-fff045789b9b"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3183\n",
       " 0.1462\n",
       " 0.3356\n",
       " 0.2171\n",
       " 0.3166\n",
       " 0.3183\n",
       " 0.2803\n",
       " 0.2898\n",
       " 0.2232\n",
       " 0.3279\n",
       " 0.2846\n",
       " 0.3045\n",
       " 0.3002\n",
       " 0.3192\n",
       "[torch.FloatTensor of size 14]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_d[9] / obs_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbpresent": {
     "id": "2b02d75e-a8b2-4d29-a9c3-27903e05d160"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fibrosis',\n",
       " 'Infiltration',\n",
       " 'Hernia',\n",
       " 'Effusion',\n",
       " 'Emphysema',\n",
       " 'Edema',\n",
       " 'Cardiomegaly',\n",
       " 'Mass',\n",
       " 'Nodule',\n",
       " 'Atelectasis',\n",
       " 'Pneumothorax',\n",
       " 'Pleural_Thickening',\n",
       " 'Consolidation',\n",
       " 'Pneumonia']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data_train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "nbpresent": {
     "id": "4feafc69-38b0-41e8-b670-92c3aa26bfff"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'state': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'val_error': val_error\n",
    "        }, model_out_path)\n",
    "'''\n",
    "test_load = torch.load('/user/xrayproj/output/20171120-01h41m56s_model_9.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'state', 'optimizer', 'scheduler', 'val_error'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_load.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_opt = test_load['optimizer']\n",
    "load_sched = test_load['scheduler']\n",
    "load_state = test_load['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.resnet18(pretrained=True)\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace FC layer\n",
    "model2.fc = nn.Linear(model2.fc.in_features, len(img_data_train.labels))\n",
    "\n",
    "model2_c = DataParallel(model2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_c.load_state_dict(load_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.7873  0.5093  0.2980  0.6386  0.4665  0.3371  0.1614  0.1846  0.1925  0.6630\n",
       "\n",
       "Columns 10 to 13 \n",
       " 0.0288  0.4468  0.3684  0.2549\n",
       "[torch.cuda.FloatTensor of size 1x14 (GPU 0)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_c.forward(Variable(img_data_train[0][0].unsqueeze(0).cuda())).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

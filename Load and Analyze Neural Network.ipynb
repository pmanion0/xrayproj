{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# General Python Packages\n",
    "import os, time, numbers, math\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import lr_scheduler, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Module\n",
    "\n",
    "# General Analytics Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization / Image Packages\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Randomization Functions\n",
    "from random import random as randuni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions for Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_bar(in_evaluator):\n",
    "    auc_out = in_evaluator.m_auc.value()\n",
    "    plt.bar(range(14), [v for v, _, _ in auc_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_curves(in_evaluator):\n",
    "    index_to_label = {idx: val for val, idx in img_data_train.label_to_index.items()}\n",
    "    auc_out = in_evaluator.m_auc.value()\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    for idx, (auc, tpr, fpr) in enumerate(auc_out):\n",
    "        disease = index_to_label[idx]\n",
    "        plt.plot(fpr, tpr, label=\"{0} (AUC: {1:0.3f})\".format(disease, auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve by Disease')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_sums(in_evaluator, bar_width = 0.4):\n",
    "    sums_out = in_evaluator.m_total_sums.get_results()\n",
    "    \n",
    "    act = sums_out['act'].data.numpy()\n",
    "    pred = sums_out['pred'].data.numpy()\n",
    "    \n",
    "    indx1 = range(14)\n",
    "    indx2 = [i+bar_width for i in indx1]\n",
    "\n",
    "    plt.bar(indx1, act,  width=bar_width, label=\"Actual\")\n",
    "    plt.bar(indx2, pred, width=bar_width, label=\"Predicted\")\n",
    "    \n",
    "    plt.xticks(indx1, img_data_train.labels, rotation=90)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Analysis of Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_evals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = torch.load('/user/rn18ff_model_9.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = learning_scheduler.last_epoch\n",
    "\n",
    "epoch_evals[epoch] = {}\n",
    "\n",
    "epoch_evals[epoch]['val'] = ModelEvaluator(model_ft, dataloaders['val'])\n",
    "#epoch_evals[epoch]['train'] = ModelEvaluator(model_ft, dataloaders['train'])\n",
    "\n",
    "epoch_evals[epoch]['val'].run()\n",
    "#epoch_evals[epoch]['train'].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curves(epoch_evals[epoch]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Training Sums\")\n",
    "#plot_compare_sums(epoch_evals[epoch]['train'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Validation Sums\")\n",
    "plot_compare_sums(epoch_evals[epoch]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_opt = test_load['optimizer']\n",
    "load_sched = test_load['scheduler']\n",
    "load_state = test_load['state']\n",
    "# Not used: 'epoch', 'val_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.resnet18(pretrained=True)\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace FC layer\n",
    "model2.fc = nn.Linear(model2.fc.in_features, len(img_data_train.labels))\n",
    "\n",
    "model2_c = DataParallel(model2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_c.load_state_dict(load_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_c.forward(Variable(img_data_train[0][0].unsqueeze(0).cuda())).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_run(model, dataset):\n",
    "    dataset\n",
    "    for data in self.dataset:\n",
    "        inputs, actuals = data\n",
    "\n",
    "        inputs = Variable(inputs.cuda(), volatile=True)\n",
    "        actuals = Variable(actuals.cuda(), volatile=True)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        preds = outputs.sigmoid()\n",
    "\n",
    "        self.m_total_sums.update(preds, actuals)\n",
    "        self.m_auc.add(preds, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model_30.train(mode=False)\n",
    "\n",
    "obs_counter = 0\n",
    "total_pred = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "total_act = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "conf_a = {}\n",
    "conf_b = {}\n",
    "conf_c = {}\n",
    "conf_d = {}\n",
    "for i in range(1,10):\n",
    "    conf_a[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_b[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_c[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_d[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "for data in dataloaders['val']:\n",
    "    print(\"STARTING ITERATION...\")\n",
    "    inputs, labels = data\n",
    "    print(\"PROCESSING FIRST {} OBSERVATIONS\".format(len(inputs)))\n",
    "\n",
    "    inputs = Variable(inputs.cuda())\n",
    "    labels = Variable(labels.cuda())\n",
    "\n",
    "    outputs = out_model_30(inputs).sigmoid()\n",
    "    \n",
    "    total_act += labels.sum(0).cpu()\n",
    "    total_pred += outputs.sum(0).cpu()\n",
    "\n",
    "    # Store statistics (convert from autograd.Variable to float/int)\n",
    "    for i in range(1,10):\n",
    "        t = i/10\n",
    "        conf_a[i] += ((outputs.sigmoid()>t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_b[i] += ((outputs.sigmoid()<t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_c[i] += ((outputs.sigmoid()>t) == (labels<0.5)).sum(0).cpu().float()\n",
    "        conf_d[i] += ((outputs.sigmoid()<t) == (labels<0.5)).sum(0).cpu().float()\n",
    "\n",
    "    obs_counter += len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = Variable(torch.FloatTensor(9, 14))\n",
    "for i in range(9):\n",
    "    comparison[0] = conf_a[1] / obs_counter\n",
    "print(comparison.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_run(model, dataset):\n",
    "    dataset\n",
    "    for data in self.dataset:\n",
    "        inputs, actuals = data\n",
    "\n",
    "        inputs = Variable(inputs.cuda(), volatile=True)\n",
    "        actuals = Variable(actuals.cuda(), volatile=True)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        preds = outputs.sigmoid()\n",
    "\n",
    "        self.m_total_sums.update(preds, actuals)\n",
    "        self.m_auc.add(preds, actuals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# General Python Packages\n",
    "import os, time, numbers, math\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import lr_scheduler, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Module\n",
    "\n",
    "# General Analytics Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization / Image Packages\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Randomization Functions\n",
    "from random import random as randuni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalSumMeter:\n",
    "    def __init__(self):\n",
    "        self.obs_counter = 0.0\n",
    "        self.total_pred = Variable(torch.FloatTensor(torch.zeros(14)), volatile=True)\n",
    "        self.total_act = Variable(torch.FloatTensor(torch.zeros(14)), volatile=True)\n",
    "        \n",
    "    def update(self, preds, actuals):\n",
    "        self.total_act += actuals.sum(0).cpu()\n",
    "        self.total_pred += preds.sum(0).cpu()\n",
    "        self.obs_counter += len(actuals)\n",
    "        \n",
    "    def get_results(self):\n",
    "        return {\n",
    "            'pred': self.total_pred / self.obs_counter,\n",
    "            'act': self.total_act / self.obs_counter\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassAUCMeter:\n",
    "    \"\"\" Wrapper on the normal AUCMeter to handle multi-class predictions \"\"\"\n",
    "    def __init__(self, num_class):\n",
    "        self.num_class = num_class\n",
    "        self.meters = []\n",
    "        for i in range(self.num_class):\n",
    "            self.meters.append(AUCMeter())\n",
    "\n",
    "    def add(self, output, target):\n",
    "        for i in range(self.num_class):\n",
    "            self.meters[i].add(output[:,i], target[:,i])\n",
    "        \n",
    "    def value(self):\n",
    "        output = []\n",
    "        for i in range(self.num_class):\n",
    "            output.append(self.meters[i].value())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCMeter:\n",
    "    \"\"\"\n",
    "    SOURCE: https://github.com/pytorch/tnt/blob/master/torchnet/meter/aucmeter.py\n",
    "    \n",
    "    The AUCMeter measures the area under the receiver-operating characteristic\n",
    "    (ROC) curve for binary classification problems. The area under the curve (AUC)\n",
    "    can be interpreted as the probability that, given a randomly selected positive\n",
    "    example and a randomly selected negative example, the positive example is\n",
    "    assigned a higher score by the classification model than the negative example.\n",
    "    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n",
    "    and `target`, where (1) the `output` contains model output scores that ought to\n",
    "    be higher when the model is more convinced that the example should be positively\n",
    "    labeled, and smaller when the model believes the example should be negatively\n",
    "    labeled (for instance, the output of a signoid function); and (2) the `target`\n",
    "    contains only values 0 (for negative examples) and 1 (for positive examples).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AUCMeter, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n",
    "        self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n",
    "\n",
    "    def add(self, output, target):\n",
    "        if torch.is_tensor(output):\n",
    "            output = output.cpu().squeeze().numpy()\n",
    "        if torch.is_tensor(target):\n",
    "            target = target.cpu().squeeze().numpy()\n",
    "        elif isinstance(target, numbers.Number):\n",
    "            target = np.asarray([target])\n",
    "        assert np.ndim(output) == 1, \\\n",
    "            'wrong output size (1D expected)'\n",
    "        assert np.ndim(target) == 1, \\\n",
    "            'wrong target size (1D expected)'\n",
    "        assert output.shape[0] == target.shape[0], \\\n",
    "            'number of outputs and targets does not match'\n",
    "        assert np.all(np.add(np.equal(target, 1), np.equal(target, 0))), \\\n",
    "            'targets should be binary (0, 1)'\n",
    "\n",
    "        self.scores = np.append(self.scores, output)\n",
    "        self.targets = np.append(self.targets, target)\n",
    "\n",
    "    def value(self):\n",
    "        # case when number of elements added are 0\n",
    "        if self.scores.shape[0] == 0:\n",
    "            return 0.5\n",
    "\n",
    "        # sorting the arrays\n",
    "        scores, sortind = torch.sort(torch.from_numpy(self.scores), dim=0, descending=True)\n",
    "        scores = scores.numpy()\n",
    "        sortind = sortind.numpy()\n",
    "\n",
    "        # creating the roc curve\n",
    "        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n",
    "\n",
    "        for i in range(1, scores.size + 1):\n",
    "            if self.targets[sortind[i - 1]] == 1:\n",
    "                tpr[i] = tpr[i - 1] + 1\n",
    "                fpr[i] = fpr[i - 1]\n",
    "            else:\n",
    "                tpr[i] = tpr[i - 1]\n",
    "                fpr[i] = fpr[i - 1] + 1\n",
    "\n",
    "        tpr /= (self.targets.sum() * 1.0)\n",
    "        fpr /= ((self.targets - 1.0).sum() * -1.0)\n",
    "\n",
    "        # calculating area under curve using trapezoidal rule\n",
    "        n = tpr.shape[0]\n",
    "        h = fpr[1:n] - fpr[0:n - 1]\n",
    "        sum_h = np.zeros(fpr.shape)\n",
    "        sum_h[0:n - 1] = h\n",
    "        sum_h[1:n] += h\n",
    "        area = (sum_h * tpr).sum() / 2.0\n",
    "\n",
    "        return (area, tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, dataset, num_classes = 14):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = num_classes\n",
    "        self.is_run = False\n",
    "        \n",
    "    def score_obs(self, data_row):\n",
    "        inputs, actuals = data_row\n",
    "\n",
    "        inputs = Variable(inputs.cuda(), volatile=True)\n",
    "        actuals = Variable(actuals.cuda(), volatile=True)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        preds = outputs.sigmoid()\n",
    "        \n",
    "        return preds, actuals\n",
    "    \n",
    "    def run(self, force_rerun = False):\n",
    "        if self.is_run and not force_rerun:\n",
    "            print(\"Already evaluated this...\")\n",
    "            return None\n",
    "        \n",
    "        self.model.train(False)\n",
    "    \n",
    "        self.m_total_sums = TotalSumMeter()\n",
    "        self.m_auc = MultiClassAUCMeter(self.num_classes)\n",
    "\n",
    "        for data in self.dataset:\n",
    "            preds, actuals = self.score_obs(data)\n",
    "\n",
    "            self.m_total_sums.update(preds, actuals)\n",
    "            self.m_auc.add(preds.data, actuals.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions for Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_bar(in_evaluator):\n",
    "    auc_out = in_evaluator.m_auc.value()\n",
    "    plt.bar(range(14), [v for v, _, _ in auc_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_curves(in_evaluator):\n",
    "    index_to_label = {idx: val for val, idx in img_data_train.label_to_index.items()}\n",
    "    auc_out = in_evaluator.m_auc.value()\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    for idx, (auc, tpr, fpr) in enumerate(auc_out):\n",
    "        disease = index_to_label[idx]\n",
    "        plt.plot(fpr, tpr, label=\"{0} (AUC: {1:0.3f})\".format(disease, auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve by Disease')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_sums(in_evaluator, bar_width = 0.4):\n",
    "    sums_out = in_evaluator.m_total_sums.get_results()\n",
    "    \n",
    "    act = sums_out['act'].data.numpy()\n",
    "    pred = sums_out['pred'].data.numpy()\n",
    "    \n",
    "    indx1 = range(14)\n",
    "    indx2 = [i+bar_width for i in indx1]\n",
    "\n",
    "    plt.bar(indx1, act,  width=bar_width, label=\"Actual\")\n",
    "    plt.bar(indx2, pred, width=bar_width, label=\"Predicted\")\n",
    "    \n",
    "    plt.xticks(indx1, img_data_train.labels, rotation=90)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Analysis of Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_evals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = torch.load('/user/rn18ff_model_9.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = learning_scheduler.last_epoch\n",
    "\n",
    "epoch_evals[epoch] = {}\n",
    "\n",
    "epoch_evals[epoch]['val'] = ModelEvaluator(model_ft, dataloaders['val'])\n",
    "#epoch_evals[epoch]['train'] = ModelEvaluator(model_ft, dataloaders['train'])\n",
    "\n",
    "epoch_evals[epoch]['val'].run()\n",
    "#epoch_evals[epoch]['train'].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_curves(epoch_evals[epoch]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Training Sums\")\n",
    "#plot_compare_sums(epoch_evals[epoch]['train'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Validation Sums\")\n",
    "plot_compare_sums(epoch_evals[epoch]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_opt = test_load['optimizer']\n",
    "load_sched = test_load['scheduler']\n",
    "load_state = test_load['state']\n",
    "# Not used: 'epoch', 'val_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.resnet18(pretrained=True)\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace FC layer\n",
    "model2.fc = nn.Linear(model2.fc.in_features, len(img_data_train.labels))\n",
    "\n",
    "model2_c = DataParallel(model2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_c.load_state_dict(load_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_c.forward(Variable(img_data_train[0][0].unsqueeze(0).cuda())).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_run(model, dataset):\n",
    "    dataset\n",
    "    for data in self.dataset:\n",
    "        inputs, actuals = data\n",
    "\n",
    "        inputs = Variable(inputs.cuda(), volatile=True)\n",
    "        actuals = Variable(actuals.cuda(), volatile=True)\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "        preds = outputs.sigmoid()\n",
    "\n",
    "        self.m_total_sums.update(preds, actuals)\n",
    "        self.m_auc.add(preds, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = Variable(torch.FloatTensor(9, 14))\n",
    "for i in range(9):\n",
    "    comparison[0] = conf_a[1] / obs_counter\n",
    "print(comparison.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model_30.train(mode=False)\n",
    "\n",
    "obs_counter = 0\n",
    "total_pred = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "total_act = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "conf_a = {}\n",
    "conf_b = {}\n",
    "conf_c = {}\n",
    "conf_d = {}\n",
    "for i in range(1,10):\n",
    "    conf_a[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_b[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_c[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "    conf_d[i] = Variable(torch.FloatTensor(torch.zeros(14)))\n",
    "\n",
    "for data in dataloaders['val']:\n",
    "    print(\"STARTING ITERATION...\")\n",
    "    inputs, labels = data\n",
    "    print(\"PROCESSING FIRST {} OBSERVATIONS\".format(len(inputs)))\n",
    "\n",
    "    inputs = Variable(inputs.cuda())\n",
    "    labels = Variable(labels.cuda())\n",
    "\n",
    "    outputs = out_model_30(inputs).sigmoid()\n",
    "    \n",
    "    total_act += labels.sum(0).cpu()\n",
    "    total_pred += outputs.sum(0).cpu()\n",
    "\n",
    "    # Store statistics (convert from autograd.Variable to float/int)\n",
    "    for i in range(1,10):\n",
    "        t = i/10\n",
    "        conf_a[i] += ((outputs.sigmoid()>t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_b[i] += ((outputs.sigmoid()<t) == (labels>0.5)).sum(0).cpu().float()\n",
    "        conf_c[i] += ((outputs.sigmoid()>t) == (labels<0.5)).sum(0).cpu().float()\n",
    "        conf_d[i] += ((outputs.sigmoid()<t) == (labels<0.5)).sum(0).cpu().float()\n",
    "\n",
    "    obs_counter += len(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ddb34fde-926f-42f6-8bfc-b1b19cb4881d"
    }
   },
   "source": [
    "# High-Level Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "80e3fe37-bc30-43b7-91c3-474b94a16db6"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# General Python Packages\n",
    "import os, time, numbers, math\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import lr_scheduler, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Module\n",
    "\n",
    "# General Analytics Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization / Image Packages\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Randomization Functions\n",
    "from random import random as randuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "9b582614-ccd7-4f48-8b66-a49ebe66807f"
    }
   },
   "outputs": [],
   "source": [
    "# Put MatPlotLib in interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Plot graphics inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6fed3f3e-b14a-457d-95d2-c3726ce0fb3e"
    }
   },
   "source": [
    "### Image Data Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "f962c744-4459-4ca1-851c-a19fe8457118"
    }
   },
   "outputs": [],
   "source": [
    "def is_image_file(fname):\n",
    "    \"\"\"Checks if a file is an image.\n",
    "    Args:\n",
    "        fname (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    return fname.lower().endswith('.png')\n",
    "\n",
    "def create_label_maps(details_df):\n",
    "    \"\"\" Take a descriptive dataframe and extract the unique labels and map to index values\n",
    "    Args:\n",
    "        details_df: Dataframe with the image details\n",
    "    Returns:\n",
    "        label_list: list of unique labels in the dataframe\n",
    "        label_to_index: map from labels to indices\n",
    "    \"\"\"\n",
    "    \"\"\" TODO: Research paper also excludes these labels but need to figure out how to handle\n",
    "              cases that have these as positive findings (completely exclude?)\n",
    "    excluded_labels = ['Edema','Hernia','Emphysema','Fibrosis','No Finding'\n",
    "                      'Pleural_Thickening','Consolidation']\n",
    "    \"\"\"\n",
    "    excluded_labels = ['No Finding']\n",
    "    \n",
    "    label_groups = details_df['Finding Labels'].unique()\n",
    "    unique_labels = set([label for sublist in label_groups.tolist() for label in sublist.split('|')])\n",
    "    \n",
    "    # Drop some label that we do not want to include\n",
    "    unique_labels = [l for l in unique_labels if l not in excluded_labels]\n",
    "\n",
    "    index_to_label = {idx: val for idx, val in enumerate(unique_labels)}\n",
    "    label_to_index = {val: idx for idx, val in index_to_label.items()}\n",
    "\n",
    "    label_list = list(label_to_index.keys())\n",
    "\n",
    "    return label_list, label_to_index\n",
    "\n",
    "def create_image_list(dir):\n",
    "    \"\"\" Create a full list of images available \n",
    "    Args:\n",
    "        dir (string): root directory of images with subdirectories underneath\n",
    "                      that have the .png images within them\n",
    "    Returns:\n",
    "        image_list: list of tuples with (image_name, full_image_path)\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for subfolder in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, subfolder)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for subfolder_path, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(subfolder_path, fname)\n",
    "                    image_list.append((fname, path))\n",
    "    return image_list\n",
    "\n",
    "def pil_loader(path):\n",
    "    \"\"\" Opens path as file with Pillow (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    Args:\n",
    "        path (string): File path to the image\n",
    "    Returns:\n",
    "        img: Image in RGB format\n",
    "    \"\"\"\n",
    "    f = open(path, 'rb')\n",
    "    return Image.open(f)\n",
    "    #with open(path, 'rb') as f:\n",
    "    #    return Image.open(f)\n",
    "        #with Image.open(f) as img:\n",
    "        #    return img.load()\n",
    "        \n",
    "def imshow(inp, title=None):\n",
    "    \"\"\" Convert tensor array to an image (only use post-dataset transform) \"\"\"\n",
    "    inp = inp[0]\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5d91972-9d4b-4022-842b-22c823f98fff"
    }
   },
   "source": [
    "### Torch Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "5bf4e82b-13ca-4ac2-bbb6-3081e820ab4e"
    }
   },
   "outputs": [],
   "source": [
    "class XrayImageSet(Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_root (string): root directory of the images in form image/subfolder/*.png\n",
    "        csv_file (string): path to the CSV data file\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "     Attributes:\n",
    "        labels (list): list of the possible label names.\n",
    "        label_to_index (dict): look from label name to a label index\n",
    "        imgs (list): List of (filename, image path) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_root, csv_file, transform=None, target_transform=None, loader = pil_loader):\n",
    "        \"\"\" Create an instance of the Xray Dataset \"\"\"\n",
    "        img_details = pd.read_csv(csv_file)\n",
    "        \n",
    "        labels, label_to_index = create_label_maps(img_details)\n",
    "        imgs = create_image_list(image_root)\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.image_details = img_details\n",
    "        self.image_root = image_root\n",
    "        self.labels = labels\n",
    "        self.label_to_index = label_to_index\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.max_label_index = max(label_to_index.values())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get image,labels pair by index\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        fname, path = self.imgs[index]\n",
    "        target = self.get_one_hot_labels(fname)\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Calculate length of the dataset (number of images) \"\"\"\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def get_labels(self, fname):\n",
    "        \"\"\" Return the label string for the file \"\"\"\n",
    "        return self.image_details[self.image_details['Image Index'] == fname]['Finding Labels'].values[0]\n",
    "    \n",
    "    def one_hot_labels(self, labels):\n",
    "        \"\"\" Convert the labels string (with each label separated by |) into 1-hot encoding \"\"\"\n",
    "        if labels == None:\n",
    "            return None\n",
    "        \n",
    "        split_label_indices = [self.label_to_index.get(label)\n",
    "                               for label in labels.split('|')\n",
    "                               if label != 'No Finding']\n",
    "        \n",
    "        out = [1 if idx in split_label_indices else 0 for idx in range(self.max_label_index+1)]\n",
    "        # This code UNHOTs the labels:\n",
    "        # out = '|'.join([index_to_label.get(idx) for idx, val in enumerate(one_hot_tuple) if val == 1])\n",
    "        return out\n",
    "\n",
    "    def get_one_hot_labels(self, fname):\n",
    "        \"\"\" Get the 1-hot encoded label array for the provided file \"\"\"\n",
    "        labels = self.get_labels(fname)\n",
    "        one_hot_labels = self.one_hot_labels(labels)\n",
    "        return torch.FloatTensor(one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Output Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "166ce9d1-ff17-4047-b9ae-b4903393ad15"
    }
   },
   "outputs": [],
   "source": [
    "class printer_writer:\n",
    "    def __init__(self, output_folder_path):\n",
    "        self.start_time = time.strftime('%Y%m%d-%Hh%Mm%Ss')\n",
    "        \n",
    "        self.outprefix = output_folder_path + '/' + self.start_time\n",
    "        \n",
    "        # Print Output File\n",
    "        self.print_out_path = self.outprefix + '_print.txt'\n",
    "        self.print_out_file = open(self.print_out_path, 'w', 1)\n",
    "        \n",
    "    def printw(self, string):\n",
    "        print(string)\n",
    "        try:\n",
    "            self.print_out_file.write(string + \"\\n\")\n",
    "        except: # Ignore errors\n",
    "            pass\n",
    "        \n",
    "    def save_checkpoint(self, epoch, model, optimizer, scheduler, val_error):\n",
    "        model_out_path = self.outprefix + '_model_' + str(epoch+1) + '.tar'\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'state': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'val_error': val_error\n",
    "        }, model_out_path)\n",
    "        \n",
    "    def close(self):\n",
    "        self.print_out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "eb99e6e4-00db-494a-ad27-70005761f49e"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, outfolder = '/user/xrayproj/output/'):\n",
    "    since = time.time()\n",
    "    scribe = printer_writer(outfolder)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scribe.printw('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        scribe.printw('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            obs_counter = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Store statistics (convert from autograd.Variable to float/int)\n",
    "                loss_val = loss.data[0]\n",
    "                correct_val = torch.sum( ((outputs.sigmoid()>0.5) == (labels>0.5)).long() ).data[0]\n",
    "                \n",
    "                running_loss += loss_val\n",
    "                running_corrects += correct_val\n",
    "                \n",
    "                obs_counter += len(inputs)\n",
    "                \n",
    "                batch_loss = 1.0 * loss_val / len(inputs)\n",
    "                batch_acc = 1.0 * correct_val / len(inputs)\n",
    "                status = ' |~~ {}@{}  Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    phase, obs_counter, batch_loss, batch_acc)\n",
    "                scribe.printw(status)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            scribe.printw('{}  Loss: {:.6f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Store the model on disk\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scribe.save_checkpoint(epoch, model, optimizer, None, epoch_loss)\n",
    "                else:\n",
    "                    scribe.save_checkpoint(epoch, model, optimizer, scheduler, epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    scribe.printw('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    scribe.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e6eebab-809d-4550-b771-135dbf2b893d"
    }
   },
   "source": [
    "### Customized Binary Cross Entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "f66b439c-1f87-4d08-939d-f64d085b846b"
    }
   },
   "outputs": [],
   "source": [
    "class BCEWithLogitsImbalanceWeightedLoss(Module):\n",
    "    def __init__(self, class_weight=None, size_average=True):\n",
    "        super(BCEWithLogitsImbalanceWeightedLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.imbalance_weighted_bce_with_logit(input, target, size_average=self.size_average)\n",
    "    \n",
    "    def imbalance_weighted_bce_with_logit(self, input, target, size_average=True):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        # Determine |P| and |N|\n",
    "        positive_labels = target.sum()\n",
    "        negative_labels = (1-target).sum()\n",
    "\n",
    "        # Upweight the less common class (very often the 1s)\n",
    "        beta_p = (positive_labels + negative_labels) / positive_labels\n",
    "        beta_n = (positive_labels + negative_labels) / negative_labels\n",
    "\n",
    "        # Adjust the losses accordingly\n",
    "        loss_weight = target * beta_p + (1-target) * beta_n\n",
    "\n",
    "        loss = loss * loss_weight\n",
    "\n",
    "        if size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNetBase(base_size = 18, only_update_fc = True):\n",
    "    \"\"\" ResNet 18 with only final FC layer updatable \"\"\"\n",
    "    m = None\n",
    "    if base_size == 18:\n",
    "        m = models.resnet18(pretrained=True)\n",
    "    elif base_size == 34:\n",
    "        m = models.resnet34(pretrained=True)\n",
    "    elif base_size == 50:\n",
    "        m = models.resnet50(pretrained=True)\n",
    "    elif base_size == 101:\n",
    "        m = models.resnet101(pretrained=True)\n",
    "    elif base_size == 152:\n",
    "        m = models.resnet152(pretrained=True)\n",
    "    \n",
    "    if only_update_fc:\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    m.fc = nn.Linear(m.fc.in_features, len(img_data_train.labels))\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean ± std. dev. of 7 runs, 10000000 loops each\n",
    "\n",
    "#### Time for __get_item__\n",
    "```\n",
    "%timeit img_data_train[3] # 30.8 ms ± 544 µs per loop\n",
    "```\n",
    "\n",
    "#### Breakdown for __get_item__\n",
    "```\n",
    "%timeit img_data_train.imgs[8] # 63 ns ± 0.0057 ns per loop\n",
    "%timeit img_data_train.get_one_hot_labels('00011558_012.png') # 8.72 ms ± 9.44 µs per loop\n",
    "%timeit img_data_train.loader('/user/images/images_006/00011558_012.png') # 14.1 ms ± 3.41 µs per loop\n",
    "%timeit img_data_train.transform(img) # 3.17 ms ± 1.32 µs per loop\n",
    "```\n",
    "\n",
    "#### Breakdown for loader() from __get_item__\n",
    "```\n",
    "%timeit open('/user/images/images_006/00011558_012.png', 'rb') # 7.72 µs ± 13.4 ns per loop\n",
    "%timeit Image.open(f) # 37.5 µs ± 2.25 µs per loop\n",
    "%timeit img.convert('RGB') # 498 µs ± 149 ns per loop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Begin Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b6f705b5-c4e4-4fbd-a517-f0c3b58c4305"
    }
   },
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU: 1\n"
     ]
    }
   ],
   "source": [
    "nn_input_size = 224 #1024\n",
    "batch_size = 64\n",
    "pin_mem_setting = True\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_workers = 10\n",
    "\n",
    "print(\"Number of GPU: {}\".format(num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "6716d746-b7b1-4aff-aec0-2bd91632bf28"
    }
   },
   "outputs": [],
   "source": [
    "img_transforms_train = transforms.Compose(\n",
    "    [#transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms_nontrain = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "62d30308-7ecb-40f4-a831-dd0a9274618a"
    }
   },
   "outputs": [],
   "source": [
    "img_data_train = XrayImageSet(image_root = '/user/images_processed/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms_train,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_train.imgs = [img for i, img in enumerate(img_data_train.imgs) if i % 10 >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "13f86c44-50f2-4809-8ee1-afb0d8ec0b7a"
    }
   },
   "outputs": [],
   "source": [
    "img_data_val   = XrayImageSet(image_root = '/user/images_processed/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms_nontrain,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_val.imgs = [img for i, img in enumerate(img_data_val.imgs) if i % 10 in (1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_test  = XrayImageSet(image_root = '/user/images_processed/',\n",
    "                              csv_file = '/user/img_details.csv',\n",
    "                              transform = img_transforms_nontrain,\n",
    "                              target_transform = None)\n",
    "\n",
    "img_data_test.imgs = [img for i, img in enumerate(img_data_test.imgs) if i % 10 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = set(img_data_train.imgs)\n",
    "val_set = set(img_data_val.imgs)\n",
    "test_set = set(img_data_test.imgs)\n",
    "assert len(train_set.intersection(val_set)) == 0\n",
    "assert len(train_set.intersection(test_set)) == 0\n",
    "assert len(val_set.intersection(test_set)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "09ab4157-ad5b-4602-8b93-85c86bd5a620"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 78484\n",
      "Validation Set Size: 22424\n",
      "Test Set Size: 11212\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size: {}\".format(len(img_data_train)))\n",
    "print(\"Validation Set Size: {}\".format(len(img_data_val)))\n",
    "print(\"Test Set Size: {}\".format(len(img_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "1001fa5d-b820-4da6-9d18-0ee7f4462b90"
    }
   },
   "outputs": [],
   "source": [
    "img_loader_train = DataLoader(img_data_train,\n",
    "                              batch_size = batch_size * num_gpus,\n",
    "                              shuffle = True,\n",
    "                              num_workers = num_workers,\n",
    "                              pin_memory = pin_mem_setting)\n",
    "\n",
    "img_loader_val   = DataLoader(img_data_val,\n",
    "                              batch_size = batch_size * num_gpus,\n",
    "                              shuffle = True,\n",
    "                              num_workers = num_workers,\n",
    "                              pin_memory = pin_mem_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "0e1d2cb0-4d0f-4ab1-a1cf-cd66a3e298a2"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': img_loader_train,\n",
    "    'val': img_loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "91946119-bf67-4871-92af-649559fa9bfd"
    }
   },
   "outputs": [],
   "source": [
    "model_base = ResNetBase(base_size = 18, only_update_fc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "0a904f1f-73ac-418b-86cf-cdafdf0f67b1"
    }
   },
   "outputs": [],
   "source": [
    "model_ft = DataParallel(model_base).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e6851de2-8645-4547-9f00-414ad8a3811a"
    }
   },
   "source": [
    "### Setup learning rates and procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "1d61f077-84ec-4d2c-bdb0-82b28f8c4ed9"
    }
   },
   "outputs": [],
   "source": [
    "#criterion = BCEWithLogitsImbalanceWeightedLoss()\n",
    "criterion_base = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_ft = SGD(model_ft.module.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "#optimizer_ft = SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "learning_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = criterion_base.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future code for allowing optimization of the base layer with a lower learning rate\n",
    "\n",
    "```\n",
    "ignored_params = list(map(id, model.fc.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in ignored_params,\n",
    "                     model.parameters())\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "            {'params': base_params},\n",
    "            {'params': model.fc.parameters(), 'lr': opt.lr}\n",
    "        ], lr=opt.lr*0.1, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f9596b5-a2fc-4e0c-984b-4e13b68dcd6d"
    }
   },
   "source": [
    "# Begin Training Network (Normal Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f1672c30-c299-4265-934b-6af391d9de8c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      " |~~ train@64  Loss: 0.011634 Acc: 6.9219\n",
      " |~~ train@128  Loss: 0.010655 Acc: 7.9219\n",
      " |~~ train@192  Loss: 0.009005 Acc: 10.2656\n",
      " |~~ train@256  Loss: 0.007177 Acc: 12.5156\n",
      " |~~ train@320  Loss: 0.005410 Acc: 13.4219\n",
      " |~~ train@384  Loss: 0.004334 Acc: 13.3438\n",
      " |~~ train@448  Loss: 0.003355 Acc: 13.4062\n",
      " |~~ train@512  Loss: 0.003276 Acc: 13.2656\n",
      " |~~ train@576  Loss: 0.003737 Acc: 13.0312\n",
      " |~~ train@640  Loss: 0.003266 Acc: 13.1875\n",
      " |~~ train@704  Loss: 0.002882 Acc: 13.2969\n",
      " |~~ train@768  Loss: 0.003613 Acc: 13.1562\n",
      " |~~ train@832  Loss: 0.002868 Acc: 13.3281\n",
      " |~~ train@896  Loss: 0.002253 Acc: 13.5312\n",
      " |~~ train@960  Loss: 0.003345 Acc: 13.3125\n",
      " |~~ train@1024  Loss: 0.002522 Acc: 13.4844\n",
      " |~~ train@1088  Loss: 0.003177 Acc: 13.3438\n",
      " |~~ train@1152  Loss: 0.004283 Acc: 13.0781\n",
      " |~~ train@1216  Loss: 0.002363 Acc: 13.5312\n",
      " |~~ train@1280  Loss: 0.003883 Acc: 13.1406\n",
      " |~~ train@1344  Loss: 0.002413 Acc: 13.5156\n",
      " |~~ train@1408  Loss: 0.002887 Acc: 13.2969\n",
      " |~~ train@1472  Loss: 0.002823 Acc: 13.3750\n",
      " |~~ train@1536  Loss: 0.003398 Acc: 13.2656\n",
      " |~~ train@1600  Loss: 0.003287 Acc: 13.2969\n",
      " |~~ train@1664  Loss: 0.004059 Acc: 13.1562\n",
      " |~~ train@1728  Loss: 0.002964 Acc: 13.2969\n",
      " |~~ train@1792  Loss: 0.002778 Acc: 13.4688\n",
      " |~~ train@1856  Loss: 0.002538 Acc: 13.4062\n",
      " |~~ train@1920  Loss: 0.004051 Acc: 13.1094\n",
      " |~~ train@1984  Loss: 0.003294 Acc: 13.3594\n",
      " |~~ train@2048  Loss: 0.004164 Acc: 13.1250\n",
      " |~~ train@2112  Loss: 0.002905 Acc: 13.4219\n",
      " |~~ train@2176  Loss: 0.003233 Acc: 13.2500\n",
      " |~~ train@2240  Loss: 0.003449 Acc: 13.2969\n",
      " |~~ train@2304  Loss: 0.003540 Acc: 13.1719\n",
      " |~~ train@2368  Loss: 0.003526 Acc: 13.2188\n",
      " |~~ train@2432  Loss: 0.003280 Acc: 13.2344\n",
      " |~~ train@2496  Loss: 0.002921 Acc: 13.3750\n",
      " |~~ train@2560  Loss: 0.002867 Acc: 13.3594\n",
      " |~~ train@2624  Loss: 0.002910 Acc: 13.3281\n",
      " |~~ train@2688  Loss: 0.002805 Acc: 13.3125\n",
      " |~~ train@2752  Loss: 0.002708 Acc: 13.4219\n",
      " |~~ train@2816  Loss: 0.002460 Acc: 13.4531\n",
      " |~~ train@2880  Loss: 0.002989 Acc: 13.2812\n",
      " |~~ train@2944  Loss: 0.003066 Acc: 13.2656\n",
      " |~~ train@3008  Loss: 0.003762 Acc: 13.0938\n",
      " |~~ train@3072  Loss: 0.002549 Acc: 13.4688\n",
      " |~~ train@3136  Loss: 0.002752 Acc: 13.2656\n",
      " |~~ train@3200  Loss: 0.002934 Acc: 13.3281\n",
      " |~~ train@3264  Loss: 0.002143 Acc: 13.5156\n",
      " |~~ train@3328  Loss: 0.003453 Acc: 13.1250\n",
      " |~~ train@3392  Loss: 0.003054 Acc: 13.2500\n",
      " |~~ train@3456  Loss: 0.002865 Acc: 13.2656\n",
      " |~~ train@3520  Loss: 0.002656 Acc: 13.3750\n",
      " |~~ train@3584  Loss: 0.002580 Acc: 13.3750\n",
      " |~~ train@3648  Loss: 0.002554 Acc: 13.4219\n",
      " |~~ train@3712  Loss: 0.003261 Acc: 13.1562\n",
      " |~~ train@3776  Loss: 0.002882 Acc: 13.2188\n",
      " |~~ train@3840  Loss: 0.002706 Acc: 13.3750\n",
      " |~~ train@3904  Loss: 0.003616 Acc: 13.2188\n",
      " |~~ train@3968  Loss: 0.003127 Acc: 13.2500\n",
      " |~~ train@4032  Loss: 0.002191 Acc: 13.4375\n",
      " |~~ train@4096  Loss: 0.002888 Acc: 13.2656\n",
      " |~~ train@4160  Loss: 0.002721 Acc: 13.3438\n",
      " |~~ train@4224  Loss: 0.003409 Acc: 13.1875\n",
      " |~~ train@4288  Loss: 0.003195 Acc: 13.1406\n",
      " |~~ train@4352  Loss: 0.003008 Acc: 13.2188\n",
      " |~~ train@4416  Loss: 0.003304 Acc: 13.1875\n",
      " |~~ train@4480  Loss: 0.002949 Acc: 13.3125\n",
      " |~~ train@4544  Loss: 0.002784 Acc: 13.3281\n",
      " |~~ train@4608  Loss: 0.002665 Acc: 13.4219\n",
      " |~~ train@4672  Loss: 0.003082 Acc: 13.2031\n",
      " |~~ train@4736  Loss: 0.003017 Acc: 13.2031\n",
      " |~~ train@4800  Loss: 0.003979 Acc: 12.9219\n",
      " |~~ train@4864  Loss: 0.002337 Acc: 13.4375\n",
      " |~~ train@4928  Loss: 0.002882 Acc: 13.2656\n",
      " |~~ train@4992  Loss: 0.002789 Acc: 13.2969\n",
      " |~~ train@5056  Loss: 0.002746 Acc: 13.3438\n",
      " |~~ train@5120  Loss: 0.003393 Acc: 13.0938\n",
      " |~~ train@5184  Loss: 0.002679 Acc: 13.3594\n",
      " |~~ train@5248  Loss: 0.002890 Acc: 13.2500\n",
      " |~~ train@5312  Loss: 0.002653 Acc: 13.3594\n",
      " |~~ train@5376  Loss: 0.002844 Acc: 13.2344\n",
      " |~~ train@5440  Loss: 0.003331 Acc: 13.0938\n",
      " |~~ train@5504  Loss: 0.003042 Acc: 13.2344\n",
      " |~~ train@5568  Loss: 0.002467 Acc: 13.4062\n",
      " |~~ train@5632  Loss: 0.002476 Acc: 13.4062\n",
      " |~~ train@5696  Loss: 0.002997 Acc: 13.2500\n",
      " |~~ train@5760  Loss: 0.003097 Acc: 13.2344\n",
      " |~~ train@5824  Loss: 0.003134 Acc: 13.1719\n",
      " |~~ train@5888  Loss: 0.003173 Acc: 13.1406\n",
      " |~~ train@5952  Loss: 0.002710 Acc: 13.3438\n",
      " |~~ train@6016  Loss: 0.002191 Acc: 13.4375\n",
      " |~~ train@6080  Loss: 0.003106 Acc: 13.1562\n",
      " |~~ train@6144  Loss: 0.002955 Acc: 13.2031\n",
      " |~~ train@6208  Loss: 0.003020 Acc: 13.2031\n",
      " |~~ train@6272  Loss: 0.003070 Acc: 13.2500\n",
      " |~~ train@6336  Loss: 0.003111 Acc: 13.2031\n",
      " |~~ train@6400  Loss: 0.003095 Acc: 13.1719\n",
      " |~~ train@6464  Loss: 0.002582 Acc: 13.3281\n",
      " |~~ train@6528  Loss: 0.002369 Acc: 13.4375\n",
      " |~~ train@6592  Loss: 0.003111 Acc: 13.2188\n",
      " |~~ train@6656  Loss: 0.003997 Acc: 12.9219\n",
      " |~~ train@6720  Loss: 0.003281 Acc: 13.1250\n",
      " |~~ train@6784  Loss: 0.002945 Acc: 13.2344\n",
      " |~~ train@6848  Loss: 0.002619 Acc: 13.3750\n",
      " |~~ train@6912  Loss: 0.003106 Acc: 13.2344\n",
      " |~~ train@6976  Loss: 0.002568 Acc: 13.3750\n",
      " |~~ train@7040  Loss: 0.002876 Acc: 13.2031\n",
      " |~~ train@7104  Loss: 0.003564 Acc: 13.0781\n",
      " |~~ train@7168  Loss: 0.002577 Acc: 13.4062\n",
      " |~~ train@7232  Loss: 0.003208 Acc: 13.1094\n",
      " |~~ train@7296  Loss: 0.002786 Acc: 13.3125\n",
      " |~~ train@7360  Loss: 0.003121 Acc: 13.1719\n",
      " |~~ train@7424  Loss: 0.002992 Acc: 13.1406\n",
      " |~~ train@7488  Loss: 0.002913 Acc: 13.2500\n",
      " |~~ train@7552  Loss: 0.002957 Acc: 13.2656\n",
      " |~~ train@7616  Loss: 0.002286 Acc: 13.4844\n",
      " |~~ train@7680  Loss: 0.003655 Acc: 13.0312\n",
      " |~~ train@7744  Loss: 0.002694 Acc: 13.2656\n",
      " |~~ train@7808  Loss: 0.002909 Acc: 13.2969\n",
      " |~~ train@7872  Loss: 0.003246 Acc: 13.1406\n",
      " |~~ train@7936  Loss: 0.002789 Acc: 13.3125\n",
      " |~~ train@8000  Loss: 0.002327 Acc: 13.4375\n",
      " |~~ train@8064  Loss: 0.003008 Acc: 13.2500\n",
      " |~~ train@8128  Loss: 0.002703 Acc: 13.3125\n",
      " |~~ train@8192  Loss: 0.002714 Acc: 13.2969\n",
      " |~~ train@8256  Loss: 0.003113 Acc: 13.2188\n",
      " |~~ train@8320  Loss: 0.003301 Acc: 13.1406\n",
      " |~~ train@8384  Loss: 0.002637 Acc: 13.3125\n",
      " |~~ train@8448  Loss: 0.002545 Acc: 13.3438\n",
      " |~~ train@8512  Loss: 0.002701 Acc: 13.3906\n",
      " |~~ train@8576  Loss: 0.003019 Acc: 13.2344\n",
      " |~~ train@8640  Loss: 0.002994 Acc: 13.2812\n",
      " |~~ train@8704  Loss: 0.002711 Acc: 13.3125\n",
      " |~~ train@8768  Loss: 0.003344 Acc: 13.0938\n",
      " |~~ train@8832  Loss: 0.002480 Acc: 13.3594\n",
      " |~~ train@8896  Loss: 0.002852 Acc: 13.2031\n",
      " |~~ train@8960  Loss: 0.002641 Acc: 13.3594\n",
      " |~~ train@9024  Loss: 0.002600 Acc: 13.3906\n",
      " |~~ train@9088  Loss: 0.002433 Acc: 13.4062\n",
      " |~~ train@9152  Loss: 0.002973 Acc: 13.1875\n",
      " |~~ train@9216  Loss: 0.002514 Acc: 13.4219\n",
      " |~~ train@9280  Loss: 0.003455 Acc: 13.1562\n",
      " |~~ train@9344  Loss: 0.002531 Acc: 13.3906\n",
      " |~~ train@9408  Loss: 0.002395 Acc: 13.4375\n",
      " |~~ train@9472  Loss: 0.003149 Acc: 13.1562\n",
      " |~~ train@9536  Loss: 0.003225 Acc: 13.1562\n",
      " |~~ train@9600  Loss: 0.002634 Acc: 13.3750\n",
      " |~~ train@9664  Loss: 0.002982 Acc: 13.2500\n",
      " |~~ train@9728  Loss: 0.002691 Acc: 13.3125\n",
      " |~~ train@9792  Loss: 0.002452 Acc: 13.3906\n",
      " |~~ train@9856  Loss: 0.003426 Acc: 13.0938\n",
      " |~~ train@9920  Loss: 0.003223 Acc: 13.2031\n",
      " |~~ train@9984  Loss: 0.002623 Acc: 13.3594\n",
      " |~~ train@10048  Loss: 0.003013 Acc: 13.2656\n",
      " |~~ train@10112  Loss: 0.002724 Acc: 13.3438\n",
      " |~~ train@10176  Loss: 0.003138 Acc: 13.1250\n",
      " |~~ train@10240  Loss: 0.003219 Acc: 13.0625\n",
      " |~~ train@10304  Loss: 0.002608 Acc: 13.3750\n",
      " |~~ train@10368  Loss: 0.002860 Acc: 13.2344\n",
      " |~~ train@10432  Loss: 0.002960 Acc: 13.2969\n",
      " |~~ train@10496  Loss: 0.003279 Acc: 13.1562\n",
      " |~~ train@10560  Loss: 0.003739 Acc: 13.0312\n",
      " |~~ train@10624  Loss: 0.002984 Acc: 13.2812\n",
      " |~~ train@10688  Loss: 0.003449 Acc: 13.1406\n",
      " |~~ train@10752  Loss: 0.002302 Acc: 13.4375\n",
      " |~~ train@10816  Loss: 0.002505 Acc: 13.3594\n",
      " |~~ train@10880  Loss: 0.002879 Acc: 13.2969\n",
      " |~~ train@10944  Loss: 0.003157 Acc: 13.1406\n",
      " |~~ train@11008  Loss: 0.002759 Acc: 13.2500\n",
      " |~~ train@11072  Loss: 0.003079 Acc: 13.2031\n",
      " |~~ train@11136  Loss: 0.003557 Acc: 13.0781\n",
      " |~~ train@11200  Loss: 0.002846 Acc: 13.2500\n",
      " |~~ train@11264  Loss: 0.003225 Acc: 13.1719\n",
      " |~~ train@11328  Loss: 0.002928 Acc: 13.1719\n",
      " |~~ train@11392  Loss: 0.002542 Acc: 13.4062\n",
      " |~~ train@11456  Loss: 0.003388 Acc: 13.1719\n",
      " |~~ train@11520  Loss: 0.003254 Acc: 13.0938\n",
      " |~~ train@11584  Loss: 0.002546 Acc: 13.3750\n",
      " |~~ train@11648  Loss: 0.002814 Acc: 13.2656\n",
      " |~~ train@11712  Loss: 0.003262 Acc: 13.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@11776  Loss: 0.003176 Acc: 13.1719\n",
      " |~~ train@11840  Loss: 0.003065 Acc: 13.1875\n",
      " |~~ train@11904  Loss: 0.002752 Acc: 13.3281\n",
      " |~~ train@11968  Loss: 0.002826 Acc: 13.3438\n",
      " |~~ train@12032  Loss: 0.002832 Acc: 13.2031\n",
      " |~~ train@12096  Loss: 0.002732 Acc: 13.3594\n",
      " |~~ train@12160  Loss: 0.003303 Acc: 13.1250\n",
      " |~~ train@12224  Loss: 0.002940 Acc: 13.2344\n",
      " |~~ train@12288  Loss: 0.002836 Acc: 13.2812\n",
      " |~~ train@12352  Loss: 0.002915 Acc: 13.2188\n",
      " |~~ train@12416  Loss: 0.003329 Acc: 13.0938\n",
      " |~~ train@12480  Loss: 0.002755 Acc: 13.2969\n",
      " |~~ train@12544  Loss: 0.002221 Acc: 13.4531\n",
      " |~~ train@12608  Loss: 0.002919 Acc: 13.2344\n",
      " |~~ train@12672  Loss: 0.003227 Acc: 13.1875\n",
      " |~~ train@12736  Loss: 0.003162 Acc: 13.1094\n",
      " |~~ train@12800  Loss: 0.002673 Acc: 13.3125\n",
      " |~~ train@12864  Loss: 0.002903 Acc: 13.2500\n",
      " |~~ train@12928  Loss: 0.002525 Acc: 13.3594\n",
      " |~~ train@12992  Loss: 0.002238 Acc: 13.4688\n",
      " |~~ train@13056  Loss: 0.003073 Acc: 13.2344\n",
      " |~~ train@13120  Loss: 0.002783 Acc: 13.2188\n",
      " |~~ train@13184  Loss: 0.002146 Acc: 13.5000\n",
      " |~~ train@13248  Loss: 0.002773 Acc: 13.2500\n",
      " |~~ train@13312  Loss: 0.003514 Acc: 13.0469\n",
      " |~~ train@13376  Loss: 0.002916 Acc: 13.2188\n",
      " |~~ train@13440  Loss: 0.002497 Acc: 13.3594\n",
      " |~~ train@13504  Loss: 0.002987 Acc: 13.2188\n",
      " |~~ train@13568  Loss: 0.003347 Acc: 13.0625\n",
      " |~~ train@13632  Loss: 0.002858 Acc: 13.2812\n",
      " |~~ train@13696  Loss: 0.002925 Acc: 13.2656\n",
      " |~~ train@13760  Loss: 0.003678 Acc: 13.0312\n",
      " |~~ train@13824  Loss: 0.003280 Acc: 13.1562\n",
      " |~~ train@13888  Loss: 0.002737 Acc: 13.3594\n",
      " |~~ train@13952  Loss: 0.003055 Acc: 13.2031\n",
      " |~~ train@14016  Loss: 0.002476 Acc: 13.3594\n",
      " |~~ train@14080  Loss: 0.002089 Acc: 13.5469\n",
      " |~~ train@14144  Loss: 0.002523 Acc: 13.2812\n",
      " |~~ train@14208  Loss: 0.002974 Acc: 13.2031\n",
      " |~~ train@14272  Loss: 0.003140 Acc: 13.2500\n",
      " |~~ train@14336  Loss: 0.002704 Acc: 13.3438\n",
      " |~~ train@14400  Loss: 0.002682 Acc: 13.3125\n",
      " |~~ train@14464  Loss: 0.003140 Acc: 13.0938\n",
      " |~~ train@14528  Loss: 0.002320 Acc: 13.4062\n",
      " |~~ train@14592  Loss: 0.002603 Acc: 13.3125\n",
      " |~~ train@14656  Loss: 0.002692 Acc: 13.3438\n",
      " |~~ train@14720  Loss: 0.002767 Acc: 13.3125\n",
      " |~~ train@14784  Loss: 0.003276 Acc: 13.0625\n",
      " |~~ train@14848  Loss: 0.002608 Acc: 13.3438\n",
      " |~~ train@14912  Loss: 0.003132 Acc: 13.1406\n",
      " |~~ train@14976  Loss: 0.002655 Acc: 13.2812\n",
      " |~~ train@15040  Loss: 0.002631 Acc: 13.2656\n",
      " |~~ train@15104  Loss: 0.003371 Acc: 13.1094\n",
      " |~~ train@15168  Loss: 0.002951 Acc: 13.2656\n",
      " |~~ train@15232  Loss: 0.002692 Acc: 13.3594\n",
      " |~~ train@15296  Loss: 0.002594 Acc: 13.4062\n",
      " |~~ train@15360  Loss: 0.002346 Acc: 13.3750\n",
      " |~~ train@15424  Loss: 0.002564 Acc: 13.3281\n",
      " |~~ train@15488  Loss: 0.002635 Acc: 13.2812\n",
      " |~~ train@15552  Loss: 0.002351 Acc: 13.3906\n",
      " |~~ train@15616  Loss: 0.002510 Acc: 13.2969\n",
      " |~~ train@15680  Loss: 0.002901 Acc: 13.2188\n",
      " |~~ train@15744  Loss: 0.002943 Acc: 13.2656\n",
      " |~~ train@15808  Loss: 0.002504 Acc: 13.3906\n",
      " |~~ train@15872  Loss: 0.003236 Acc: 13.1250\n",
      " |~~ train@15936  Loss: 0.002506 Acc: 13.3594\n",
      " |~~ train@16000  Loss: 0.002898 Acc: 13.2344\n",
      " |~~ train@16064  Loss: 0.002632 Acc: 13.3281\n",
      " |~~ train@16128  Loss: 0.002381 Acc: 13.3906\n",
      " |~~ train@16192  Loss: 0.002687 Acc: 13.3438\n",
      " |~~ train@16256  Loss: 0.002853 Acc: 13.2344\n",
      " |~~ train@16320  Loss: 0.002604 Acc: 13.3281\n",
      " |~~ train@16384  Loss: 0.002492 Acc: 13.4375\n",
      " |~~ train@16448  Loss: 0.002895 Acc: 13.2812\n",
      " |~~ train@16512  Loss: 0.003022 Acc: 13.1875\n",
      " |~~ train@16576  Loss: 0.002407 Acc: 13.3750\n",
      " |~~ train@16640  Loss: 0.002496 Acc: 13.3438\n",
      " |~~ train@16704  Loss: 0.002414 Acc: 13.3594\n",
      " |~~ train@16768  Loss: 0.002617 Acc: 13.3125\n",
      " |~~ train@16832  Loss: 0.003123 Acc: 13.0938\n",
      " |~~ train@16896  Loss: 0.002788 Acc: 13.3438\n",
      " |~~ train@16960  Loss: 0.002919 Acc: 13.2500\n",
      " |~~ train@17024  Loss: 0.002599 Acc: 13.3438\n",
      " |~~ train@17088  Loss: 0.003033 Acc: 13.2031\n",
      " |~~ train@17152  Loss: 0.003206 Acc: 13.2188\n",
      " |~~ train@17216  Loss: 0.002814 Acc: 13.3750\n",
      " |~~ train@17280  Loss: 0.003162 Acc: 13.1406\n",
      " |~~ train@17344  Loss: 0.002281 Acc: 13.4062\n",
      " |~~ train@17408  Loss: 0.003115 Acc: 13.2188\n",
      " |~~ train@17472  Loss: 0.003263 Acc: 13.1250\n",
      " |~~ train@17536  Loss: 0.002447 Acc: 13.4062\n",
      " |~~ train@17600  Loss: 0.003139 Acc: 13.2031\n",
      " |~~ train@17664  Loss: 0.002748 Acc: 13.2656\n",
      " |~~ train@17728  Loss: 0.003129 Acc: 13.2188\n",
      " |~~ train@17792  Loss: 0.003074 Acc: 13.2656\n",
      " |~~ train@17856  Loss: 0.002719 Acc: 13.3281\n",
      " |~~ train@17920  Loss: 0.002480 Acc: 13.3750\n",
      " |~~ train@17984  Loss: 0.002979 Acc: 13.2500\n",
      " |~~ train@18048  Loss: 0.003094 Acc: 13.1719\n",
      " |~~ train@18112  Loss: 0.003021 Acc: 13.2812\n",
      " |~~ train@18176  Loss: 0.003742 Acc: 12.9844\n",
      " |~~ train@18240  Loss: 0.002734 Acc: 13.3281\n",
      " |~~ train@18304  Loss: 0.002516 Acc: 13.4062\n",
      " |~~ train@18368  Loss: 0.002469 Acc: 13.3594\n",
      " |~~ train@18432  Loss: 0.002828 Acc: 13.2969\n",
      " |~~ train@18496  Loss: 0.002585 Acc: 13.3594\n",
      " |~~ train@18560  Loss: 0.002416 Acc: 13.3438\n",
      " |~~ train@18624  Loss: 0.003337 Acc: 13.1094\n",
      " |~~ train@18688  Loss: 0.002312 Acc: 13.4531\n",
      " |~~ train@18752  Loss: 0.002100 Acc: 13.5625\n",
      " |~~ train@18816  Loss: 0.002409 Acc: 13.4062\n",
      " |~~ train@18880  Loss: 0.002648 Acc: 13.3125\n",
      " |~~ train@18944  Loss: 0.002473 Acc: 13.4531\n",
      " |~~ train@19008  Loss: 0.002831 Acc: 13.2500\n",
      " |~~ train@19072  Loss: 0.002469 Acc: 13.3906\n",
      " |~~ train@19136  Loss: 0.003347 Acc: 13.0469\n",
      " |~~ train@19200  Loss: 0.003494 Acc: 13.1406\n",
      " |~~ train@19264  Loss: 0.002868 Acc: 13.2969\n",
      " |~~ train@19328  Loss: 0.002098 Acc: 13.5156\n",
      " |~~ train@19392  Loss: 0.002821 Acc: 13.2656\n",
      " |~~ train@19456  Loss: 0.002433 Acc: 13.4219\n",
      " |~~ train@19520  Loss: 0.002782 Acc: 13.2344\n",
      " |~~ train@19584  Loss: 0.002431 Acc: 13.3906\n",
      " |~~ train@19648  Loss: 0.002510 Acc: 13.3438\n",
      " |~~ train@19712  Loss: 0.002911 Acc: 13.2500\n",
      " |~~ train@19776  Loss: 0.003052 Acc: 13.2500\n",
      " |~~ train@19840  Loss: 0.002188 Acc: 13.4688\n",
      " |~~ train@19904  Loss: 0.002616 Acc: 13.3438\n",
      " |~~ train@19968  Loss: 0.002842 Acc: 13.3281\n",
      " |~~ train@20032  Loss: 0.002661 Acc: 13.3594\n",
      " |~~ train@20096  Loss: 0.002984 Acc: 13.1719\n",
      " |~~ train@20160  Loss: 0.002704 Acc: 13.3125\n",
      " |~~ train@20224  Loss: 0.002222 Acc: 13.4375\n",
      " |~~ train@20288  Loss: 0.001913 Acc: 13.5781\n",
      " |~~ train@20352  Loss: 0.002694 Acc: 13.3438\n",
      " |~~ train@20416  Loss: 0.003463 Acc: 13.0938\n",
      " |~~ train@20480  Loss: 0.002734 Acc: 13.2969\n",
      " |~~ train@20544  Loss: 0.003358 Acc: 13.1094\n",
      " |~~ train@20608  Loss: 0.002738 Acc: 13.3125\n",
      " |~~ train@20672  Loss: 0.002926 Acc: 13.2031\n",
      " |~~ train@20736  Loss: 0.002686 Acc: 13.2812\n",
      " |~~ train@20800  Loss: 0.003237 Acc: 13.1250\n",
      " |~~ train@20864  Loss: 0.003046 Acc: 13.2344\n",
      " |~~ train@20928  Loss: 0.002237 Acc: 13.4531\n",
      " |~~ train@20992  Loss: 0.002540 Acc: 13.4375\n",
      " |~~ train@21056  Loss: 0.003053 Acc: 13.2344\n",
      " |~~ train@21120  Loss: 0.002693 Acc: 13.2812\n",
      " |~~ train@21184  Loss: 0.002274 Acc: 13.4375\n",
      " |~~ train@21248  Loss: 0.002269 Acc: 13.5000\n",
      " |~~ train@21312  Loss: 0.002498 Acc: 13.3750\n",
      " |~~ train@21376  Loss: 0.002415 Acc: 13.3906\n",
      " |~~ train@21440  Loss: 0.002716 Acc: 13.2656\n",
      " |~~ train@21504  Loss: 0.002868 Acc: 13.3438\n",
      " |~~ train@21568  Loss: 0.002511 Acc: 13.3750\n",
      " |~~ train@21632  Loss: 0.003129 Acc: 13.1562\n",
      " |~~ train@21696  Loss: 0.002385 Acc: 13.4062\n",
      " |~~ train@21760  Loss: 0.003014 Acc: 13.1406\n",
      " |~~ train@21824  Loss: 0.002584 Acc: 13.3750\n",
      " |~~ train@21888  Loss: 0.002798 Acc: 13.2656\n",
      " |~~ train@21952  Loss: 0.002359 Acc: 13.3750\n",
      " |~~ train@22016  Loss: 0.002613 Acc: 13.3281\n",
      " |~~ train@22080  Loss: 0.003100 Acc: 13.2031\n",
      " |~~ train@22144  Loss: 0.003178 Acc: 13.1562\n",
      " |~~ train@22208  Loss: 0.002419 Acc: 13.3594\n",
      " |~~ train@22272  Loss: 0.002795 Acc: 13.2656\n",
      " |~~ train@22336  Loss: 0.002903 Acc: 13.2344\n",
      " |~~ train@22400  Loss: 0.002587 Acc: 13.3281\n",
      " |~~ train@22464  Loss: 0.002663 Acc: 13.3125\n",
      " |~~ train@22528  Loss: 0.002345 Acc: 13.4219\n",
      " |~~ train@22592  Loss: 0.003182 Acc: 13.1250\n",
      " |~~ train@22656  Loss: 0.003102 Acc: 13.2344\n",
      " |~~ train@22720  Loss: 0.002571 Acc: 13.3281\n",
      " |~~ train@22784  Loss: 0.002393 Acc: 13.3750\n",
      " |~~ train@22848  Loss: 0.002989 Acc: 13.1875\n",
      " |~~ train@22912  Loss: 0.002564 Acc: 13.3594\n",
      " |~~ train@22976  Loss: 0.002572 Acc: 13.3281\n",
      " |~~ train@23040  Loss: 0.002858 Acc: 13.1875\n",
      " |~~ train@23104  Loss: 0.002987 Acc: 13.1875\n",
      " |~~ train@23168  Loss: 0.003012 Acc: 13.2031\n",
      " |~~ train@23232  Loss: 0.002246 Acc: 13.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@23296  Loss: 0.002679 Acc: 13.3125\n",
      " |~~ train@23360  Loss: 0.003396 Acc: 13.1250\n",
      " |~~ train@23424  Loss: 0.002786 Acc: 13.2500\n",
      " |~~ train@23488  Loss: 0.003905 Acc: 13.0156\n",
      " |~~ train@23552  Loss: 0.002662 Acc: 13.2812\n",
      " |~~ train@23616  Loss: 0.003110 Acc: 13.2031\n",
      " |~~ train@23680  Loss: 0.002407 Acc: 13.3906\n",
      " |~~ train@23744  Loss: 0.002552 Acc: 13.3281\n",
      " |~~ train@23808  Loss: 0.002014 Acc: 13.5469\n",
      " |~~ train@23872  Loss: 0.002823 Acc: 13.2500\n",
      " |~~ train@23936  Loss: 0.003279 Acc: 13.1719\n",
      " |~~ train@24000  Loss: 0.002694 Acc: 13.2969\n",
      " |~~ train@24064  Loss: 0.002676 Acc: 13.3906\n",
      " |~~ train@24128  Loss: 0.002488 Acc: 13.3750\n",
      " |~~ train@24192  Loss: 0.002430 Acc: 13.4531\n",
      " |~~ train@24256  Loss: 0.002403 Acc: 13.4219\n",
      " |~~ train@24320  Loss: 0.002413 Acc: 13.4062\n",
      " |~~ train@24384  Loss: 0.002811 Acc: 13.2812\n",
      " |~~ train@24448  Loss: 0.002920 Acc: 13.2344\n",
      " |~~ train@24512  Loss: 0.002944 Acc: 13.2500\n",
      " |~~ train@24576  Loss: 0.002297 Acc: 13.3750\n",
      " |~~ train@24640  Loss: 0.002313 Acc: 13.4219\n",
      " |~~ train@24704  Loss: 0.002713 Acc: 13.2812\n",
      " |~~ train@24768  Loss: 0.003184 Acc: 13.1406\n",
      " |~~ train@24832  Loss: 0.002037 Acc: 13.5156\n",
      " |~~ train@24896  Loss: 0.003000 Acc: 13.2031\n",
      " |~~ train@24960  Loss: 0.002675 Acc: 13.2969\n",
      " |~~ train@25024  Loss: 0.002550 Acc: 13.3438\n",
      " |~~ train@25088  Loss: 0.002519 Acc: 13.3125\n",
      " |~~ train@25152  Loss: 0.002671 Acc: 13.3594\n",
      " |~~ train@25216  Loss: 0.002954 Acc: 13.2344\n",
      " |~~ train@25280  Loss: 0.002897 Acc: 13.2031\n",
      " |~~ train@25344  Loss: 0.002510 Acc: 13.2969\n",
      " |~~ train@25408  Loss: 0.003288 Acc: 13.1406\n",
      " |~~ train@25472  Loss: 0.002995 Acc: 13.1719\n",
      " |~~ train@25536  Loss: 0.002439 Acc: 13.3594\n",
      " |~~ train@25600  Loss: 0.003013 Acc: 13.1719\n",
      " |~~ train@25664  Loss: 0.003164 Acc: 13.2031\n",
      " |~~ train@25728  Loss: 0.003328 Acc: 13.0469\n",
      " |~~ train@25792  Loss: 0.002589 Acc: 13.3125\n",
      " |~~ train@25856  Loss: 0.002341 Acc: 13.4219\n",
      " |~~ train@25920  Loss: 0.002850 Acc: 13.2031\n",
      " |~~ train@25984  Loss: 0.002972 Acc: 13.1875\n",
      " |~~ train@26048  Loss: 0.002256 Acc: 13.4375\n",
      " |~~ train@26112  Loss: 0.002985 Acc: 13.2812\n",
      " |~~ train@26176  Loss: 0.003012 Acc: 13.2500\n",
      " |~~ train@26240  Loss: 0.002307 Acc: 13.4688\n",
      " |~~ train@26304  Loss: 0.002577 Acc: 13.3594\n",
      " |~~ train@26368  Loss: 0.003291 Acc: 13.1250\n",
      " |~~ train@26432  Loss: 0.002830 Acc: 13.2188\n",
      " |~~ train@26496  Loss: 0.002985 Acc: 13.1875\n",
      " |~~ train@26560  Loss: 0.003635 Acc: 12.9844\n",
      " |~~ train@26624  Loss: 0.002242 Acc: 13.4688\n",
      " |~~ train@26688  Loss: 0.002927 Acc: 13.2500\n",
      " |~~ train@26752  Loss: 0.003021 Acc: 13.1562\n",
      " |~~ train@26816  Loss: 0.002845 Acc: 13.2656\n",
      " |~~ train@26880  Loss: 0.002583 Acc: 13.3281\n",
      " |~~ train@26944  Loss: 0.002534 Acc: 13.3750\n",
      " |~~ train@27008  Loss: 0.002575 Acc: 13.3750\n",
      " |~~ train@27072  Loss: 0.002735 Acc: 13.2500\n",
      " |~~ train@27136  Loss: 0.002903 Acc: 13.2500\n",
      " |~~ train@27200  Loss: 0.002787 Acc: 13.2500\n",
      " |~~ train@27264  Loss: 0.003302 Acc: 13.0469\n",
      " |~~ train@27328  Loss: 0.003323 Acc: 13.2031\n",
      " |~~ train@27392  Loss: 0.002584 Acc: 13.3281\n",
      " |~~ train@27456  Loss: 0.001919 Acc: 13.5625\n",
      " |~~ train@27520  Loss: 0.002190 Acc: 13.4531\n",
      " |~~ train@27584  Loss: 0.003269 Acc: 13.0469\n",
      " |~~ train@27648  Loss: 0.002909 Acc: 13.2031\n",
      " |~~ train@27712  Loss: 0.002731 Acc: 13.2656\n",
      " |~~ train@27776  Loss: 0.002614 Acc: 13.3281\n",
      " |~~ train@27840  Loss: 0.002559 Acc: 13.3906\n",
      " |~~ train@27904  Loss: 0.002631 Acc: 13.3281\n",
      " |~~ train@27968  Loss: 0.003313 Acc: 13.0938\n",
      " |~~ train@28032  Loss: 0.002588 Acc: 13.3125\n",
      " |~~ train@28096  Loss: 0.002475 Acc: 13.3438\n",
      " |~~ train@28160  Loss: 0.003132 Acc: 13.1406\n",
      " |~~ train@28224  Loss: 0.002981 Acc: 13.1562\n",
      " |~~ train@28288  Loss: 0.002537 Acc: 13.3750\n",
      " |~~ train@28352  Loss: 0.002883 Acc: 13.2344\n",
      " |~~ train@28416  Loss: 0.002581 Acc: 13.3281\n",
      " |~~ train@28480  Loss: 0.002802 Acc: 13.2812\n",
      " |~~ train@28544  Loss: 0.002400 Acc: 13.4062\n",
      " |~~ train@28608  Loss: 0.002946 Acc: 13.1562\n",
      " |~~ train@28672  Loss: 0.002703 Acc: 13.2812\n",
      " |~~ train@28736  Loss: 0.003204 Acc: 13.1406\n",
      " |~~ train@28800  Loss: 0.002871 Acc: 13.2031\n",
      " |~~ train@28864  Loss: 0.002279 Acc: 13.4531\n",
      " |~~ train@28928  Loss: 0.002528 Acc: 13.4688\n",
      " |~~ train@28992  Loss: 0.003390 Acc: 13.1094\n",
      " |~~ train@29056  Loss: 0.002804 Acc: 13.2500\n",
      " |~~ train@29120  Loss: 0.002995 Acc: 13.1562\n",
      " |~~ train@29184  Loss: 0.002824 Acc: 13.2812\n",
      " |~~ train@29248  Loss: 0.002743 Acc: 13.1875\n",
      " |~~ train@29312  Loss: 0.002735 Acc: 13.2656\n",
      " |~~ train@29376  Loss: 0.003393 Acc: 13.0938\n",
      " |~~ train@29440  Loss: 0.002690 Acc: 13.3906\n",
      " |~~ train@29504  Loss: 0.002732 Acc: 13.2969\n",
      " |~~ train@29568  Loss: 0.002795 Acc: 13.2344\n",
      " |~~ train@29632  Loss: 0.002868 Acc: 13.2188\n",
      " |~~ train@29696  Loss: 0.002562 Acc: 13.3594\n",
      " |~~ train@29760  Loss: 0.002288 Acc: 13.4219\n",
      " |~~ train@29824  Loss: 0.002603 Acc: 13.3281\n",
      " |~~ train@29888  Loss: 0.002699 Acc: 13.3281\n",
      " |~~ train@29952  Loss: 0.003015 Acc: 13.1719\n",
      " |~~ train@30016  Loss: 0.002932 Acc: 13.2344\n",
      " |~~ train@30080  Loss: 0.002817 Acc: 13.2344\n",
      " |~~ train@30144  Loss: 0.002387 Acc: 13.3906\n",
      " |~~ train@30208  Loss: 0.002751 Acc: 13.3281\n",
      " |~~ train@30272  Loss: 0.002667 Acc: 13.2344\n",
      " |~~ train@30336  Loss: 0.002860 Acc: 13.2500\n",
      " |~~ train@30400  Loss: 0.002627 Acc: 13.3750\n",
      " |~~ train@30464  Loss: 0.002725 Acc: 13.2812\n",
      " |~~ train@30528  Loss: 0.002944 Acc: 13.2656\n",
      " |~~ train@30592  Loss: 0.002639 Acc: 13.3906\n",
      " |~~ train@30656  Loss: 0.003261 Acc: 13.0781\n",
      " |~~ train@30720  Loss: 0.003252 Acc: 13.1719\n",
      " |~~ train@30784  Loss: 0.002773 Acc: 13.2656\n",
      " |~~ train@30848  Loss: 0.002751 Acc: 13.2500\n",
      " |~~ train@30912  Loss: 0.002068 Acc: 13.5312\n",
      " |~~ train@30976  Loss: 0.002948 Acc: 13.2344\n",
      " |~~ train@31040  Loss: 0.002804 Acc: 13.2812\n",
      " |~~ train@31104  Loss: 0.003170 Acc: 13.1719\n",
      " |~~ train@31168  Loss: 0.002709 Acc: 13.2656\n",
      " |~~ train@31232  Loss: 0.002714 Acc: 13.2188\n",
      " |~~ train@31296  Loss: 0.002743 Acc: 13.2344\n",
      " |~~ train@31360  Loss: 0.002316 Acc: 13.4375\n",
      " |~~ train@31424  Loss: 0.002452 Acc: 13.4062\n",
      " |~~ train@31488  Loss: 0.002312 Acc: 13.4688\n",
      " |~~ train@31552  Loss: 0.002397 Acc: 13.3906\n",
      " |~~ train@31616  Loss: 0.002407 Acc: 13.4062\n",
      " |~~ train@31680  Loss: 0.002835 Acc: 13.3281\n",
      " |~~ train@31744  Loss: 0.002633 Acc: 13.2500\n",
      " |~~ train@31808  Loss: 0.002609 Acc: 13.3438\n",
      " |~~ train@31872  Loss: 0.002525 Acc: 13.3594\n",
      " |~~ train@31936  Loss: 0.002637 Acc: 13.2969\n",
      " |~~ train@32000  Loss: 0.002823 Acc: 13.2344\n",
      " |~~ train@32064  Loss: 0.002209 Acc: 13.4688\n",
      " |~~ train@32128  Loss: 0.002851 Acc: 13.2656\n",
      " |~~ train@32192  Loss: 0.002367 Acc: 13.3594\n",
      " |~~ train@32256  Loss: 0.002601 Acc: 13.3906\n",
      " |~~ train@32320  Loss: 0.002375 Acc: 13.4219\n",
      " |~~ train@32384  Loss: 0.002635 Acc: 13.3438\n",
      " |~~ train@32448  Loss: 0.002771 Acc: 13.2969\n",
      " |~~ train@32512  Loss: 0.002591 Acc: 13.3125\n",
      " |~~ train@32576  Loss: 0.002168 Acc: 13.4531\n",
      " |~~ train@32640  Loss: 0.002225 Acc: 13.4531\n",
      " |~~ train@32704  Loss: 0.003304 Acc: 13.1094\n",
      " |~~ train@32768  Loss: 0.002590 Acc: 13.2656\n",
      " |~~ train@32832  Loss: 0.002364 Acc: 13.4062\n",
      " |~~ train@32896  Loss: 0.002876 Acc: 13.2344\n",
      " |~~ train@32960  Loss: 0.002804 Acc: 13.3125\n",
      " |~~ train@33024  Loss: 0.003109 Acc: 13.1562\n",
      " |~~ train@33088  Loss: 0.002631 Acc: 13.3594\n",
      " |~~ train@33152  Loss: 0.002879 Acc: 13.2812\n",
      " |~~ train@33216  Loss: 0.002617 Acc: 13.3281\n",
      " |~~ train@33280  Loss: 0.003543 Acc: 13.0625\n",
      " |~~ train@33344  Loss: 0.002771 Acc: 13.2969\n",
      " |~~ train@33408  Loss: 0.002606 Acc: 13.2812\n",
      " |~~ train@33472  Loss: 0.002515 Acc: 13.3281\n",
      " |~~ train@33536  Loss: 0.002979 Acc: 13.2188\n",
      " |~~ train@33600  Loss: 0.002193 Acc: 13.4531\n",
      " |~~ train@33664  Loss: 0.002690 Acc: 13.3125\n",
      " |~~ train@33728  Loss: 0.002283 Acc: 13.4531\n",
      " |~~ train@33792  Loss: 0.002961 Acc: 13.2500\n",
      " |~~ train@33856  Loss: 0.002963 Acc: 13.2500\n",
      " |~~ train@33920  Loss: 0.002166 Acc: 13.4844\n",
      " |~~ train@33984  Loss: 0.001911 Acc: 13.5312\n",
      " |~~ train@34048  Loss: 0.002923 Acc: 13.2500\n",
      " |~~ train@34112  Loss: 0.002668 Acc: 13.2969\n",
      " |~~ train@34176  Loss: 0.002355 Acc: 13.4219\n",
      " |~~ train@34240  Loss: 0.002865 Acc: 13.2031\n",
      " |~~ train@34304  Loss: 0.002772 Acc: 13.2031\n",
      " |~~ train@34368  Loss: 0.002949 Acc: 13.1094\n",
      " |~~ train@34432  Loss: 0.002238 Acc: 13.4219\n",
      " |~~ train@34496  Loss: 0.002869 Acc: 13.2812\n",
      " |~~ train@34560  Loss: 0.002685 Acc: 13.2656\n",
      " |~~ train@34624  Loss: 0.002593 Acc: 13.3438\n",
      " |~~ train@34688  Loss: 0.002799 Acc: 13.2656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@34752  Loss: 0.002070 Acc: 13.5312\n",
      " |~~ train@34816  Loss: 0.002962 Acc: 13.2188\n",
      " |~~ train@34880  Loss: 0.002740 Acc: 13.2969\n",
      " |~~ train@34944  Loss: 0.002272 Acc: 13.3906\n",
      " |~~ train@35008  Loss: 0.002660 Acc: 13.2812\n",
      " |~~ train@35072  Loss: 0.002186 Acc: 13.4844\n",
      " |~~ train@35136  Loss: 0.003058 Acc: 13.2344\n",
      " |~~ train@35200  Loss: 0.002460 Acc: 13.3438\n",
      " |~~ train@35264  Loss: 0.002845 Acc: 13.2188\n",
      " |~~ train@35328  Loss: 0.002775 Acc: 13.2031\n",
      " |~~ train@35392  Loss: 0.002507 Acc: 13.4062\n",
      " |~~ train@35456  Loss: 0.003053 Acc: 13.2031\n",
      " |~~ train@35520  Loss: 0.002842 Acc: 13.2812\n",
      " |~~ train@35584  Loss: 0.002843 Acc: 13.2812\n",
      " |~~ train@35648  Loss: 0.002518 Acc: 13.3281\n",
      " |~~ train@35712  Loss: 0.002796 Acc: 13.2031\n",
      " |~~ train@35776  Loss: 0.002914 Acc: 13.1875\n",
      " |~~ train@35840  Loss: 0.002330 Acc: 13.4531\n",
      " |~~ train@35904  Loss: 0.002830 Acc: 13.1719\n",
      " |~~ train@35968  Loss: 0.002275 Acc: 13.4844\n",
      " |~~ train@36032  Loss: 0.003402 Acc: 13.0156\n",
      " |~~ train@36096  Loss: 0.002603 Acc: 13.3281\n",
      " |~~ train@36160  Loss: 0.003281 Acc: 13.0938\n",
      " |~~ train@36224  Loss: 0.002622 Acc: 13.3125\n",
      " |~~ train@36288  Loss: 0.002657 Acc: 13.2969\n",
      " |~~ train@36352  Loss: 0.002662 Acc: 13.2500\n",
      " |~~ train@36416  Loss: 0.002604 Acc: 13.3438\n",
      " |~~ train@36480  Loss: 0.003056 Acc: 13.1719\n",
      " |~~ train@36544  Loss: 0.002176 Acc: 13.4844\n",
      " |~~ train@36608  Loss: 0.002860 Acc: 13.3125\n",
      " |~~ train@36672  Loss: 0.002679 Acc: 13.3594\n",
      " |~~ train@36736  Loss: 0.002479 Acc: 13.3438\n",
      " |~~ train@36800  Loss: 0.002469 Acc: 13.4219\n",
      " |~~ train@36864  Loss: 0.002813 Acc: 13.2344\n",
      " |~~ train@36928  Loss: 0.002718 Acc: 13.2500\n",
      " |~~ train@36992  Loss: 0.003562 Acc: 13.0469\n",
      " |~~ train@37056  Loss: 0.002996 Acc: 13.2031\n",
      " |~~ train@37120  Loss: 0.002830 Acc: 13.2656\n",
      " |~~ train@37184  Loss: 0.002925 Acc: 13.2031\n",
      " |~~ train@37248  Loss: 0.003014 Acc: 13.1719\n",
      " |~~ train@37312  Loss: 0.003557 Acc: 13.0312\n",
      " |~~ train@37376  Loss: 0.002421 Acc: 13.3750\n",
      " |~~ train@37440  Loss: 0.003332 Acc: 13.0781\n",
      " |~~ train@37504  Loss: 0.002957 Acc: 13.2656\n",
      " |~~ train@37568  Loss: 0.002556 Acc: 13.3125\n",
      " |~~ train@37632  Loss: 0.002787 Acc: 13.3750\n",
      " |~~ train@37696  Loss: 0.003113 Acc: 13.0781\n",
      " |~~ train@37760  Loss: 0.002744 Acc: 13.3281\n",
      " |~~ train@37824  Loss: 0.002366 Acc: 13.3906\n",
      " |~~ train@37888  Loss: 0.002422 Acc: 13.4219\n",
      " |~~ train@37952  Loss: 0.002959 Acc: 13.1719\n",
      " |~~ train@38016  Loss: 0.003092 Acc: 13.2031\n",
      " |~~ train@38080  Loss: 0.003106 Acc: 13.1719\n",
      " |~~ train@38144  Loss: 0.002153 Acc: 13.5312\n",
      " |~~ train@38208  Loss: 0.002378 Acc: 13.3750\n",
      " |~~ train@38272  Loss: 0.002819 Acc: 13.3281\n",
      " |~~ train@38336  Loss: 0.002772 Acc: 13.2500\n",
      " |~~ train@38400  Loss: 0.003118 Acc: 13.1250\n",
      " |~~ train@38464  Loss: 0.003177 Acc: 13.2188\n",
      " |~~ train@38528  Loss: 0.002801 Acc: 13.2812\n",
      " |~~ train@38592  Loss: 0.002686 Acc: 13.2969\n",
      " |~~ train@38656  Loss: 0.002624 Acc: 13.3281\n",
      " |~~ train@38720  Loss: 0.003371 Acc: 13.0781\n",
      " |~~ train@38784  Loss: 0.003032 Acc: 13.1875\n",
      " |~~ train@38848  Loss: 0.002778 Acc: 13.2656\n",
      " |~~ train@38912  Loss: 0.003494 Acc: 13.0000\n",
      " |~~ train@38976  Loss: 0.003000 Acc: 13.2188\n",
      " |~~ train@39040  Loss: 0.003000 Acc: 13.0625\n",
      " |~~ train@39104  Loss: 0.002398 Acc: 13.4219\n",
      " |~~ train@39168  Loss: 0.002531 Acc: 13.3125\n",
      " |~~ train@39232  Loss: 0.003149 Acc: 13.0625\n",
      " |~~ train@39296  Loss: 0.003090 Acc: 13.1406\n",
      " |~~ train@39360  Loss: 0.002470 Acc: 13.4219\n",
      " |~~ train@39424  Loss: 0.002325 Acc: 13.4219\n",
      " |~~ train@39488  Loss: 0.002679 Acc: 13.2812\n",
      " |~~ train@39552  Loss: 0.002932 Acc: 13.1562\n",
      " |~~ train@39616  Loss: 0.002342 Acc: 13.4688\n",
      " |~~ train@39680  Loss: 0.003136 Acc: 13.0938\n",
      " |~~ train@39744  Loss: 0.002944 Acc: 13.2812\n",
      " |~~ train@39808  Loss: 0.003077 Acc: 13.1719\n",
      " |~~ train@39872  Loss: 0.002460 Acc: 13.4062\n",
      " |~~ train@39936  Loss: 0.002581 Acc: 13.2812\n",
      " |~~ train@40000  Loss: 0.002890 Acc: 13.3125\n",
      " |~~ train@40064  Loss: 0.003075 Acc: 13.1094\n",
      " |~~ train@40128  Loss: 0.002805 Acc: 13.2188\n",
      " |~~ train@40192  Loss: 0.003232 Acc: 13.2031\n",
      " |~~ train@40256  Loss: 0.002146 Acc: 13.5469\n",
      " |~~ train@40320  Loss: 0.002597 Acc: 13.3125\n",
      " |~~ train@40384  Loss: 0.002286 Acc: 13.4531\n",
      " |~~ train@40448  Loss: 0.003137 Acc: 13.1562\n",
      " |~~ train@40512  Loss: 0.002724 Acc: 13.2344\n",
      " |~~ train@40576  Loss: 0.002681 Acc: 13.2812\n",
      " |~~ train@40640  Loss: 0.002871 Acc: 13.1875\n",
      " |~~ train@40704  Loss: 0.003229 Acc: 13.1562\n",
      " |~~ train@40768  Loss: 0.002443 Acc: 13.4219\n",
      " |~~ train@40832  Loss: 0.002711 Acc: 13.3281\n",
      " |~~ train@40896  Loss: 0.003535 Acc: 13.0469\n",
      " |~~ train@40960  Loss: 0.002406 Acc: 13.4062\n",
      " |~~ train@41024  Loss: 0.002795 Acc: 13.2969\n",
      " |~~ train@41088  Loss: 0.002015 Acc: 13.6094\n",
      " |~~ train@41152  Loss: 0.002879 Acc: 13.2812\n",
      " |~~ train@41216  Loss: 0.002946 Acc: 13.2344\n",
      " |~~ train@41280  Loss: 0.002993 Acc: 13.2031\n",
      " |~~ train@41344  Loss: 0.002552 Acc: 13.3438\n",
      " |~~ train@41408  Loss: 0.002963 Acc: 13.2656\n",
      " |~~ train@41472  Loss: 0.002892 Acc: 13.1719\n",
      " |~~ train@41536  Loss: 0.002768 Acc: 13.2656\n",
      " |~~ train@41600  Loss: 0.002092 Acc: 13.4688\n",
      " |~~ train@41664  Loss: 0.002415 Acc: 13.3125\n",
      " |~~ train@41728  Loss: 0.002918 Acc: 13.2031\n",
      " |~~ train@41792  Loss: 0.002422 Acc: 13.3750\n",
      " |~~ train@41856  Loss: 0.002787 Acc: 13.2656\n",
      " |~~ train@41920  Loss: 0.003022 Acc: 13.2031\n",
      " |~~ train@41984  Loss: 0.002739 Acc: 13.3125\n",
      " |~~ train@42048  Loss: 0.002992 Acc: 13.2188\n",
      " |~~ train@42112  Loss: 0.002617 Acc: 13.3750\n",
      " |~~ train@42176  Loss: 0.002478 Acc: 13.3750\n",
      " |~~ train@42240  Loss: 0.002985 Acc: 13.2812\n",
      " |~~ train@42304  Loss: 0.002475 Acc: 13.3906\n",
      " |~~ train@42368  Loss: 0.002968 Acc: 13.1094\n",
      " |~~ train@42432  Loss: 0.002635 Acc: 13.3750\n",
      " |~~ train@42496  Loss: 0.002427 Acc: 13.4688\n",
      " |~~ train@42560  Loss: 0.002489 Acc: 13.3594\n",
      " |~~ train@42624  Loss: 0.003025 Acc: 13.2188\n",
      " |~~ train@42688  Loss: 0.003372 Acc: 13.1719\n",
      " |~~ train@42752  Loss: 0.003209 Acc: 13.1719\n",
      " |~~ train@42816  Loss: 0.002788 Acc: 13.3125\n",
      " |~~ train@42880  Loss: 0.003028 Acc: 13.1875\n",
      " |~~ train@42944  Loss: 0.003355 Acc: 13.0625\n",
      " |~~ train@43008  Loss: 0.002437 Acc: 13.2969\n",
      " |~~ train@43072  Loss: 0.002619 Acc: 13.3594\n",
      " |~~ train@43136  Loss: 0.002626 Acc: 13.2656\n",
      " |~~ train@43200  Loss: 0.002490 Acc: 13.3906\n",
      " |~~ train@43264  Loss: 0.002782 Acc: 13.2969\n",
      " |~~ train@43328  Loss: 0.002887 Acc: 13.2500\n",
      " |~~ train@43392  Loss: 0.002385 Acc: 13.3281\n",
      " |~~ train@43456  Loss: 0.003143 Acc: 13.1250\n",
      " |~~ train@43520  Loss: 0.002817 Acc: 13.2188\n",
      " |~~ train@43584  Loss: 0.002600 Acc: 13.3125\n",
      " |~~ train@43648  Loss: 0.002750 Acc: 13.2969\n",
      " |~~ train@43712  Loss: 0.002199 Acc: 13.4531\n",
      " |~~ train@43776  Loss: 0.002751 Acc: 13.2969\n",
      " |~~ train@43840  Loss: 0.002693 Acc: 13.2969\n",
      " |~~ train@43904  Loss: 0.002895 Acc: 13.1406\n",
      " |~~ train@43968  Loss: 0.002787 Acc: 13.2188\n",
      " |~~ train@44032  Loss: 0.002691 Acc: 13.3750\n",
      " |~~ train@44096  Loss: 0.003358 Acc: 13.0469\n",
      " |~~ train@44160  Loss: 0.002604 Acc: 13.3438\n",
      " |~~ train@44224  Loss: 0.003519 Acc: 13.0938\n",
      " |~~ train@44288  Loss: 0.002362 Acc: 13.4375\n",
      " |~~ train@44352  Loss: 0.002259 Acc: 13.4375\n",
      " |~~ train@44416  Loss: 0.003261 Acc: 13.1406\n",
      " |~~ train@44480  Loss: 0.002613 Acc: 13.2812\n",
      " |~~ train@44544  Loss: 0.002711 Acc: 13.3281\n",
      " |~~ train@44608  Loss: 0.002770 Acc: 13.2969\n",
      " |~~ train@44672  Loss: 0.002932 Acc: 13.2188\n",
      " |~~ train@44736  Loss: 0.002989 Acc: 13.0938\n",
      " |~~ train@44800  Loss: 0.003183 Acc: 13.0938\n",
      " |~~ train@44864  Loss: 0.002533 Acc: 13.3281\n",
      " |~~ train@44928  Loss: 0.002720 Acc: 13.2812\n",
      " |~~ train@44992  Loss: 0.003104 Acc: 13.1875\n",
      " |~~ train@45056  Loss: 0.002396 Acc: 13.3906\n",
      " |~~ train@45120  Loss: 0.002768 Acc: 13.2500\n",
      " |~~ train@45184  Loss: 0.003094 Acc: 13.1719\n",
      " |~~ train@45248  Loss: 0.003038 Acc: 13.2656\n",
      " |~~ train@45312  Loss: 0.002855 Acc: 13.2031\n",
      " |~~ train@45376  Loss: 0.003400 Acc: 13.0312\n",
      " |~~ train@45440  Loss: 0.002571 Acc: 13.3906\n",
      " |~~ train@45504  Loss: 0.002736 Acc: 13.2969\n",
      " |~~ train@45568  Loss: 0.002377 Acc: 13.4062\n",
      " |~~ train@45632  Loss: 0.002714 Acc: 13.3281\n",
      " |~~ train@45696  Loss: 0.002970 Acc: 13.2031\n",
      " |~~ train@45760  Loss: 0.003268 Acc: 13.1562\n",
      " |~~ train@45824  Loss: 0.002940 Acc: 13.1875\n",
      " |~~ train@45888  Loss: 0.002403 Acc: 13.4375\n",
      " |~~ train@45952  Loss: 0.002928 Acc: 13.1719\n",
      " |~~ train@46016  Loss: 0.002844 Acc: 13.1875\n",
      " |~~ train@46080  Loss: 0.002310 Acc: 13.3906\n",
      " |~~ train@46144  Loss: 0.002405 Acc: 13.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@46208  Loss: 0.003442 Acc: 13.0625\n",
      " |~~ train@46272  Loss: 0.003063 Acc: 13.2031\n",
      " |~~ train@46336  Loss: 0.003308 Acc: 13.1094\n",
      " |~~ train@46400  Loss: 0.002494 Acc: 13.4375\n",
      " |~~ train@46464  Loss: 0.002983 Acc: 13.2031\n",
      " |~~ train@46528  Loss: 0.002407 Acc: 13.3438\n",
      " |~~ train@46592  Loss: 0.002821 Acc: 13.2812\n",
      " |~~ train@46656  Loss: 0.002755 Acc: 13.2812\n",
      " |~~ train@46720  Loss: 0.002084 Acc: 13.4844\n",
      " |~~ train@46784  Loss: 0.003080 Acc: 13.2188\n",
      " |~~ train@46848  Loss: 0.002419 Acc: 13.3906\n",
      " |~~ train@46912  Loss: 0.002331 Acc: 13.4062\n",
      " |~~ train@46976  Loss: 0.002897 Acc: 13.2188\n",
      " |~~ train@47040  Loss: 0.002764 Acc: 13.2656\n",
      " |~~ train@47104  Loss: 0.002119 Acc: 13.4844\n",
      " |~~ train@47168  Loss: 0.002560 Acc: 13.3594\n",
      " |~~ train@47232  Loss: 0.002174 Acc: 13.5156\n",
      " |~~ train@47296  Loss: 0.002848 Acc: 13.2500\n",
      " |~~ train@47360  Loss: 0.002955 Acc: 13.2188\n",
      " |~~ train@47424  Loss: 0.002848 Acc: 13.2188\n",
      " |~~ train@47488  Loss: 0.002451 Acc: 13.3906\n",
      " |~~ train@47552  Loss: 0.002156 Acc: 13.5000\n",
      " |~~ train@47616  Loss: 0.002474 Acc: 13.4375\n",
      " |~~ train@47680  Loss: 0.003213 Acc: 13.0625\n",
      " |~~ train@47744  Loss: 0.002969 Acc: 13.2031\n",
      " |~~ train@47808  Loss: 0.003187 Acc: 13.1094\n",
      " |~~ train@47872  Loss: 0.002427 Acc: 13.4219\n",
      " |~~ train@47936  Loss: 0.002807 Acc: 13.2500\n",
      " |~~ train@48000  Loss: 0.002473 Acc: 13.3906\n",
      " |~~ train@48064  Loss: 0.002737 Acc: 13.2969\n",
      " |~~ train@48128  Loss: 0.002746 Acc: 13.2500\n",
      " |~~ train@48192  Loss: 0.002651 Acc: 13.3750\n",
      " |~~ train@48256  Loss: 0.002600 Acc: 13.2812\n",
      " |~~ train@48320  Loss: 0.002282 Acc: 13.4844\n",
      " |~~ train@48384  Loss: 0.002638 Acc: 13.3281\n",
      " |~~ train@48448  Loss: 0.002258 Acc: 13.3594\n",
      " |~~ train@48512  Loss: 0.002851 Acc: 13.2812\n",
      " |~~ train@48576  Loss: 0.002720 Acc: 13.3125\n",
      " |~~ train@48640  Loss: 0.002221 Acc: 13.5000\n",
      " |~~ train@48704  Loss: 0.002389 Acc: 13.3438\n",
      " |~~ train@48768  Loss: 0.002286 Acc: 13.4688\n",
      " |~~ train@48832  Loss: 0.003193 Acc: 13.0781\n",
      " |~~ train@48896  Loss: 0.002586 Acc: 13.3281\n",
      " |~~ train@48960  Loss: 0.003106 Acc: 13.1406\n",
      " |~~ train@49024  Loss: 0.002782 Acc: 13.2500\n",
      " |~~ train@49088  Loss: 0.002992 Acc: 13.2344\n",
      " |~~ train@49152  Loss: 0.002958 Acc: 13.2344\n",
      " |~~ train@49216  Loss: 0.002423 Acc: 13.4062\n",
      " |~~ train@49280  Loss: 0.002995 Acc: 13.1875\n",
      " |~~ train@49344  Loss: 0.003026 Acc: 13.1875\n",
      " |~~ train@49408  Loss: 0.003191 Acc: 13.1250\n",
      " |~~ train@49472  Loss: 0.002410 Acc: 13.3906\n",
      " |~~ train@49536  Loss: 0.002552 Acc: 13.3438\n",
      " |~~ train@49600  Loss: 0.003113 Acc: 13.1875\n",
      " |~~ train@49664  Loss: 0.002743 Acc: 13.3438\n",
      " |~~ train@49728  Loss: 0.003199 Acc: 13.1875\n",
      " |~~ train@49792  Loss: 0.003312 Acc: 13.1250\n",
      " |~~ train@49856  Loss: 0.002614 Acc: 13.2812\n",
      " |~~ train@49920  Loss: 0.003088 Acc: 13.0781\n",
      " |~~ train@49984  Loss: 0.002975 Acc: 13.1719\n",
      " |~~ train@50048  Loss: 0.002536 Acc: 13.3750\n",
      " |~~ train@50112  Loss: 0.003269 Acc: 13.0938\n",
      " |~~ train@50176  Loss: 0.003226 Acc: 13.1719\n",
      " |~~ train@50240  Loss: 0.002380 Acc: 13.4062\n",
      " |~~ train@50304  Loss: 0.002862 Acc: 13.2500\n",
      " |~~ train@50368  Loss: 0.002512 Acc: 13.3438\n",
      " |~~ train@50432  Loss: 0.003202 Acc: 13.0938\n",
      " |~~ train@50496  Loss: 0.002642 Acc: 13.2656\n",
      " |~~ train@50560  Loss: 0.002976 Acc: 13.2031\n",
      " |~~ train@50624  Loss: 0.003694 Acc: 12.8750\n",
      " |~~ train@50688  Loss: 0.002918 Acc: 13.1562\n",
      " |~~ train@50752  Loss: 0.003104 Acc: 13.2031\n",
      " |~~ train@50816  Loss: 0.002718 Acc: 13.2812\n",
      " |~~ train@50880  Loss: 0.002833 Acc: 13.2656\n",
      " |~~ train@50944  Loss: 0.003345 Acc: 13.1094\n",
      " |~~ train@51008  Loss: 0.003050 Acc: 13.2188\n",
      " |~~ train@51072  Loss: 0.003227 Acc: 13.1250\n",
      " |~~ train@51136  Loss: 0.002424 Acc: 13.3750\n",
      " |~~ train@51200  Loss: 0.003218 Acc: 13.0938\n",
      " |~~ train@51264  Loss: 0.002623 Acc: 13.3281\n",
      " |~~ train@51328  Loss: 0.003150 Acc: 13.0469\n",
      " |~~ train@51392  Loss: 0.002625 Acc: 13.3281\n",
      " |~~ train@51456  Loss: 0.002701 Acc: 13.2031\n",
      " |~~ train@51520  Loss: 0.002602 Acc: 13.3281\n",
      " |~~ train@51584  Loss: 0.002957 Acc: 13.2500\n",
      " |~~ train@51648  Loss: 0.002821 Acc: 13.3125\n",
      " |~~ train@51712  Loss: 0.002433 Acc: 13.3594\n",
      " |~~ train@51776  Loss: 0.003431 Acc: 13.0312\n",
      " |~~ train@51840  Loss: 0.002742 Acc: 13.2344\n",
      " |~~ train@51904  Loss: 0.002498 Acc: 13.3281\n",
      " |~~ train@51968  Loss: 0.002906 Acc: 13.2812\n",
      " |~~ train@52032  Loss: 0.002920 Acc: 13.2188\n",
      " |~~ train@52096  Loss: 0.002817 Acc: 13.3125\n",
      " |~~ train@52160  Loss: 0.002873 Acc: 13.2031\n",
      " |~~ train@52224  Loss: 0.002464 Acc: 13.2969\n",
      " |~~ train@52288  Loss: 0.002652 Acc: 13.3281\n",
      " |~~ train@52352  Loss: 0.002660 Acc: 13.3594\n",
      " |~~ train@52416  Loss: 0.002315 Acc: 13.4531\n",
      " |~~ train@52480  Loss: 0.002381 Acc: 13.3750\n",
      " |~~ train@52544  Loss: 0.002179 Acc: 13.5000\n",
      " |~~ train@52608  Loss: 0.002954 Acc: 13.2344\n",
      " |~~ train@52672  Loss: 0.002269 Acc: 13.4688\n",
      " |~~ train@52736  Loss: 0.002804 Acc: 13.3125\n",
      " |~~ train@52800  Loss: 0.002977 Acc: 13.1875\n",
      " |~~ train@52864  Loss: 0.003230 Acc: 13.1094\n",
      " |~~ train@52928  Loss: 0.002783 Acc: 13.3438\n",
      " |~~ train@52992  Loss: 0.002197 Acc: 13.4531\n",
      " |~~ train@53056  Loss: 0.002781 Acc: 13.2344\n",
      " |~~ train@53120  Loss: 0.003138 Acc: 13.1562\n",
      " |~~ train@53184  Loss: 0.002794 Acc: 13.2188\n",
      " |~~ train@53248  Loss: 0.003216 Acc: 13.1094\n",
      " |~~ train@53312  Loss: 0.002455 Acc: 13.3281\n",
      " |~~ train@53376  Loss: 0.003148 Acc: 13.1094\n",
      " |~~ train@53440  Loss: 0.002792 Acc: 13.2812\n",
      " |~~ train@53504  Loss: 0.002713 Acc: 13.2969\n",
      " |~~ train@53568  Loss: 0.002391 Acc: 13.4219\n",
      " |~~ train@53632  Loss: 0.002740 Acc: 13.2812\n",
      " |~~ train@53696  Loss: 0.003364 Acc: 13.1094\n",
      " |~~ train@53760  Loss: 0.002827 Acc: 13.2812\n",
      " |~~ train@53824  Loss: 0.002708 Acc: 13.3438\n",
      " |~~ train@53888  Loss: 0.002594 Acc: 13.3438\n",
      " |~~ train@53952  Loss: 0.002843 Acc: 13.2500\n",
      " |~~ train@54016  Loss: 0.003003 Acc: 13.1094\n",
      " |~~ train@54080  Loss: 0.003079 Acc: 13.1875\n",
      " |~~ train@54144  Loss: 0.001944 Acc: 13.4844\n",
      " |~~ train@54208  Loss: 0.002705 Acc: 13.2812\n",
      " |~~ train@54272  Loss: 0.002204 Acc: 13.4375\n",
      " |~~ train@54336  Loss: 0.002977 Acc: 13.1562\n",
      " |~~ train@54400  Loss: 0.002388 Acc: 13.4688\n",
      " |~~ train@54464  Loss: 0.003061 Acc: 13.2188\n",
      " |~~ train@54528  Loss: 0.002858 Acc: 13.2188\n",
      " |~~ train@54592  Loss: 0.003473 Acc: 13.0000\n",
      " |~~ train@54656  Loss: 0.002798 Acc: 13.2500\n",
      " |~~ train@54720  Loss: 0.002407 Acc: 13.4375\n",
      " |~~ train@54784  Loss: 0.002844 Acc: 13.2344\n",
      " |~~ train@54848  Loss: 0.002845 Acc: 13.2344\n",
      " |~~ train@54912  Loss: 0.003280 Acc: 13.0781\n",
      " |~~ train@54976  Loss: 0.002280 Acc: 13.4375\n",
      " |~~ train@55040  Loss: 0.002397 Acc: 13.4219\n",
      " |~~ train@55104  Loss: 0.002813 Acc: 13.2344\n",
      " |~~ train@55168  Loss: 0.002643 Acc: 13.2969\n",
      " |~~ train@55232  Loss: 0.002532 Acc: 13.3281\n",
      " |~~ train@55296  Loss: 0.002204 Acc: 13.4375\n",
      " |~~ train@55360  Loss: 0.002617 Acc: 13.3125\n",
      " |~~ train@55424  Loss: 0.002617 Acc: 13.2656\n",
      " |~~ train@55488  Loss: 0.002850 Acc: 13.2344\n",
      " |~~ train@55552  Loss: 0.002760 Acc: 13.2969\n",
      " |~~ train@55616  Loss: 0.002646 Acc: 13.2969\n",
      " |~~ train@55680  Loss: 0.002661 Acc: 13.3125\n",
      " |~~ train@55744  Loss: 0.002691 Acc: 13.2812\n",
      " |~~ train@55808  Loss: 0.002223 Acc: 13.4688\n",
      " |~~ train@55872  Loss: 0.002846 Acc: 13.2344\n",
      " |~~ train@55936  Loss: 0.003203 Acc: 13.1719\n",
      " |~~ train@56000  Loss: 0.003016 Acc: 13.1562\n",
      " |~~ train@56064  Loss: 0.002839 Acc: 13.2344\n",
      " |~~ train@56128  Loss: 0.003108 Acc: 13.1406\n",
      " |~~ train@56192  Loss: 0.002989 Acc: 13.1875\n",
      " |~~ train@56256  Loss: 0.002410 Acc: 13.4062\n",
      " |~~ train@56320  Loss: 0.002473 Acc: 13.3281\n",
      " |~~ train@56384  Loss: 0.003205 Acc: 13.1094\n",
      " |~~ train@56448  Loss: 0.002932 Acc: 13.2031\n",
      " |~~ train@56512  Loss: 0.002921 Acc: 13.3125\n",
      " |~~ train@56576  Loss: 0.002901 Acc: 13.2188\n",
      " |~~ train@56640  Loss: 0.001940 Acc: 13.5156\n",
      " |~~ train@56704  Loss: 0.002511 Acc: 13.3281\n",
      " |~~ train@56768  Loss: 0.003076 Acc: 13.2031\n",
      " |~~ train@56832  Loss: 0.003178 Acc: 13.0938\n",
      " |~~ train@56896  Loss: 0.002649 Acc: 13.3125\n",
      " |~~ train@56960  Loss: 0.002264 Acc: 13.4062\n",
      " |~~ train@57024  Loss: 0.003078 Acc: 13.1094\n",
      " |~~ train@57088  Loss: 0.002425 Acc: 13.3750\n",
      " |~~ train@57152  Loss: 0.003171 Acc: 13.1562\n",
      " |~~ train@57216  Loss: 0.002199 Acc: 13.5000\n",
      " |~~ train@57280  Loss: 0.002525 Acc: 13.3906\n",
      " |~~ train@57344  Loss: 0.002303 Acc: 13.4219\n",
      " |~~ train@57408  Loss: 0.002679 Acc: 13.1562\n",
      " |~~ train@57472  Loss: 0.002764 Acc: 13.2969\n",
      " |~~ train@57536  Loss: 0.003013 Acc: 13.1562\n",
      " |~~ train@57600  Loss: 0.002811 Acc: 13.2812\n",
      " |~~ train@57664  Loss: 0.002547 Acc: 13.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@57728  Loss: 0.002769 Acc: 13.2188\n",
      " |~~ train@57792  Loss: 0.002607 Acc: 13.2031\n",
      " |~~ train@57856  Loss: 0.002971 Acc: 13.2500\n",
      " |~~ train@57920  Loss: 0.002846 Acc: 13.2500\n",
      " |~~ train@57984  Loss: 0.002744 Acc: 13.2969\n",
      " |~~ train@58048  Loss: 0.002958 Acc: 13.2031\n",
      " |~~ train@58112  Loss: 0.003361 Acc: 13.0781\n",
      " |~~ train@58176  Loss: 0.002194 Acc: 13.4531\n",
      " |~~ train@58240  Loss: 0.002592 Acc: 13.3438\n",
      " |~~ train@58304  Loss: 0.002927 Acc: 13.2344\n",
      " |~~ train@58368  Loss: 0.002907 Acc: 13.2188\n",
      " |~~ train@58432  Loss: 0.002797 Acc: 13.3438\n",
      " |~~ train@58496  Loss: 0.002835 Acc: 13.2344\n",
      " |~~ train@58560  Loss: 0.003050 Acc: 13.1250\n",
      " |~~ train@58624  Loss: 0.003145 Acc: 13.1562\n",
      " |~~ train@58688  Loss: 0.002638 Acc: 13.2969\n",
      " |~~ train@58752  Loss: 0.002500 Acc: 13.3281\n",
      " |~~ train@58816  Loss: 0.002397 Acc: 13.3906\n",
      " |~~ train@58880  Loss: 0.002370 Acc: 13.3750\n",
      " |~~ train@58944  Loss: 0.002449 Acc: 13.3281\n",
      " |~~ train@59008  Loss: 0.002793 Acc: 13.2812\n",
      " |~~ train@59072  Loss: 0.003323 Acc: 13.0312\n",
      " |~~ train@59136  Loss: 0.002408 Acc: 13.4062\n",
      " |~~ train@59200  Loss: 0.003000 Acc: 13.1719\n",
      " |~~ train@59264  Loss: 0.002403 Acc: 13.3438\n",
      " |~~ train@59328  Loss: 0.002177 Acc: 13.5000\n",
      " |~~ train@59392  Loss: 0.003070 Acc: 13.1250\n",
      " |~~ train@59456  Loss: 0.002898 Acc: 13.2031\n",
      " |~~ train@59520  Loss: 0.002151 Acc: 13.4219\n",
      " |~~ train@59584  Loss: 0.002929 Acc: 13.1875\n",
      " |~~ train@59648  Loss: 0.003070 Acc: 13.1250\n",
      " |~~ train@59712  Loss: 0.002886 Acc: 13.1562\n",
      " |~~ train@59776  Loss: 0.002672 Acc: 13.3594\n",
      " |~~ train@59840  Loss: 0.002829 Acc: 13.1875\n",
      " |~~ train@59904  Loss: 0.002830 Acc: 13.1719\n",
      " |~~ train@59968  Loss: 0.003270 Acc: 13.0938\n",
      " |~~ train@60032  Loss: 0.003073 Acc: 13.2188\n",
      " |~~ train@60096  Loss: 0.002340 Acc: 13.4688\n",
      " |~~ train@60160  Loss: 0.002061 Acc: 13.5156\n",
      " |~~ train@60224  Loss: 0.002762 Acc: 13.2969\n",
      " |~~ train@60288  Loss: 0.003522 Acc: 13.0156\n",
      " |~~ train@60352  Loss: 0.003370 Acc: 13.0156\n",
      " |~~ train@60416  Loss: 0.002806 Acc: 13.2344\n",
      " |~~ train@60480  Loss: 0.002360 Acc: 13.4531\n",
      " |~~ train@60544  Loss: 0.002544 Acc: 13.4219\n",
      " |~~ train@60608  Loss: 0.002316 Acc: 13.3750\n",
      " |~~ train@60672  Loss: 0.002766 Acc: 13.3281\n",
      " |~~ train@60736  Loss: 0.002227 Acc: 13.4844\n",
      " |~~ train@60800  Loss: 0.003342 Acc: 12.9844\n",
      " |~~ train@60864  Loss: 0.002421 Acc: 13.3594\n",
      " |~~ train@60928  Loss: 0.002963 Acc: 13.2188\n",
      " |~~ train@60992  Loss: 0.002277 Acc: 13.3906\n",
      " |~~ train@61056  Loss: 0.003112 Acc: 13.0938\n",
      " |~~ train@61120  Loss: 0.003111 Acc: 13.0938\n",
      " |~~ train@61184  Loss: 0.002799 Acc: 13.2812\n",
      " |~~ train@61248  Loss: 0.002284 Acc: 13.4531\n",
      " |~~ train@61312  Loss: 0.002579 Acc: 13.3125\n",
      " |~~ train@61376  Loss: 0.002771 Acc: 13.3125\n",
      " |~~ train@61440  Loss: 0.002672 Acc: 13.2500\n",
      " |~~ train@61504  Loss: 0.002825 Acc: 13.1406\n",
      " |~~ train@61568  Loss: 0.002527 Acc: 13.3125\n",
      " |~~ train@61632  Loss: 0.002835 Acc: 13.2344\n",
      " |~~ train@61696  Loss: 0.002802 Acc: 13.3125\n",
      " |~~ train@61760  Loss: 0.002592 Acc: 13.2969\n",
      " |~~ train@61824  Loss: 0.002380 Acc: 13.3906\n",
      " |~~ train@61888  Loss: 0.003085 Acc: 13.1875\n",
      " |~~ train@61952  Loss: 0.002661 Acc: 13.2812\n",
      " |~~ train@62016  Loss: 0.002541 Acc: 13.3438\n",
      " |~~ train@62080  Loss: 0.002439 Acc: 13.4219\n",
      " |~~ train@62144  Loss: 0.002580 Acc: 13.3125\n",
      " |~~ train@62208  Loss: 0.002903 Acc: 13.2031\n",
      " |~~ train@62272  Loss: 0.002453 Acc: 13.4062\n",
      " |~~ train@62336  Loss: 0.001997 Acc: 13.5469\n",
      " |~~ train@62400  Loss: 0.003028 Acc: 13.2344\n",
      " |~~ train@62464  Loss: 0.002449 Acc: 13.3281\n",
      " |~~ train@62528  Loss: 0.002413 Acc: 13.4062\n",
      " |~~ train@62592  Loss: 0.002959 Acc: 13.1875\n",
      " |~~ train@62656  Loss: 0.002862 Acc: 13.2031\n",
      " |~~ train@62720  Loss: 0.002394 Acc: 13.3281\n",
      " |~~ train@62784  Loss: 0.002422 Acc: 13.3281\n",
      " |~~ train@62848  Loss: 0.002783 Acc: 13.2656\n",
      " |~~ train@62912  Loss: 0.002661 Acc: 13.3125\n",
      " |~~ train@62976  Loss: 0.002260 Acc: 13.4375\n",
      " |~~ train@63040  Loss: 0.002944 Acc: 13.1094\n",
      " |~~ train@63104  Loss: 0.002923 Acc: 13.2031\n",
      " |~~ train@63168  Loss: 0.003276 Acc: 13.0781\n",
      " |~~ train@63232  Loss: 0.002448 Acc: 13.3906\n",
      " |~~ train@63296  Loss: 0.003114 Acc: 13.1719\n",
      " |~~ train@63360  Loss: 0.002611 Acc: 13.3438\n",
      " |~~ train@63424  Loss: 0.002515 Acc: 13.3438\n",
      " |~~ train@63488  Loss: 0.002566 Acc: 13.2969\n",
      " |~~ train@63552  Loss: 0.002796 Acc: 13.2812\n",
      " |~~ train@63616  Loss: 0.002575 Acc: 13.3594\n",
      " |~~ train@63680  Loss: 0.003089 Acc: 13.1406\n",
      " |~~ train@63744  Loss: 0.002831 Acc: 13.3125\n",
      " |~~ train@63808  Loss: 0.002921 Acc: 13.2188\n",
      " |~~ train@63872  Loss: 0.002584 Acc: 13.3438\n",
      " |~~ train@63936  Loss: 0.002808 Acc: 13.2031\n",
      " |~~ train@64000  Loss: 0.002094 Acc: 13.4219\n",
      " |~~ train@64064  Loss: 0.002149 Acc: 13.4219\n",
      " |~~ train@64128  Loss: 0.002597 Acc: 13.2500\n",
      " |~~ train@64192  Loss: 0.002216 Acc: 13.4219\n",
      " |~~ train@64256  Loss: 0.002146 Acc: 13.4531\n",
      " |~~ train@64320  Loss: 0.002671 Acc: 13.3281\n",
      " |~~ train@64384  Loss: 0.002842 Acc: 13.1875\n",
      " |~~ train@64448  Loss: 0.002539 Acc: 13.3750\n",
      " |~~ train@64512  Loss: 0.003296 Acc: 13.1250\n",
      " |~~ train@64576  Loss: 0.002561 Acc: 13.3125\n",
      " |~~ train@64640  Loss: 0.002929 Acc: 13.2188\n",
      " |~~ train@64704  Loss: 0.002879 Acc: 13.1562\n",
      " |~~ train@64768  Loss: 0.003166 Acc: 13.1562\n",
      " |~~ train@64832  Loss: 0.002321 Acc: 13.4062\n",
      " |~~ train@64896  Loss: 0.002738 Acc: 13.1562\n",
      " |~~ train@64960  Loss: 0.002768 Acc: 13.1406\n",
      " |~~ train@65024  Loss: 0.002268 Acc: 13.4062\n",
      " |~~ train@65088  Loss: 0.002320 Acc: 13.4844\n",
      " |~~ train@65152  Loss: 0.002178 Acc: 13.3281\n",
      " |~~ train@65216  Loss: 0.002187 Acc: 13.4219\n",
      " |~~ train@65280  Loss: 0.002585 Acc: 13.3281\n",
      " |~~ train@65344  Loss: 0.002249 Acc: 13.4844\n",
      " |~~ train@65408  Loss: 0.002289 Acc: 13.4688\n",
      " |~~ train@65472  Loss: 0.003164 Acc: 13.0625\n",
      " |~~ train@65536  Loss: 0.002891 Acc: 13.2031\n",
      " |~~ train@65600  Loss: 0.002766 Acc: 13.2500\n",
      " |~~ train@65664  Loss: 0.003190 Acc: 13.1875\n",
      " |~~ train@65728  Loss: 0.003399 Acc: 13.0938\n",
      " |~~ train@65792  Loss: 0.002985 Acc: 13.2188\n",
      " |~~ train@65856  Loss: 0.002947 Acc: 13.2031\n",
      " |~~ train@65920  Loss: 0.002723 Acc: 13.2344\n",
      " |~~ train@65984  Loss: 0.002925 Acc: 13.2344\n",
      " |~~ train@66048  Loss: 0.003166 Acc: 13.0781\n",
      " |~~ train@66112  Loss: 0.002728 Acc: 13.3125\n",
      " |~~ train@66176  Loss: 0.002540 Acc: 13.3594\n",
      " |~~ train@66240  Loss: 0.002961 Acc: 13.2188\n",
      " |~~ train@66304  Loss: 0.002857 Acc: 13.1875\n",
      " |~~ train@66368  Loss: 0.003228 Acc: 13.0938\n",
      " |~~ train@66432  Loss: 0.002432 Acc: 13.4062\n",
      " |~~ train@66496  Loss: 0.002813 Acc: 13.2344\n",
      " |~~ train@66560  Loss: 0.002865 Acc: 13.2031\n",
      " |~~ train@66624  Loss: 0.002589 Acc: 13.2500\n",
      " |~~ train@66688  Loss: 0.002779 Acc: 13.2500\n",
      " |~~ train@66752  Loss: 0.003087 Acc: 13.0938\n",
      " |~~ train@66816  Loss: 0.003194 Acc: 13.1406\n",
      " |~~ train@66880  Loss: 0.002760 Acc: 13.2500\n",
      " |~~ train@66944  Loss: 0.003070 Acc: 13.1406\n",
      " |~~ train@67008  Loss: 0.003075 Acc: 13.1250\n",
      " |~~ train@67072  Loss: 0.002662 Acc: 13.2500\n",
      " |~~ train@67136  Loss: 0.002437 Acc: 13.3281\n",
      " |~~ train@67200  Loss: 0.002057 Acc: 13.5625\n",
      " |~~ train@67264  Loss: 0.002771 Acc: 13.2656\n",
      " |~~ train@67328  Loss: 0.002835 Acc: 13.3125\n",
      " |~~ train@67392  Loss: 0.002344 Acc: 13.3750\n",
      " |~~ train@67456  Loss: 0.002135 Acc: 13.4844\n",
      " |~~ train@67520  Loss: 0.002663 Acc: 13.3438\n",
      " |~~ train@67584  Loss: 0.002580 Acc: 13.3438\n",
      " |~~ train@67648  Loss: 0.002014 Acc: 13.5469\n",
      " |~~ train@67712  Loss: 0.002201 Acc: 13.5156\n",
      " |~~ train@67776  Loss: 0.002250 Acc: 13.4219\n",
      " |~~ train@67840  Loss: 0.002585 Acc: 13.2656\n",
      " |~~ train@67904  Loss: 0.003362 Acc: 13.1094\n",
      " |~~ train@67968  Loss: 0.002888 Acc: 13.1562\n",
      " |~~ train@68032  Loss: 0.002974 Acc: 13.1875\n",
      " |~~ train@68096  Loss: 0.002460 Acc: 13.3125\n",
      " |~~ train@68160  Loss: 0.002396 Acc: 13.3594\n",
      " |~~ train@68224  Loss: 0.002502 Acc: 13.3281\n",
      " |~~ train@68288  Loss: 0.002243 Acc: 13.4219\n",
      " |~~ train@68352  Loss: 0.003218 Acc: 13.1562\n",
      " |~~ train@68416  Loss: 0.002916 Acc: 13.1875\n",
      " |~~ train@68480  Loss: 0.002988 Acc: 13.2500\n",
      " |~~ train@68544  Loss: 0.003040 Acc: 13.1719\n",
      " |~~ train@68608  Loss: 0.003096 Acc: 13.1719\n",
      " |~~ train@68672  Loss: 0.002749 Acc: 13.2969\n",
      " |~~ train@68736  Loss: 0.002474 Acc: 13.3750\n",
      " |~~ train@68800  Loss: 0.002816 Acc: 13.2031\n",
      " |~~ train@68864  Loss: 0.003191 Acc: 13.1250\n",
      " |~~ train@68928  Loss: 0.002734 Acc: 13.3438\n",
      " |~~ train@68992  Loss: 0.002745 Acc: 13.2500\n",
      " |~~ train@69056  Loss: 0.002433 Acc: 13.4219\n",
      " |~~ train@69120  Loss: 0.002637 Acc: 13.3281\n",
      " |~~ train@69184  Loss: 0.003068 Acc: 13.0938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ train@69248  Loss: 0.002459 Acc: 13.3125\n",
      " |~~ train@69312  Loss: 0.002961 Acc: 13.2031\n",
      " |~~ train@69376  Loss: 0.002897 Acc: 13.2031\n",
      " |~~ train@69440  Loss: 0.003031 Acc: 13.1875\n",
      " |~~ train@69504  Loss: 0.002810 Acc: 13.2812\n",
      " |~~ train@69568  Loss: 0.002474 Acc: 13.3281\n",
      " |~~ train@69632  Loss: 0.002386 Acc: 13.3750\n",
      " |~~ train@69696  Loss: 0.002955 Acc: 13.1094\n",
      " |~~ train@69760  Loss: 0.002445 Acc: 13.3906\n",
      " |~~ train@69824  Loss: 0.002939 Acc: 13.2188\n",
      " |~~ train@69888  Loss: 0.002832 Acc: 13.2812\n",
      " |~~ train@69952  Loss: 0.003327 Acc: 13.0469\n",
      " |~~ train@70016  Loss: 0.003065 Acc: 13.1250\n",
      " |~~ train@70080  Loss: 0.002476 Acc: 13.2969\n",
      " |~~ train@70144  Loss: 0.002855 Acc: 13.1250\n",
      " |~~ train@70208  Loss: 0.002303 Acc: 13.3438\n",
      " |~~ train@70272  Loss: 0.002641 Acc: 13.2500\n",
      " |~~ train@70336  Loss: 0.002994 Acc: 13.1094\n",
      " |~~ train@70400  Loss: 0.002655 Acc: 13.2812\n",
      " |~~ train@70464  Loss: 0.002868 Acc: 13.1719\n",
      " |~~ train@70528  Loss: 0.002871 Acc: 13.2656\n",
      " |~~ train@70592  Loss: 0.002936 Acc: 13.1562\n",
      " |~~ train@70656  Loss: 0.002320 Acc: 13.3750\n",
      " |~~ train@70720  Loss: 0.002533 Acc: 13.3438\n",
      " |~~ train@70784  Loss: 0.002701 Acc: 13.3125\n",
      " |~~ train@70848  Loss: 0.002418 Acc: 13.2969\n",
      " |~~ train@70912  Loss: 0.003037 Acc: 13.1406\n",
      " |~~ train@70976  Loss: 0.002817 Acc: 13.2344\n",
      " |~~ train@71040  Loss: 0.002706 Acc: 13.3281\n",
      " |~~ train@71104  Loss: 0.002218 Acc: 13.4688\n",
      " |~~ train@71168  Loss: 0.003036 Acc: 13.1250\n",
      " |~~ train@71232  Loss: 0.003390 Acc: 13.0312\n",
      " |~~ train@71296  Loss: 0.002895 Acc: 13.2188\n",
      " |~~ train@71360  Loss: 0.002015 Acc: 13.4844\n",
      " |~~ train@71424  Loss: 0.002717 Acc: 13.3281\n",
      " |~~ train@71488  Loss: 0.002778 Acc: 13.3438\n",
      " |~~ train@71552  Loss: 0.002436 Acc: 13.3750\n",
      " |~~ train@71616  Loss: 0.002710 Acc: 13.3281\n",
      " |~~ train@71680  Loss: 0.002682 Acc: 13.3594\n",
      " |~~ train@71744  Loss: 0.002887 Acc: 13.2656\n",
      " |~~ train@71808  Loss: 0.002627 Acc: 13.2812\n",
      " |~~ train@71872  Loss: 0.002227 Acc: 13.4688\n",
      " |~~ train@71936  Loss: 0.002198 Acc: 13.4688\n",
      " |~~ train@72000  Loss: 0.003036 Acc: 13.1875\n",
      " |~~ train@72064  Loss: 0.002132 Acc: 13.4844\n",
      " |~~ train@72128  Loss: 0.002591 Acc: 13.2812\n",
      " |~~ train@72192  Loss: 0.002607 Acc: 13.3281\n",
      " |~~ train@72256  Loss: 0.002460 Acc: 13.3125\n",
      " |~~ train@72320  Loss: 0.003475 Acc: 13.0625\n",
      " |~~ train@72384  Loss: 0.002103 Acc: 13.4531\n",
      " |~~ train@72448  Loss: 0.003521 Acc: 13.0000\n",
      " |~~ train@72512  Loss: 0.003729 Acc: 12.9688\n",
      " |~~ train@72576  Loss: 0.002376 Acc: 13.4219\n",
      " |~~ train@72640  Loss: 0.002642 Acc: 13.3281\n",
      " |~~ train@72704  Loss: 0.002644 Acc: 13.3125\n",
      " |~~ train@72768  Loss: 0.002905 Acc: 13.1875\n",
      " |~~ train@72832  Loss: 0.002157 Acc: 13.4375\n",
      " |~~ train@72896  Loss: 0.002232 Acc: 13.5000\n",
      " |~~ train@72960  Loss: 0.002491 Acc: 13.3906\n",
      " |~~ train@73024  Loss: 0.002739 Acc: 13.2500\n",
      " |~~ train@73088  Loss: 0.002776 Acc: 13.2188\n",
      " |~~ train@73152  Loss: 0.002196 Acc: 13.4531\n",
      " |~~ train@73216  Loss: 0.002483 Acc: 13.3438\n",
      " |~~ train@73280  Loss: 0.002651 Acc: 13.3438\n",
      " |~~ train@73344  Loss: 0.002087 Acc: 13.4844\n",
      " |~~ train@73408  Loss: 0.002951 Acc: 13.1719\n",
      " |~~ train@73472  Loss: 0.002503 Acc: 13.3281\n",
      " |~~ train@73536  Loss: 0.003099 Acc: 13.1719\n",
      " |~~ train@73600  Loss: 0.002463 Acc: 13.2812\n",
      " |~~ train@73664  Loss: 0.002665 Acc: 13.3125\n",
      " |~~ train@73728  Loss: 0.002618 Acc: 13.2188\n",
      " |~~ train@73792  Loss: 0.002791 Acc: 13.2656\n",
      " |~~ train@73856  Loss: 0.002303 Acc: 13.4531\n",
      " |~~ train@73920  Loss: 0.002706 Acc: 13.2969\n",
      " |~~ train@73984  Loss: 0.003331 Acc: 13.0469\n",
      " |~~ train@74048  Loss: 0.002653 Acc: 13.3281\n",
      " |~~ train@74112  Loss: 0.002581 Acc: 13.2812\n",
      " |~~ train@74176  Loss: 0.002365 Acc: 13.3750\n",
      " |~~ train@74240  Loss: 0.003127 Acc: 13.1406\n",
      " |~~ train@74304  Loss: 0.002512 Acc: 13.3125\n",
      " |~~ train@74368  Loss: 0.002575 Acc: 13.3125\n",
      " |~~ train@74432  Loss: 0.002467 Acc: 13.3906\n",
      " |~~ train@74496  Loss: 0.003164 Acc: 13.1250\n",
      " |~~ train@74560  Loss: 0.002660 Acc: 13.3125\n",
      " |~~ train@74624  Loss: 0.002464 Acc: 13.3125\n",
      " |~~ train@74688  Loss: 0.002784 Acc: 13.2188\n",
      " |~~ train@74752  Loss: 0.002857 Acc: 13.2812\n",
      " |~~ train@74816  Loss: 0.002803 Acc: 13.2031\n",
      " |~~ train@74880  Loss: 0.002096 Acc: 13.5000\n",
      " |~~ train@74944  Loss: 0.002676 Acc: 13.2344\n",
      " |~~ train@75008  Loss: 0.003026 Acc: 13.1094\n",
      " |~~ train@75072  Loss: 0.002678 Acc: 13.2500\n",
      " |~~ train@75136  Loss: 0.002582 Acc: 13.3125\n",
      " |~~ train@75200  Loss: 0.002311 Acc: 13.4062\n",
      " |~~ train@75264  Loss: 0.002500 Acc: 13.3594\n",
      " |~~ train@75328  Loss: 0.002666 Acc: 13.2656\n",
      " |~~ train@75392  Loss: 0.002579 Acc: 13.2969\n",
      " |~~ train@75456  Loss: 0.002855 Acc: 13.3594\n",
      " |~~ train@75520  Loss: 0.002167 Acc: 13.4531\n",
      " |~~ train@75584  Loss: 0.002800 Acc: 13.2500\n",
      " |~~ train@75648  Loss: 0.002490 Acc: 13.4062\n",
      " |~~ train@75712  Loss: 0.003088 Acc: 13.1719\n",
      " |~~ train@75776  Loss: 0.002579 Acc: 13.3438\n",
      " |~~ train@75840  Loss: 0.002920 Acc: 13.2188\n",
      " |~~ train@75904  Loss: 0.002907 Acc: 13.1875\n",
      " |~~ train@75968  Loss: 0.002903 Acc: 13.2188\n",
      " |~~ train@76032  Loss: 0.002295 Acc: 13.3438\n",
      " |~~ train@76096  Loss: 0.002404 Acc: 13.3281\n",
      " |~~ train@76160  Loss: 0.003043 Acc: 13.1719\n",
      " |~~ train@76224  Loss: 0.002656 Acc: 13.2344\n",
      " |~~ train@76288  Loss: 0.002521 Acc: 13.2969\n",
      " |~~ train@76352  Loss: 0.002812 Acc: 13.2031\n",
      " |~~ train@76416  Loss: 0.002587 Acc: 13.2969\n",
      " |~~ train@76480  Loss: 0.002627 Acc: 13.3125\n",
      " |~~ train@76544  Loss: 0.002152 Acc: 13.4688\n",
      " |~~ train@76608  Loss: 0.002528 Acc: 13.2812\n",
      " |~~ train@76672  Loss: 0.002362 Acc: 13.3906\n",
      " |~~ train@76736  Loss: 0.002829 Acc: 13.2188\n",
      " |~~ train@76800  Loss: 0.002848 Acc: 13.2188\n",
      " |~~ train@76864  Loss: 0.003494 Acc: 12.9531\n",
      " |~~ train@76928  Loss: 0.002654 Acc: 13.2969\n",
      " |~~ train@76992  Loss: 0.002662 Acc: 13.3281\n",
      " |~~ train@77056  Loss: 0.002749 Acc: 13.1875\n",
      " |~~ train@77120  Loss: 0.002652 Acc: 13.1875\n",
      " |~~ train@77184  Loss: 0.002900 Acc: 13.2344\n",
      " |~~ train@77248  Loss: 0.003105 Acc: 13.1406\n",
      " |~~ train@77312  Loss: 0.003715 Acc: 12.9688\n",
      " |~~ train@77376  Loss: 0.002590 Acc: 13.2500\n",
      " |~~ train@77440  Loss: 0.002485 Acc: 13.3125\n",
      " |~~ train@77504  Loss: 0.002911 Acc: 13.2188\n",
      " |~~ train@77568  Loss: 0.001944 Acc: 13.5781\n",
      " |~~ train@77632  Loss: 0.002646 Acc: 13.2656\n",
      " |~~ train@77696  Loss: 0.002781 Acc: 13.2812\n",
      " |~~ train@77760  Loss: 0.002473 Acc: 13.3750\n",
      " |~~ train@77824  Loss: 0.002829 Acc: 13.1875\n",
      " |~~ train@77888  Loss: 0.002408 Acc: 13.3594\n",
      " |~~ train@77952  Loss: 0.003205 Acc: 13.0781\n",
      " |~~ train@78016  Loss: 0.002338 Acc: 13.4062\n",
      " |~~ train@78080  Loss: 0.003030 Acc: 13.1250\n",
      " |~~ train@78144  Loss: 0.002596 Acc: 13.3281\n",
      " |~~ train@78208  Loss: 0.002848 Acc: 13.2812\n",
      " |~~ train@78272  Loss: 0.002715 Acc: 13.3750\n",
      " |~~ train@78336  Loss: 0.003141 Acc: 13.0312\n",
      " |~~ train@78400  Loss: 0.002806 Acc: 13.2344\n",
      " |~~ train@78464  Loss: 0.002912 Acc: 13.2031\n",
      " |~~ train@78484  Loss: 0.010911 Acc: 13.0000\n",
      "train  Loss: 0.002805 Acc: 13.2652\n",
      " |~~ val@64  Loss: 0.002041 Acc: 13.5156\n",
      " |~~ val@128  Loss: 0.003399 Acc: 13.0156\n",
      " |~~ val@192  Loss: 0.002907 Acc: 13.2344\n",
      " |~~ val@256  Loss: 0.002934 Acc: 13.1719\n",
      " |~~ val@320  Loss: 0.002255 Acc: 13.3750\n",
      " |~~ val@384  Loss: 0.002950 Acc: 13.2344\n",
      " |~~ val@448  Loss: 0.002380 Acc: 13.4375\n",
      " |~~ val@512  Loss: 0.002902 Acc: 13.1875\n",
      " |~~ val@576  Loss: 0.002718 Acc: 13.2656\n",
      " |~~ val@640  Loss: 0.002927 Acc: 13.1406\n",
      " |~~ val@704  Loss: 0.002319 Acc: 13.4062\n",
      " |~~ val@768  Loss: 0.003080 Acc: 13.2031\n",
      " |~~ val@832  Loss: 0.002463 Acc: 13.3594\n",
      " |~~ val@896  Loss: 0.002838 Acc: 13.2031\n",
      " |~~ val@960  Loss: 0.002910 Acc: 13.2500\n",
      " |~~ val@1024  Loss: 0.002793 Acc: 13.2656\n",
      " |~~ val@1088  Loss: 0.002495 Acc: 13.3125\n",
      " |~~ val@1152  Loss: 0.002836 Acc: 13.1875\n",
      " |~~ val@1216  Loss: 0.003400 Acc: 12.9531\n",
      " |~~ val@1280  Loss: 0.002544 Acc: 13.2656\n",
      " |~~ val@1344  Loss: 0.002643 Acc: 13.3125\n",
      " |~~ val@1408  Loss: 0.002467 Acc: 13.3594\n",
      " |~~ val@1472  Loss: 0.002391 Acc: 13.3750\n",
      " |~~ val@1536  Loss: 0.002463 Acc: 13.3438\n",
      " |~~ val@1600  Loss: 0.002564 Acc: 13.2656\n",
      " |~~ val@1664  Loss: 0.002554 Acc: 13.3438\n",
      " |~~ val@1728  Loss: 0.002694 Acc: 13.2500\n",
      " |~~ val@1792  Loss: 0.002357 Acc: 13.3750\n",
      " |~~ val@1856  Loss: 0.003280 Acc: 13.0938\n",
      " |~~ val@1920  Loss: 0.003022 Acc: 13.2188\n",
      " |~~ val@1984  Loss: 0.002755 Acc: 13.2188\n",
      " |~~ val@2048  Loss: 0.002653 Acc: 13.3281\n",
      " |~~ val@2112  Loss: 0.001976 Acc: 13.5781\n",
      " |~~ val@2176  Loss: 0.002256 Acc: 13.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |~~ val@2240  Loss: 0.003036 Acc: 13.2031\n",
      " |~~ val@2304  Loss: 0.002827 Acc: 13.2812\n",
      " |~~ val@2368  Loss: 0.003150 Acc: 13.0938\n",
      " |~~ val@2432  Loss: 0.002489 Acc: 13.3906\n",
      " |~~ val@2496  Loss: 0.002255 Acc: 13.4219\n",
      " |~~ val@2560  Loss: 0.002170 Acc: 13.4688\n",
      " |~~ val@2624  Loss: 0.003219 Acc: 13.0625\n",
      " |~~ val@2688  Loss: 0.002803 Acc: 13.1875\n",
      " |~~ val@2752  Loss: 0.002508 Acc: 13.3125\n",
      " |~~ val@2816  Loss: 0.002630 Acc: 13.2500\n",
      " |~~ val@2880  Loss: 0.002987 Acc: 13.2031\n",
      " |~~ val@2944  Loss: 0.002489 Acc: 13.3438\n",
      " |~~ val@3008  Loss: 0.002990 Acc: 13.1875\n",
      " |~~ val@3072  Loss: 0.002601 Acc: 13.3750\n",
      " |~~ val@3136  Loss: 0.003126 Acc: 13.1406\n",
      " |~~ val@3200  Loss: 0.002609 Acc: 13.2656\n",
      " |~~ val@3264  Loss: 0.002609 Acc: 13.2344\n",
      " |~~ val@3328  Loss: 0.002784 Acc: 13.3281\n",
      " |~~ val@3392  Loss: 0.002730 Acc: 13.3125\n",
      " |~~ val@3456  Loss: 0.002506 Acc: 13.3750\n",
      " |~~ val@3520  Loss: 0.003262 Acc: 13.0781\n",
      " |~~ val@3584  Loss: 0.003136 Acc: 13.1875\n",
      " |~~ val@3648  Loss: 0.002262 Acc: 13.4062\n",
      " |~~ val@3712  Loss: 0.002696 Acc: 13.2656\n",
      " |~~ val@3776  Loss: 0.002733 Acc: 13.1875\n",
      " |~~ val@3840  Loss: 0.002726 Acc: 13.2500\n",
      " |~~ val@3904  Loss: 0.002530 Acc: 13.3125\n",
      " |~~ val@3968  Loss: 0.002674 Acc: 13.2500\n",
      " |~~ val@4032  Loss: 0.002573 Acc: 13.2969\n",
      " |~~ val@4096  Loss: 0.003174 Acc: 13.0469\n",
      " |~~ val@4160  Loss: 0.002425 Acc: 13.2656\n",
      " |~~ val@4224  Loss: 0.002749 Acc: 13.3125\n",
      " |~~ val@4288  Loss: 0.002763 Acc: 13.2031\n",
      " |~~ val@4352  Loss: 0.002815 Acc: 13.2500\n",
      " |~~ val@4416  Loss: 0.002943 Acc: 13.2500\n",
      " |~~ val@4480  Loss: 0.003037 Acc: 13.1875\n",
      " |~~ val@4544  Loss: 0.002930 Acc: 13.1250\n",
      " |~~ val@4608  Loss: 0.002502 Acc: 13.2656\n",
      " |~~ val@4672  Loss: 0.002774 Acc: 13.2031\n",
      " |~~ val@4736  Loss: 0.002727 Acc: 13.3125\n",
      " |~~ val@4800  Loss: 0.002387 Acc: 13.3750\n",
      " |~~ val@4864  Loss: 0.003025 Acc: 13.1562\n",
      " |~~ val@4928  Loss: 0.002944 Acc: 13.1250\n",
      " |~~ val@4992  Loss: 0.002037 Acc: 13.5000\n"
     ]
    }
   ],
   "source": [
    "train_model(model_ft,\n",
    "            criterion,\n",
    "            optimizer_ft,\n",
    "            learning_scheduler,\n",
    "            num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model results to S3\n",
    "\n",
    "aws s3 cp ResNet18PlusFlexibleFC_Epoch9.tar s3://bdh-xrayproj-modelparameters/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.list_buckets()\n",
    "\n",
    "S3 Commands: http://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html\n",
    "\n",
    "Boto3 QuickStart: http://boto3.readthedocs.io/en/latest/guide/quickstart.html\n",
    "\n",
    "Key Management: https://aws.amazon.com/blogs/security/a-safer-way-to-distribute-aws-credentials-to-ec2/\n",
    "\n",
    "AWS IAM Rules: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-api.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
